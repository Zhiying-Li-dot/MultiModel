\begin{abstract}
There is a rapidly growing interest in controlling consistency across multiple generated images using diffusion models. Among various methods, recent works have found that simply manipulating attention modules by concatenating features from multiple reference images provides an efficient approach to enhancing consistency without fine-tuning. Despite its popularity and success, few studies have elucidated the underlying mechanisms that contribute to its effectiveness. In this work, we reveal that the popular approach is a linear interpolation of image self-attention and cross-attention between synthesized content and reference features, with a constant rank-1 coefficient. Motivated by this observation, we find that a rank-1 coefficient is not necessary and simplifies the controllable generation mechanism. The resulting algorithm, which we coin as \ours, allows users to control the influence of reference context in a direct and precise manner. Besides further enhancing consistency in single-subject image generation, our method also enables more interesting applications, such as the consistent generation of multiple subjects, suppressing specific features to encourage more diverse content, and high-quality personalized video generation by boosting temporal consistency. Even compared with state-of-the-art image-prompt-based generators, such as IP-Adapter, \ours ~is competitive in terms of controllability and quality while avoiding the need to train a separate image encoder for feature injection from reference images, making it a versatile plug-and-play solution for any image or video diffusion model. Our project webpage is \url{https://sbyebss.github.io/refdrop/}.
\end{abstract}

\begin{figure}[h]
  \centering
    \makebox[\linewidth]{\includegraphics[width=1\linewidth]{images/teaser3.pdf}}
  \caption{
\ours ~achieves controllable consistency in visual content synthesis for free.
\ours ~exihibits great flexibility in
(Upper) multi-subject consistency generation given one reference image,
(Middle) blending different characters from multiple images seamlessly,
(Buttom) enhancing temporal consistency for personalized video generation.
   \ours ~is short for "reference drop". We named our method \ours ~to metaphorically represent the process by which a drop of colored water influences a larger body of clear water.
 }
\end{figure}

\section{Introduction}

Large-scale diffusion models have demonstrated remarkable capabilities in aiding content creation for artists~\citep{podell2023sdxl,sora,balaji2022ediff}. Numerous text-to-image models are expediting content production in various domains, including advertising and art studios. Similarly, video generation models have shown significant advancements recently~\citep{girdhar2023emu,chen2023videocrafter1,guo2023animatediff,Hu2023-wr, makeavideo, blattmann2023align, bar2024lumiere, vdm}. However, enhancing these models to better support artistic creativity requires improved controllability, particularly in content consistency. This paper explores consistency from two perspectives: 1) controlling subject consistency across multiple images, and 2) maintaining subject consistency across multiple frames within a video. 

We name a few tasks where the controllable consistency is crucial in AI content generation.
In image generation for storytelling~\citep{rahman2023make,maharana2022storydall, pan2024synthesizing, jeong2023zero} or advertising, content creators often strive to produce consistent characters, a task that proves challenging with foundational generative models~\citep{tewel2024training}. Personalization approaches based on fine-tuning~\citep{ruiz2023dreambooth} require a minimum of 5 to 10 images to achieve satisfactory quality, and encoder-based methods~\citep{wei2023elite,wang2024instantid, pan2023kosmos} demand weeks of training 
with millions of images
for a single diffusion model and lack transferability to other foundational models. 
On the other hand, diverse image generation is less addressed but persistently challenging. In this scenario, it is desired to \textit{decrease} the consistency among image generations. For example,
artists can sometimes seek to enhance diversity and avoid clichÃ©s, such as the stereotypical depiction of Barbie girls with curly blonde hair. 
For video generation, another challenging task is
maintaining temporal consistency in video generation, yet most existing solutions are confined to video editing tasks~\citep{ku2024anyv2v}, demanding high-quality input videos. 

These emerging tasks motivate us to develop \ours, a \textbf{
training-free}, \textbf{plug-and-play} method designed to 
provide flexible control over the consistency in image and video generation.
Specifically, we modify the self-attention mechanism in the diffusion model UNet~\citep{Ronneberger2015-iw} architecture and introduce a coefficient to modulate the influence of a reference image on the generation process. Our contributions are outlined as follows:

1. We conduct a detailed analysis of popular consistency generation methods based on concatenated attention, revealing that their consistency is actually contributed by extra guidance applied implicitly.
2. Inspired by this finding, we propose \RMA~(\rma), a natural extension that explicitly controls the guidance from reference context in a precise and direct manner. Building upon~\rma, we introduce \ours, a flexible and efficient approach to controlling consistency without the need for network fine-tuning or optimization.
3. Besides improvements in character consistency using a single reference image, \ours ~enables more creative applications with controllable consistency, including (i) seamless integration of distinct features into a single cohesive image (ii) suppressing specific features by negatively decreasing the consistency influenced by the reference context, thereby enhancing diversity in layout, accessories, and image style; (iii) high-quality personalized video generation by boosting temporal consistency,
and minimizing facial distortions. 
4. We conduct comprehensive experiments and demonstrate that \ours ~achieves a good balance between flexibility and effectiveness while being lightweight compared to existing works. 

















\section{Related work}
Among the works most similar to ours are IP-Adapter~\citep{ye2023ipa} and concatenated attention~\citep{wu2023tune}. Our approach is closely related to IP-Adapter, as both methods utilize the sum of two decoupled attention outputs. However, while IP-Adapter modifies cross-attention and requires separate training of an image encoder to embed the reference image, we integrate the reference image directly into the self-attention layer without needing additional training. Furthermore, our reference images are \textit{generated} by the same model, in contrast to IP-Adapter's reliance on externally sourced image. Both techniques permit the use of negative or positive coefficients for the reference image, but IP-Adapter may compromise text alignment~\cite{tewel2024training} due to its reference image being intertwined with the text prompt during cross-attention. Additionally, the IP-Adapter requires separate training for different versions of the diffusion model, such as SD2.1 and SDXL. In contrast, \ours ~is a simple plug-and-play.

Concatenated attention, first introduced in video generation literature by \citet{wu2023tune} as spatio-temporal attention, injects temporal information into a T2I model. It has since been widely adopted for feature injection across various applications \citep{luo2024diffusion, chang2023magicdance, huang2024parts, tewel2024training} in content generation and video editing. This concept has evolved into Cross-Frame Attention~\citep{khachatryan2023text2video}, another prevalent technique used to inflate T2I models~\citep{zhang2024videoelevator} for video generation. We will demonstrate later that our framework can replicate these two types of attention as special cases.




\vspace{-0.2cm}
\paragraph{Consistency in Image Generation}
ConsiStory \citep{tewel2024training} and StoryDiffusion \citep{zhou2024storydiffusion} are closely related to our work. They are training-free methods that employs concatenated attention to enhance consistency in generation. Our \rma ~framework is {\em orthogonal} to the techniques other than concatenated attention in those works, such as subject masking and attention dropout.
\citet{avrahami2023chosen} explores a fine-tuning-based method aimed at recovering tightly clustered images. 
Other approaches, such as those by \citep{jeong2023zero,feng2023improved,liu2023intelligent}, predominantly utilize a personalization process \citep{ruiz2023dreambooth,gal2022image,kumari2023multi,tewel2023key} requiring multiple input images for training.
Finally, several encoder-based methods \citep{wei2023elite,wang2024instantid,li2024blip,ruiz2023hyperdreambooth,xiao2023fastcomposer,kim2024instantfamily,li2023photomaker} do not require additional training for new subjects. However, these methods necessitate days or weeks of initial training for the encoder and face limitations in adaptability to different versions of foundational generative models. 


\paragraph{Temporal-consistency in video generation}
Concatenated attention~\citep{wu2023tune,ren2024consisti2v} and Cross-Frame Attention~\citep{khachatryan2023text2video,Chen2024-ag} are popular techniques used to inflate T2I models for video generation. \citet{wu2023freeinit, ren2024consisti2v} mitigate video flickering by applying a low-pass filter to noisy latent images, effectively removing disruptive high-frequency content. Many other methods are tailored for video editing tasks, and they either extract features from high-quality input videos to enhance the current generation \citep{ku2024anyv2v, zhang2024fastvideoedit, yang2024fresco, yang2023rerender} or use them as references during editing \citep{geyer2023tokenflow, cong2023flatten}. \ours ~improves temporal consistency directly within video generation, obviating the need for an input video.






\section{Method}
\label{headings}
We first introduce how existing works achieve consistency generation by leveraging the concatenation of reference features in the self-attention block. Then we reformulate the concatenation as a linear interpolation of self-attention on synthesized content and cross-attention between generated and reference content with a constant rank-1 coefficient. We highlight that this specific coefficient is not a necessity, while linear interpolation is critical to minimizing the training-inference gap. Building upon these observations, we propose \RMA~(\rma), an extension of concatenation attention that allows for flexible feature interpolation and extrapolation in attention modules. Based on~\rma, we introduce~\ours, a versatile method for controllable consistency generation across various applications.


\begin{figure}[t!]
  \centering
  \vspace{-0.2cm}
    \makebox[\linewidth]{\includegraphics[width=1\linewidth]{images/diagram.pdf}}
  \caption{During each diffusion denoising step, we facilitate the injection of features from a \textit{generated} reference image $I_1$ into the generation process of other images through \rma. The \rma ~layer produces a linear combination of the attention outputs from both the standard and referenced routes. A negative coefficient $c$ encourages divergence of $I_i$ from $I_1$, while a positive coefficient fosters consistency.
  }\label{fig:rma}
\end{figure}

\subsection{Background}
Self-attention in diffusion model networks operates by applying the attention mechanism~\citep{Vaswani2017-hk} on synthesized latent features.
A self-attention layer processes latent representations \(X\) by passing them through linear projection layers to produce queries $Q = X W_Q$, keys $K = X W_K$, and values $V = X W_V$, which then undergo the attention operation as follows:
\begin{align}
     X' = \text{Attention} (Q, K, V) = \text{Softmax}\left(\frac{QK^\top}{\sqrt{d}}\right) V, \label{eq:self-attn}
\end{align}
where $X'$ is the output of self-attention operation, and $d$ is the feature dimension of projection matrices $W_Q, W_K,W_V.$
Previous consistency generation~\citep{tewel2024training,zhou2024storydiffusion} is based on concatenated attention via a simple batch image generation, where the first sample in the batch serves as a reference for the $i$-th sample generation.
We denote the latent feature for $i$-th sample as $X_i$. Instead of solely depending on its own content, concatenated attention suggests
\begin{equation}\label{eq:concat}
X_{\texttt{CAT}}' =  
\text{Attention}\left({Q_i, [K_1; K_i], [V_1; V_i]}\right),
\end{equation}
where \(Q_i = X_i W_Q\), \(K_i = X_i W_K\), and \(V_i = X_i W_V\).


\subsection{Reference feature guidance}\label{sec:rma}

To illustrate why concatenated attention can help boost consistency between generated samples with reference samples, we can reformulate~\cref{eq:concat} as the following~(Proof in~\cref{sec:relation_concat})
\begin{align}\label{eq:concat_rewrite}
X_{\texttt{CAT}}' &=  
C \odot \text{Attention}\left({Q_i, K_1, V_1}\right) + (\mathbf{1}-C) \odot \text{Attention}\left({Q_i, K_i, V_i}\right)
\end{align}
where \(C\) is a rank-1 matrix of the same size as the attention output, \(\odot\) is the point-wise multiplication and \(\mathbf{1}\) is an all-ones matrix.

\Cref{eq:concat_rewrite} depicts that the concatenated attention is a linear interpolation between the output $X'$ without concatenated attention in~\cref{eq:self-attn} and cross-attention between the $i$-th image $X_i$ and the reference image $X_1$, while the coefficient matrix $C$ is determined by the synthesized content $X_i$ and the reference content $X_1$. Before we further improve concatenated attention, we first discuss two related questions for~\cref{eq:concat_rewrite}.
\textbf{Is linear interpolation a necessity?} 
It may be tempting to highlight the role of the second cross-attention term naively while keeping the weights for the first term unchanged, such as $\text{Attention}\left(Q_i, K_1, V_1\right) + \text{Attention}\left(Q_i, K_i, V_i\right)$. However, we find that naively breaking the linear interpolation disrupts image generation. In fact, we can interpret concatenated attention in \cref{eq:concat_rewrite} as applying extra guidance on the original self-attention output 
\begin{align}\label{eq:guidance}
X_{\texttt{CAT}}'
&= \text{Attention}\left({Q_i, K_i, V_i}\right) + C \odot ( \text{Attention}\left({Q_i, K_1, V_1}\right) - \text{Attention}\left({Q_i, K_i, V_i}\right))
\end{align}
which resembles the form of guidance used in diffusion literature~\citep{song2023loss,ho2022video}, such as classifier-free guidance~\citep{ho2022classifier}. 
Notably, the linear interpolation helps keep the attention output $X_{\texttt{CAT}}'$ norm close to self attention output $X'$; otherwise, arbitrary weights would pose a training and inference discrepancy and degrade the generation quality.
However, different from various guidance methods used in the diffusion literature, the guidance weights in \cref{eq:guidance} are constants determined by latent features $X_i$ and reference context $X_1$ and have no user control. 
Therefore we question 
\textbf{Is constant $C$ matrix coefficient is a necessity?} 
As an attempt to bypass the rigid form of concatenated attention, we propose a simple and flexible approach named \textit{Reference feature guidance}~(\rma) (see \cref{fig:rma})
\begin{equation}
X_{\rma}' = c \cdot \text{Attention}\left({Q_i, K_1, V_1}\right) + (1-c) \cdot \text{Attention}\left({Q_i, K_i, V_i}\right),
\label{eq:rma}
\end{equation}
where \(c\) is a scalar coefficient that controls the strength of the reference image influence.







\begin{wrapfigure}[16]{r}{0.6\textwidth}
\vspace{-0.5cm}
\centering
    \includegraphics[width=1\linewidth , align = c]{images/coeff_simplepdf.pdf}   
    \caption{We allow flexible control over the reference effect through a reference strength coefficient. 
    }  \label{fig:coeff}
\end{wrapfigure} 
While most previous methods for consistent generation, including feature combination~\citep{kim2024instantfamily,zhou2024storydiffusion} and injection~\citep{tewel2024training}, employ concatenated attention~\eqref{eq:concat}, our \rma~offers several advantages. First, it grants users greater control over the extent of influence from the reference image, as illustrated in \cref{fig:coeff}.
Second, this flexibility proves especially beneficial in a novel application: blending features from multiple reference images. Our method allows users to selectively determine the influence of each reference image. We have observed that the most harmonious blending often results from varying the strength of each reference, rather than maintaining equal strength across all images. Third, by enabling negative coefficients, we find that our method can simulate a concept suppression effect, meaning it generates images that are dissimilar to a reference image. Finally, it allows for the injection of reference image features into video generation slightly to reduce flickering, while the concatenated attention keeps the video completely static.



Therefore, we introduce~\ours, a training-free approach to flexibly control consistency generation, which replaces the self-attention blocks in the diffusion model with~\rma.
For Video Diffusion Models (VDM)~\citep{blattmann2023stable, ge2023preserve, he2022latent}, we modify \textit{every} \textit{spatial} self-attention layers to bolster temporal consistency. 


















\begin{wraptable}[11]{r}{0.5\textwidth} 
  \vspace{-1.3cm}
  \caption{Comparison of Controllable Consistent Image Generation Methods. `Training-free' indicates no encoder training or diffusion model fine-tuning is needed. 
  `Single ref.' means the method can operate with only one reference image.
  }
  \label{tab:img_compare}
  \centering
 \small %
  \begin{tabular}{l p{1cm} p{1.5cm} p{0.9cm} }
    \toprule
    Name     &  Training free  & 
    Concept suppression  & Single ref. \\
    \midrule
    IP-Adapter~\citep{ye2023ipa} &  \xmark  
    & \cmark  & \cmark     \\
    Consistory~\citep{tewel2024training}    & \cmark      
    &  \xmark & \cmark \\
    Chosen one~\citep{avrahami2023chosen} & \xmark   
    &  \xmark & \xmark \\
    ELITE~\citep{wei2023elite}   & \xmark 
    & \xmark  &  \cmark \\
    BLIPD~\citep{li2024blip}    & \xmark
    &  \xmark    & \cmark  \\
    Ours & \cmark 
    & \cmark & \cmark \\
    \bottomrule
  \end{tabular}
  \normalsize %
\end{wraptable}
\section{Experiments}
\label{others}

We conduct experiments to show that \ours ~can help control consistency in two important tasks: image generation and video generation.




\vspace{-0.2cm}
\subsection{Controllable consistency in \\ image generation}



We use a fine-tuned SDXL of higher quality, ProtoVision-XL, as the base model for our experiments. For simplicity, we will refer to it as SDXL hereafter. We have replaced all the self-attention layers in SDXL with \rma, using the first sample in the batch as the reference image.

\vspace{-0.1cm}
\paragraph{Evaluation baselines}
In this section, we compare \ours ~with several baseline approaches: (1) SDXL~\cite{podell2023sdxl} without any modifications to its architecture; (2) encoder-based methods, such as IP-Adapter~\cite{ye2023ipa} and BLIPD~\cite{li2024blip}. For encoder-based methods, we initially generate a reference image using SDXL and then utilize this image as input. Additionally, we present a comparison of several other methods in \cref{tab:img_compare}.





\subsubsection{Consistent image generation}\label{sec:consist}
\begin{figure}[h]     
	\centering
    \vspace{-0.3cm} 
      \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=1\linewidth , align = c]{images/comparison_image_single_subject_6_our_coeff_0.4_seed_1-compressed.pdf}
  \end{subfigure} 
  \begin{subfigure}{0.485\textwidth}
    \centering
\includegraphics[width=1\linewidth, align = c]{images/comparison_image_multi_human_0_our_coeff_0.4_seed_5-compressed.pdf}
  \end{subfigure}    
    \caption{
    The {\color{darkred}reference} image for all methods is framed in {\color{darkred}red}. Our method tends to produce more consistent outfits, hairstyles, and facial features compared to IP-Adapter and BLIPD. The visual quality of BLIPD is not comparable, as it utilizes SD1.5~\citep{rombach2022highresolution} as its base model.
    }  \label{fig:comp1}
\end{figure} 
For this task, we use 
$c \in [0.3,0.4]$ for our method.
We show qualitative results in \cref{fig:comp1}. 
IP-Adapter establishes a strong baseline,  especially on single subject consistent generation. However, it requires additional computational resources and data for training the image encoder compared to our approach.
BLIPD underperforms in terms of both visual quality and consistency relative to~\ours.
Additionally, we find that the concatenated attention used in studies like \citet{tewel2024training} and \citet{zhou2024storydiffusion} produces results that are quite similar to our \rma~mechanism, demonstrating that \rma~is capable of replicating the effects of concatenated attention. We also have the flexibility to incorporate other techniques from these studies~\citep{tewel2024training,zhou2024storydiffusion} to enhance pose diversity and reduce background leakage.
For \textbf{multi-subject} consistent generation, we find \ours ~can straightforwardly work for semantically different objects 
even without using separate subject masks.
This observation aligns with ConsiStory~\citep{tewel2024training}.
Further comparative results are available in the supplementary material.
















\subsubsection{Blend features from multiple images}\label{sec:blend}




\begin{wrapfigure}[17]{r}{0.5\textwidth}
\vspace{-1.5cm}
\centering
    \includegraphics[width=1\linewidth , align = c]{images/blend_compare.pdf}   
    \caption{\textbf{Multiple Reference Images:} The {\color{darkred}reference} images are highlighted with a {\color{darkred}red} frame, and the third image in each set is the resultant blended image. \ours~effectively assimilates features from the distinct reference images into a single and cohesive entity, demonstrating robust feature integration capability.  
    }  \label{fig:blend}
\end{wrapfigure} 
$\ours$ also supports the use of multiple reference images. In our implementation, we designate the first $N$ images in a batch as reference images. Features from these reference images are then incorporated into the subsequent images within the same batch through every self-attention layer. Formally, the extended \rma~with multiple references is defined as
\begin{align}
&X_{\rma}' = \sum_{j=1}^N c_j \cdot \text{Attention}(Q_i, K_j, V_j) + \nonumber \\
& (1 - \sum_{j=1}^N c_j) \cdot \text{Attention}(Q_i, K_i, V_i), \quad 
\label{eq:rma_multi}
\end{align}
for certain $ i > N$. Here, the attention mechanism ensures that the $i$-th image in the batch receives features from the first $1 \sim N$ reference images. 
In practice, we use $c_j \in [0.2,0.4]$ for any $j=1,\ldots, N$ in our method.
We demonstrate the capability of \ours ~to seamlessly blend distinct semantic features from two reference objects into a new object in \cref{fig:blend}. 
This task proves challenging when relying solely on prompt engineering. We attempted to achieve this task with SDXL using text prompts. For instance, if we aim to merge two objects, $\alpha$ and $\beta$, we might use prompts like ``an $\alpha$-like $\beta$'' or ``a $\beta$ in the style of $\alpha$.'' However, with such text prompts, SDXL either ignores the similarity with one of the reference images or frequently produces multiple objects instead of a single and cohesive entity. In \cref{fig:blend_3reference}, we show that \ours ~can blend \textit{three} distinct subjects: a dwarf, Black Widow, and Winnie the Pooh, encompassing a range of mythological being, human, and animal.









\subsubsection{Diverse image generation}\label{sec:diverse}

\begin{wrapfigure}[5]{r}{0.3\textwidth} %
\vspace{-1.7cm}
\centering
      \begin{subfigure}{0.14\textwidth}
    \centering
    \includegraphics[width=1\linewidth , align = c]{images/neg_anchor1.png}
  \end{subfigure} 
  \begin{subfigure}{0.14\textwidth}
    \centering
\includegraphics[width=1\linewidth, align = c]{images/neg_anchor2.png}
  \end{subfigure}    
    \caption{Negative reference images for examples in \cref{fig:diverse_comp}.
    }  \label{fig:neg_anchor}
\end{wrapfigure} 

Our method offers substantial flexibility in parameter tuning, enabling diverse image generation by setting the coefficient $c$ to a negative value. This feature is particularly valuable in addressing overfitting issues in image generation. 
For instance, when using SDXL to generate Middle Eastern faces, the output frequently includes similar headscarves, faces and outfits, as illustrated on the left side of \cref{fig:diverse_comp}.

In this task, we use $c = -0.3$ for our method.
We present a qualitative comparison in \cref{fig:diverse_comp}, with negative reference images displayed in \cref{fig:neg_anchor}. Upon comparing our method with IP-Adapter, we note that IP-Adapter may not adhere as closely to the text prompt. We attribute this to IP-Adapter's modification of cross-attention, which can impact text alignment. In contrast, our method focuses on modifying self-attention, thereby preserving the integrity of cross-attention and ensuring more accurate text alignment. We show additional quantitative results in \cref{fig:img_quant}.





\begin{figure}[h]     
	\centering
      \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=1\linewidth , align = c]{images/comparison_image_single_subject_0_our_coeff_-0.3_seed_1-compressed.pdf}
  \end{subfigure} 
  \begin{subfigure}{0.485\textwidth}
    \centering
\includegraphics[width=1\linewidth, align = c]{images/comparison_image_single_subject_8_our_coeff_-0.3_seed_1-compressed.pdf}
  \end{subfigure}    
    \caption{{\color{Blue} Diverse image generation}: 
   Our method enhances diversity in outfits, hairstyles, and facial features, all while ensuring accurate text alignment. For example, while SDXL frequently generates headscarves in the first scenario and beige-colored clothes in the second, \ours ~can vary the presence of headscarves in the left example and produce clothing in different colors in the right example. Conversely, although IP-Adapter can create even more diverse images, it often fails to adhere to the style and human activity instructions in the text prompts. Additionally, it often produces overly small persons that lack detail.
    }  \label{fig:diverse_comp}
\end{figure} 



\vspace{-0.3cm}
\subsection{Improving temporal-consistency in video generation}
Not only can we apply \rma ~to T2I generation, but it also effectively stabilizes video generation, where flickering commonly degrades quality. This section shows that using the first generated frame as a reference can greatly improve video generation. By injecting its features into the spatial self-attention layers of subsequent frames with a reference strength of 
$c=0.2$, we significantly stabilize these frames and enhance the temporal consistency of VDM.

We employ \texttt{SVD-img2vid-xt-1-1} as our I2V base model. Technically, our approach is 
compatible with any VDM, but we choose SVD as it is the best open-source model available. Although this model usually produces consistent videos from visually perfect images, we have noted that minor, often imperceptible flaws in the input images can significantly degrade the quality of the generated videos. Our method effectively stabilizes video quality in these scenarios.





\subsubsection{Temporal-consistent video generation}\label{sec:consist_video}

\begin{figure}[h]     
\centering
\vspace{-0.2cm}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth , align = c]{images/compare_male_naive_15.pdf}
  \end{subfigure} 
  \begin{subfigure}{0.49\textwidth}
    \centering
\includegraphics[width=1\linewidth, align = c]{images/compare_female_naive_2.pdf}
  \end{subfigure}    
    \caption{Comparison of training free techniques to improve temporal consistency in video generation.
    }  \label{fig:consistent_vid}
\end{figure} 

In this part, we use the SDXL model to generate an image from a prompt and then pass this image to the SVD model. For \textbf{evaluation baselines}, we compare \ours ~with several training-free methods: unmodified SVD, Cross-Frame Attention~\citep{khachatryan2023text2video}, Concatenated Attention~\citep{wu2023tune}, and Temporal Low Pass Frequency Filter (LPFF)~\citep{zhang2024videoelevator}. Temporal LPFF is arguably superior to Spatial-Temporal LPFF by \citet{wu2023freeinit}, which shows Spatial-Temporal LPFF can result in blurry frames. We evaluate the Temporal LPFF using a fast sampling method that avoids the computationally intensive process of iteratively performing backward and forward diffusion at each denoising step.


The visualization results are displayed in \cref{fig:consistent_vid}. We observe that both Cross-Frame Attention and Concatenated Attention result in completely static videos, whereas LPFF shows minimal improvement. Our method proves to be the most effective in preventing flickering while preserving motion.















\subsubsection{Stabilizing personalized video generation}\label{sec:pv}
Finally, we explore the application of \ours ~to personalized video generation. Inspired by \citet{ku2024anyv2v}, starting with an image of a person, we use InstantID~\citep{wang2024instantid} to generate a personalized initial frame. This frame is then fed into SVD to create a short video. However, we observe that using the output from InstantID for SVD generation leads to a significantly higher failure rate compared to using the initial frame generated by SDXL. We attribute this increased failure rate to InstantID's propensity for producing images with more flaws, such as overly saturated colors, and distorted limbs, highlighting the potential demand for \ours ~in this task.

\begin{figure}[h!]
  \centering
    \makebox[\linewidth]{\includegraphics[width=1\linewidth]{images/pv1.pdf}}
  \caption{By injecting the features of the first frame into the generation of subsequent frames, \ours ~reduces flickering and facial distortions. The additional videos can be viewed \href{https://refdrop-anonymouspaper-f37a6c745f264e0ff8b994669d71e9ca5f34d07a.gitlab.io/index.html}{here}.
  }
  \label{fig:person_vid}
\end{figure}


Several other methods are available for personalized video generation, as described in works \citep{wang2024customvideo,jiang2023videobooth,ma2024magic,Hu2023-wr}. 
Our method, which is designed to enhance temporal consistency, can be integrated with some of these existing approaches.
For example, in the case of Magic-Me \citep{ma2024magic}, 
our attention mechanism \rma ~ can be incorporated into their AnimateDiff \citep{guo2023animatediff} backbone. 
For the evaluation in this section, we primarily focus on comparisons with naive SVD generation, as it directly relates to our goal of enhancing temporal consistency.

We present such comparison 
between our \ours ~enhanced generation to the naive SVD generation
in \cref{fig:person_vid}. \ours ~effectively preserves identity during video generation, offering improvements similar to those achieved by increasing the CFG. However, unlike increasing CFG, which often results in over-saturation of videos, our approach does not produce such artifacts. 
We present additional automatic metrics in \cref{tab:comparison_metrics} to show that \ours~can enhance the quality of the generated videos.





















\section{Human evaluation}
\begin{figure}[h!]
  \centering
    \centering
    \vspace{-0.3cm}
    \includegraphics[width=1\linewidth]{images/he.pdf}
  \caption{A higher score indicates a superior result. In the category of consistent image generation, participants showed a preference for \ours, with IP-Adapter ranking slightly behind. In diverse image generation, while IP-Adapter was favored for its variety, it significantly compromised text alignment. Conversely, \ours ~maintained a good balance, achieving diversity while preserving text alignment. In personalized video generation, users clearly preferred our approach, demonstrating substantial improvements over the SVD results.} \label{fig:human}
  \end{figure}
We conducted a human evaluation study using Google Forms. Our survey is structured into three distinct categories: 1) Consistent Image Generation, 2) Diverse Image Generation, and 3) Personalized Video Generation. Initially, we utilized ChatGPT to generate text prompts, then processed approximately 100 small tasks per category using both baseline methods and our approach. From these, we randomly selected 10 sets for evaluation. In the first two categories, participants assessed the consistency and diversity of the images, as well as text alignment. For the third category, participants were asked to select the video with better quality. The aggregated scores are detailed in \cref{fig:human}. We collected responses from 44 distinct users in total.
More details appear in \cref{sec:user_study}.









\section{Conclusion}\label{sec:conclude}
In this study, we propose a method that effectively uses one or multiple generated images to guide the generation of other images or video frames. Through extensive experiments, our method has proven useful for flexible consistency control in image generation and has improved temporal consistency in video generation. In particular, we show applications in consistent and diverse image generation, feature blending from multiple images, and enhancement of video temporal consistency.
Moreover, our approach is versatile on network architecture as it applies not only to UNet-based models but also to transformer-based diffusion models like DiT~\citep{peebles2023scalable}.  

Looking ahead, several promising avenues for further research emerge from this study. Firstly, our experiments have not yet explored the use of attention masks; investigating their potential for precise control in image generation presents a compelling opportunity for future work. Another exciting prospect involves enhancing our method to accept clean reference images as input, similar to the IP-Adapter and other image personalization techniques. Achieving this capability would represent a significant advancement, particularly if coupled with  an optimal image inversion method.

\section{Limitation} \label{sec:limit}
In consistent image generation, our model sometimes struggles to exactly replicate the appearance of specific objects, tending to hallucinate rather than accurately reproduce items like lotion bottles.









\bibliography{ref}
\bibliographystyle{tmlr}
