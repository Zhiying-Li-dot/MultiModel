@inproceedings{song2023loss,
  title={Loss-guided diffusion models for plug-and-play controllable generation},
  author={Song, Jiaming and Zhang, Qinsheng and Yin, Hongxu and Mardani, Morteza and Liu, Ming-Yu and Kautz, Jan and Chen, Yongxin and Vahdat, Arash},
  booktitle={International Conference on Machine Learning},
  pages={32483--32498},
  year={2023},
  organization={PMLR}
}

@article{ho2022classifier,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}


@article{ho2022video,
  title={Video diffusion models},
  author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8633--8646},
  year={2022}
}

@article{ku2024anyv2v,
  title={AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks},
  author={Ku, Max and Wei, Cong and Ren, Weiming and Yang, Huan and Chen, Wenhu},
  journal={arXiv preprint arXiv:2403.14468},
  year={2024}
}

@article{meng2021sdedit,
  title={Sdedit: Guided image synthesis and editing with stochastic differential equations},
  author={Meng, Chenlin and He, Yutong and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano},
  journal={arXiv preprint arXiv:2108.01073},
  year={2021}
}

@article{zhang2024fastvideoedit,
  title={FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing},
  author={Zhang, Youyuan and Ju, Xuan and Clark, James J},
  journal={arXiv preprint arXiv:2403.06269},
  year={2024}
}

@article{geyer2023tokenflow,
  title={Tokenflow: Consistent diffusion features for consistent video editing},
  author={Geyer, Michal and Bar-Tal, Omer and Bagon, Shai and Dekel, Tali},
  journal={arXiv preprint arXiv:2307.10373},
  year={2023}
}

@article{zhang2024videoelevator,
  title={VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models},
  author={Zhang, Yabo and Wei, Yuxiang and Lin, Xianhui and Hui, Zheng and Ren, Peiran and Xie, Xuansong and Ji, Xiangyang and Zuo, Wangmeng},
  journal={arXiv preprint arXiv:2403.05438},
  year={2024}
}

@inproceedings{khachatryan2023text2video,
  title={Text2video-zero: Text-to-image diffusion models are zero-shot video generators},
  author={Khachatryan, Levon and Movsisyan, Andranik and Tadevosyan, Vahram and Henschel, Roberto and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15954--15964},
  year={2023}
}

@article{ma2024magic,
  title={Magic-Me: Identity-Specific Video Customized Diffusion},
  author={Ma, Ze and Zhou, Daquan and Yeh, Chun-Hsiao and Wang, Xue-She and Li, Xiuyu and Yang, Huanrui and Dong, Zhen and Keutzer, Kurt and Feng, Jiashi},
  journal={arXiv preprint arXiv:2402.09368},
  year={2024}
}

@article{he2024id,
  title={ID-Animator: Zero-Shot Identity-Preserving Human Video Generation},
  author={He, Xuanhua and Liu, Quande and Qian, Shengju and Wang, Xin and Hu, Tao and Cao, Ke and Yan, Keyu and Zhou, Man and Zhang, Jie},
  journal={arXiv preprint arXiv:2404.15275},
  year={2024}
}

@article{jiang2023videobooth,
  title={VideoBooth: Diffusion-based Video Generation with Image Prompts},
  author={Jiang, Yuming and Wu, Tianxing and Yang, Shuai and Si, Chenyang and Lin, Dahua and Qiao, Yu and Loy, Chen Change and Liu, Ziwei},
  journal={arXiv preprint arXiv:2312.00777},
  year={2023}
}

@article{huang2024parts,
  title={From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation},
  author={Huang, Zehuan and Fan, Hongxing and Wang, Lipeng and Sheng, Lu},
  journal={arXiv preprint arXiv:2404.15267},
  year={2024}
}

@article{wang2024customvideo,
  title={CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects},
  author={Wang, Zhao and Li, Aoxue and Xie, Enze and Zhu, Lingting and Guo, Yong and Dou, Qi and Li, Zhenguo},
  journal={arXiv preprint arXiv:2401.09962},
  year={2024}
}

@article{podell2023sdxl,
  title={Sdxl: Improving latent diffusion models for high-resolution image synthesis},
  author={Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  journal={arXiv preprint arXiv:2307.01952},
  year={2023}
}

@article{sora,
  title={Video generation models as world simulators},
  author={Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  year={2024},
  url={https://openai.com/research/video-generation-models-as-world-simulators},
}

@article{balaji2022ediff,
  title={ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers},
  author={Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, Arash and Song, Jiaming and Zhang, Qinsheng and Kreis, Karsten and Aittala, Miika and Aila, Timo and Laine, Samuli and others},
  journal={arXiv preprint arXiv:2211.01324},
  year={2022}
}

@inproceedings{rahman2023make,
  title={Make-a-story: Visual memory conditioned consistent story generation},
  author={Rahman, Tanzila and Lee, Hsin-Ying and Ren, Jian and Tulyakov, Sergey and Mahajan, Shweta and Sigal, Leonid},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2493--2502},
  year={2023}
}

@inproceedings{maharana2022storydall,
  title={Storydall-e: Adapting pretrained text-to-image transformers for story continuation},
  author={Maharana, Adyasha and Hannan, Darryl and Bansal, Mohit},
  booktitle={European Conference on Computer Vision},
  pages={70--87},
  year={2022},
  organization={Springer}
}

@article{tewel2024training,
  title={Training-Free Consistent Text-to-Image Generation},
  author={Tewel, Yoad and Kaduri, Omri and Gal, Rinon and Kasten, Yoni and Wolf, Lior and Chechik, Gal and Atzmon, Yuval},
  journal={arXiv preprint arXiv:2402.03286},
  year={2024}
}

@article{wang2024instantid,
  title={Instantid: Zero-shot identity-preserving generation in seconds},
  author={Wang, Qixun and Bai, Xu and Wang, Haofan and Qin, Zekui and Chen, Anthony},
  journal={arXiv preprint arXiv:2401.07519},
  year={2024}
}

@inproceedings{ruiz2023dreambooth,
  title={Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22500--22510},
  year={2023}
}

@inproceedings{wei2023elite,
  title={Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation},
  author={Wei, Yuxiang and Zhang, Yabo and Ji, Zhilong and Bai, Jinfeng and Zhang, Lei and Zuo, Wangmeng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15943--15953},
  year={2023}
}

@article{ye2023ipa,
  title={IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models},
  author={Ye, Hu and Zhang, Jun and Liu, Sibo and Han, Xiao and Yang, Wei},
  booktitle={arXiv preprint arxiv:2308.06721},
  year={2023}
}

@inproceedings{wu2023tune,
  title={Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation},
  author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7623--7633},
  year={2023}
}

@article{luo2024diffusion,
  title={Diffusion hyperfeatures: Searching through time and space for semantic correspondence},
  author={Luo, Grace and Dunlap, Lisa and Park, Dong Huk and Holynski, Aleksander and Darrell, Trevor},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{chang2023magicdance,
  title={Magicdance: Realistic human dance video generation with motions \& facial expressions transfer},
  author={Chang, Di and Shi, Yichun and Gao, Quankai and Fu, Jessica and Xu, Hongyi and Song, Guoxian and Yan, Qing and Yang, Xiao and Soleymani, Mohammad},
  journal={arXiv preprint arXiv:2311.12052},
  year={2023}
}

@article{avrahami2023chosen,
  title={The Chosen One: Consistent Characters in Text-to-Image Diffusion Models},
  author={Avrahami, Omri and Hertz, Amir and Vinker, Yael and Arar, Moab and Fruchter, Shlomi and Fried, Ohad and Cohen-Or, Daniel and Lischinski, Dani},
  journal={arXiv preprint arXiv:2311.10093},
  year={2023}
}

@article{feng2023improved,
  title={Improved visual story generation with adaptive context modeling},
  author={Feng, Zhangyin and Ren, Yuchen and Yu, Xinmiao and Feng, Xiaocheng and Tang, Duyu and Shi, Shuming and Qin, Bing},
  journal={arXiv preprint arXiv:2305.16811},
  year={2023}
}

@article{gal2022image,
  title={An image is worth one word: Personalizing text-to-image generation using textual inversion},
  author={Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H and Chechik, Gal and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2208.01618},
  year={2022}
}

@article{li2023photomaker,
  title={Photomaker: Customizing realistic human photos via stacked id embedding},
  author={Li, Zhen and Cao, Mingdeng and Wang, Xintao and Qi, Zhongang and Cheng, Ming-Ming and Shan, Ying},
  journal={arXiv preprint arXiv:2312.04461},
  year={2023}
}

@software{ilharco_gabriel_2021_5143773,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}

@inproceedings{Radford2021LearningTV,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={ICML},
  year={2021}
}

@misc{fu2023dreamsim,
      title={DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data}, 
      author={Stephanie Fu and Netanel Tamir and Shobhita Sundaram and Lucy Chai and Richard Zhang and Tali Dekel and Phillip Isola},
      year={2023},
      eprint={2306.09344},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{liu2023evalcrafter,
  title={Evalcrafter: Benchmarking and evaluating large video generation models},
  author={Liu, Yaofang and Cun, Xiaodong and Liu, Xuebo and Wang, Xintao and Zhang, Yong and Chen, Haoxin and Liu, Yang and Zeng, Tieyong and Chan, Raymond and Shan, Ying},
  journal={arXiv preprint arXiv:2310.11440},
  year={2023}
}

@article{hessel2021clipscore,
  title={Clipscore: A reference-free evaluation metric for image captioning},
  author={Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras, Ronan Le and Choi, Yejin},
  journal={arXiv preprint arXiv:2104.08718},
  year={2021}
}

@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}

@inproceedings{kumari2023multi,
  title={Multi-concept customization of text-to-image diffusion},
  author={Kumari, Nupur and Zhang, Bingliang and Zhang, Richard and Shechtman, Eli and Zhu, Jun-Yan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1931--1941},
  year={2023}
}

@inproceedings{tewel2023key,
  title={Key-locked rank one editing for text-to-image personalization},
  author={Tewel, Yoad and Gal, Rinon and Chechik, Gal and Atzmon, Yuval},
  booktitle={ACM SIGGRAPH 2023 Conference Proceedings},
  pages={1--11},
  year={2023}
}

@article{liu2023intelligent,
  title={Intelligent Grimm--Open-ended Visual Storytelling via Latent Diffusion Models},
  author={Liu, Chang and Wu, Haoning and Zhong, Yujie and Zhang, Xiaoyun and Xie, Weidi},
  journal={arXiv preprint arXiv:2306.00973},
  year={2023}
}

@article{li2024blip,
  title={Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing},
  author={Li, Dongxu and Li, Junnan and Hoi, Steven},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ruiz2023hyperdreambooth,
  title={Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Wei, Wei and Hou, Tingbo and Pritch, Yael and Wadhwa, Neal and Rubinstein, Michael and Aberman, Kfir},
  journal={arXiv preprint arXiv:2307.06949},
  year={2023}
}

@article{xiao2023fastcomposer,
  title={Fastcomposer: Tuning-free multi-subject image generation with localized attention},
  author={Xiao, Guangxuan and Yin, Tianwei and Freeman, William T and Durand, Fr{\'e}do and Han, Song},
  journal={arXiv preprint arXiv:2305.10431},
  year={2023}
}

@article{wu2023freeinit,
  title={Freeinit: Bridging initialization gap in video diffusion models},
  author={Wu, Tianxing and Si, Chenyang and Jiang, Yuming and Huang, Ziqi and Liu, Ziwei},
  journal={arXiv preprint arXiv:2312.07537},
  year={2023}
}

@article{yang2024fresco,
  title={FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation},
  author={Yang, Shuai and Zhou, Yifan and Liu, Ziwei and Loy, Chen Change},
  journal={arXiv preprint arXiv:2403.12962},
  year={2024}
}
@article{cong2023flatten,
  title={FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing},
  author={Cong, Yuren and Xu, Mengmeng and Simon, Christian and Chen, Shoufa and Ren, Jiawei and Xie, Yanping and Perez-Rua, Juan-Manuel and Rosenhahn, Bodo and Xiang, Tao and He, Sen},
  journal={arXiv preprint arXiv:2310.05922},
  year={2023}
}

@inproceedings{yang2023rerender,
  title={Rerender a video: Zero-shot text-guided video-to-video translation},
  author={Yang, Shuai and Zhou, Yifan and Liu, Ziwei and Loy, Chen Change},
  booktitle={SIGGRAPH Asia 2023 Conference Papers},
  pages={1--11},
  year={2023}
}

@article{ren2024consisti2v,
  title={ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation},
  author={Ren, Weiming and Yang, Harry and Zhang, Ge and Wei, Cong and Du, Xinrun and Huang, Stephen and Chen, Wenhu},
  journal={arXiv preprint arXiv:2402.04324},
  year={2024}
}

@ARTICLE{Chen2024-ag,
  title         = "{UniCtrl}: Improving the Spatiotemporal Consistency of
                   {Text-to-Video} Diffusion Models via {Training-Free} Unified
                   Attention Control",
  author        = "Chen, Xuweiyi and Xia, Tian and Xu, Sihan",
  abstract      = "Video Diffusion Models have been developed for video
                   generation, usually integrating text and image conditioning
                   to enhance control over the generated content. Despite the
                   progress, ensuring consistency across frames remains a
                   challenge, particularly when using text prompts as control
                   conditions. To address this problem, we introduce UniCtrl, a
                   novel, plug-and-play method that is universally applicable
                   to improve the spatiotemporal consistency and motion
                   diversity of videos generated by text-to-video models
                   without additional training. UniCtrl ensures semantic
                   consistency across different frames through cross-frame
                   self-attention control, and meanwhile, enhances the motion
                   quality and spatiotemporal consistency through motion
                   injection and spatiotemporal synchronization. Our
                   experimental results demonstrate UniCtrl's efficacy in
                   enhancing various text-to-video models, confirming its
                   effectiveness and universality.",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2403.02332"
}



@INPROCEEDINGS{Ronneberger2015-iw,
  title     = "{U-Net}: Convolutional Networks for Biomedical Image
               Segmentation",
  booktitle = "Medical Image Computing and {Computer-Assisted} Intervention --
               {MICCAI} 2015",
  author    = "Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas",
  abstract  = "There is large consent that successful training of deep networks
               requires many thousand annotated training samples. In this
               paper, we present a network and training strategy that relies on
               the strong use of data augmentation to use the available
               annotated samples more efficiently. The architecture consists of
               a contracting path to capture context and a symmetric expanding
               path that enables precise localization. We show that such a
               network can be trained end-to-end from very few images and
               outperforms the prior best method (a sliding-window
               convolutional network) on the ISBI challenge for segmentation of
               neuronal structures in electron microscopic stacks. Using the
               same network trained on transmitted light microscopy images
               (phase contrast and DIC) we won the ISBI cell tracking challenge
               2015 in these categories by a large margin. Moreover, the
               network is fast. Segmentation of a 512x512 image takes less than
               a second on a recent GPU. The full implementation (based on
               Caffe) and the trained networks are available at
               http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
  publisher = "Springer International Publishing",
  pages     = "234--241",
  year      =  2015
}

@misc{kim2024instantfamily,
      title={InstantFamily: Masked Attention for Zero-shot Multi-ID Image Generation}, 
      author={Chanran Kim and Jeongin Lee and Shichang Joung and Bongmo Kim and Yeul-Min Baek},
      year={2024},
      eprint={2404.19427},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@ARTICLE{Vaswani2017-hk,
  title    = "Attention is All you Need",
  author   = "Vaswani, Ashish and Shazeer, Noam M and Parmar, Niki and
              Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser,
              Lukasz and Polosukhin, Illia",
  abstract = "The dominant sequence transduction models are based on complex
              recurrent or convolutional neural networks in an encoder-decoder
              configuration. The best performing models also connect the
              encoder and decoder through an attention mechanism. We propose a
              new simple network architecture, the Transformer, based solely on
              attention mechanisms, dispensing with recurrence and convolutions
              entirely. Experiments on two machine translation tasks show these
              models to be superior in quality while being more parallelizable
              and requiring significantly less time to train. Our model
              achieves 28.4 BLEU on the WMT 2014 English-to-German translation
              task, improving over the existing best results, including
              ensembles by over 2 BLEU. On the WMT 2014 English-to-French
              translation task, our model establishes a new single-model
              state-of-the-art BLEU score of 41.8 after training for 3.5 days
              on eight GPUs, a small fraction of the training costs of the best
              models from the literature. We show that the Transformer
              generalizes well to other tasks by applying it successfully to
              English constituency parsing both with large and limited training
              data.",
  journal  = "Adv. Neural Inf. Process. Syst.",
  pages    = "5998--6008",
  month    =  jun,
  year     =  2017
}


@misc{blattmann2023stable,
      title={Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets}, 
      author={Andreas Blattmann and Tim Dockhorn and Sumith Kulal and Daniel Mendelevitch and Maciej Kilian and Dominik Lorenz and Yam Levi and Zion English and Vikram Voleti and Adam Letts and Varun Jampani and Robin Rombach},
      year={2023},
      eprint={2311.15127},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{guo2023animatediff,
  title={Animatediff: Animate your personalized text-to-image diffusion models without specific tuning},
  author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Wang, Yaohui and Qiao, Yu and Lin, Dahua and Dai, Bo},
  journal={arXiv preprint arXiv:2307.04725},
  year={2023}
}


@ARTICLE{Hu2023-wr,
  title         = "Animate Anyone: Consistent and Controllable {Image-to-Video}
                   Synthesis for Character Animation",
  author        = "Hu, Li and Gao, Xin and Zhang, Peng and Sun, Ke and Zhang,
                   Bang and Bo, Liefeng",
  abstract      = "Character Animation aims to generating character videos from
                   still images through driving signals. Currently, diffusion
                   models have become the mainstream in visual generation
                   research, owing to their robust generative capabilities.
                   However, challenges persist in the realm of image-to-video,
                   especially in character animation, where temporally
                   maintaining consistency with detailed information from
                   character remains a formidable problem. In this paper, we
                   leverage the power of diffusion models and propose a novel
                   framework tailored for character animation. To preserve
                   consistency of intricate appearance features from reference
                   image, we design ReferenceNet to merge detail features via
                   spatial attention. To ensure controllability and continuity,
                   we introduce an efficient pose guider to direct character's
                   movements and employ an effective temporal modeling approach
                   to ensure smooth inter-frame transitions between video
                   frames. By expanding the training data, our approach can
                   animate arbitrary characters, yielding superior results in
                   character animation compared to other image-to-video
                   methods. Furthermore, we evaluate our method on benchmarks
                   for fashion video and human dance synthesis, achieving
                   state-of-the-art results.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2311.17117"
}

@inproceedings{ge2023preserve,
  title={Preserve your own correlation: A noise prior for video diffusion models},
  author={Ge, Songwei and Nah, Seungjun and Liu, Guilin and Poon, Tyler and Tao, Andrew and Catanzaro, Bryan and Jacobs, David and Huang, Jia-Bin and Liu, Ming-Yu and Balaji, Yogesh},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={22930--22941},
  year={2023}
}
@article{he2022latent,
  title={Latent video diffusion models for high-fidelity long video generation},
  author={He, Yingqing and Yang, Tianyu and Zhang, Yong and Shan, Ying and Chen, Qifeng},
  journal={arXiv preprint arXiv:2211.13221},
  year={2022}
}

@misc{rombach2022highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj√∂rn Ommer},
      year={2022},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{chen2023videocrafter1,
      title={VideoCrafter1: Open Diffusion Models for High-Quality Video Generation}, 
      author={Haoxin Chen and Menghan Xia and Yingqing He and Yong Zhang and Xiaodong Cun and Shaoshu Yang and Jinbo Xing and Yaofang Liu and Qifeng Chen and Xintao Wang and Chao Weng and Ying Shan},
      year={2023},
      eprint={2310.19512},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{girdhar2023emu,
      title={Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning}, 
      author={Rohit Girdhar and Mannat Singh and Andrew Brown and Quentin Duval and Samaneh Azadi and Sai Saketh Rambhatla and Akbar Shah and Xi Yin and Devi Parikh and Ishan Misra},
      year={2023},
      eprint={2311.10709},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{hong2022cogvideo,
      title={CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers}, 
      author={Wenyi Hong and Ming Ding and Wendi Zheng and Xinghan Liu and Jie Tang},
      year={2022},
      eprint={2205.15868},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{singer2022makeavideo,
      title={Make-A-Video: Text-to-Video Generation without Text-Video Data}, 
      author={Uriel Singer and Adam Polyak and Thomas Hayes and Xi Yin and Jie An and Songyang Zhang and Qiyuan Hu and Harry Yang and Oron Ashual and Oran Gafni and Devi Parikh and Sonal Gupta and Yaniv Taigman},
      year={2022},
      eprint={2209.14792},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{song2022denoising,
      title={Denoising Diffusion Implicit Models}, 
      author={Jiaming Song and Chenlin Meng and Stefano Ermon},
      year={2022},
      eprint={2010.02502},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{mokady2022nulltext,
      title={Null-text Inversion for Editing Real Images using Guided Diffusion Models}, 
      author={Ron Mokady and Amir Hertz and Kfir Aberman and Yael Pritch and Daniel Cohen-Or},
      year={2022},
      eprint={2211.09794},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@software{diffusers,
author = {von Platen, Patrick and Patil, Suraj and Lozhkov, Anton and Cuenca, Pedro and Lambert, Nathan and Rasul, Kashif and Davaadorj, Mishig and Nair, Dhruv and Paul, Sayak and Liu, Steven and Berman, William and Xu, Yiyi and Wolf, Thomas},
license = {Apache-2.0},
title = {{Diffusers: State-of-the-art diffusion models}},
url = {https://github.com/huggingface/diffusers},
version = {0.12.1}
}

@misc{zhou2024storydiffusion,
      title={StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation}, 
      author={Yupeng Zhou and Daquan Zhou and Ming-Ming Cheng and Jiashi Feng and Qibin Hou},
      year={2024},
      eprint={2405.01434},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}



@article{makeavideo,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  journal={arXiv preprint arXiv:2209.14792},
  year={2022}
}

@article{vdm,
  title={Video diffusion models},
  author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8633--8646},
  year={2022}
}


@article{svd,
  title={Stable video diffusion: Scaling latent video diffusion models to large datasets},
  author={Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and others},
  journal={arXiv preprint arXiv:2311.15127},
  year={2023}
}

@article{bar2024lumiere,
  title={Lumiere: A space-time diffusion model for video generation},
  author={Bar-Tal, Omer and Chefer, Hila and Tov, Omer and Herrmann, Charles and Paiss, Roni and Zada, Shiran and Ephrat, Ariel and Hur, Junhwa and Li, Yuanzhen and Michaeli, Tomer and others},
  journal={arXiv preprint arXiv:2401.12945},
  year={2024}
}

@inproceedings{blattmann2023align,
  title={Align your latents: High-resolution video synthesis with latent diffusion models},
  author={Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22563--22575},
  year={2023}
}


@article{pan2023kosmos,
  title={Kosmos-g: Generating images in context with multimodal large language models},
  author={Pan, Xichen and Dong, Li and Huang, Shaohan and Peng, Zhiliang and Chen, Wenhu and Wei, Furu},
  journal={arXiv preprint arXiv:2310.02992},
  year={2023}
}


% story diffusion model
@inproceedings{pan2024synthesizing,
  title={Synthesizing coherent story with auto-regressive latent diffusion models},
  author={Pan, Xichen and Qin, Pengda and Li, Yuhong and Xue, Hui and Chen, Wenhu},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2920--2930},
  year={2024}
}

@article{jeong2023zero,
  title={Zero-shot generation of coherent storybook from plain text story using diffusion models},
  author={Jeong, Hyeonho and Kwon, Gihyun and Ye, Jong Chul},
  journal={arXiv preprint arXiv:2302.03900},
  year={2023}
}