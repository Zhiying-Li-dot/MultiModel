# 第二章：核心架构

## 2.1 视觉编码器

### Vision Transformer (ViT)

ViT是目前多模态模型中最常用的视觉编码器。

```
输入图像 (224×224×3)
        │
        ▼
┌─────────────────────────────┐
│  分割成patches (16×16)       │
│  得到 14×14 = 196 个patch    │
└──────────────┬──────────────┘
               ▼
┌─────────────────────────────┐
│  线性投影: patch → embedding │
│  每个patch变成768维向量      │
└──────────────┬──────────────┘
               ▼
┌─────────────────────────────┐
│  添加位置编码 + [CLS] token  │
│  序列长度: 197               │
└──────────────┬──────────────┘
               ▼
┌─────────────────────────────┐
│  N层 Transformer Encoder    │
│  自注意力 + FFN              │
└──────────────┬──────────────┘
               ▼
        [CLS] 输出作为图像表示
```

### 核心代码实现

```python
class ViT(nn.Module):
    def __init__(self, image_size=224, patch_size=16, dim=768, depth=12):
        super().__init__()
        num_patches = (image_size // patch_size) ** 2
        patch_dim = 3 * patch_size ** 2

        # Patch Embedding
        self.patch_embed = nn.Sequential(
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
        )

        # CLS token 和位置编码
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, dim))

        # Transformer layers
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=dim, nhead=12),
            num_layers=depth
        )

    def forward(self, img):
        # img: (B, 3, 224, 224)
        patches = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)',
                           p1=16, p2=16)
        x = self.patch_embed(patches)

        # 添加CLS token
        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)

        # 添加位置编码
        x = x + self.pos_embed

        # Transformer
        x = self.transformer(x)

        return x[:, 0]  # 返回CLS token
```

### 常用视觉编码器对比

| 编码器 | 训练方式 | 特点 | 使用模型 |
|--------|----------|------|----------|
| **CLIP ViT** | 对比学习 | 视觉-语言对齐好 | LLaVA, OpenFlamingo |
| **SigLIP** | Sigmoid对比 | 更大batch，效果更好 | PaliGemma, Gemini |
| **EVA** | MIM + CLIP | 视觉理解能力强 | BLIP-2, InternVL |
| **DINOv2** | 自监督 | 局部特征好 | 部分研究工作 |

### CLIP视觉编码器

- 基于ViT架构
- 在4亿图文对上用对比学习训练
- 输出的特征天然具有语义信息
- 是目前最流行的选择

### SigLIP

- CLIP的改进版
- 使用Sigmoid损失替代Softmax
- 不需要全局负样本，支持更大batch size
- 被Gemini、PaliGemma等模型采用

---

## 2.2 语言模型

语言模型作为多模态模型的"大脑"，负责理解和生成文本。

### 常用语言模型

| 模型 | 参数量 | 特点 |
|------|--------|------|
| LLaMA/LLaMA2/3 | 7B-70B | 开源、社区活跃 |
| Qwen | 1.8B-72B | 中文能力强 |
| Vicuna | 7B-13B | 对话能力好 |
| Mistral | 7B | 效率高 |
| GPT系列 | 未知 | 闭源、能力最强 |

### 为什么选择Decoder-only架构

1. **统一的生成范式**：所有任务都转化为生成
2. **便于扩展**：视觉token和文本token统一处理
3. **更好的few-shot能力**：自回归天然支持上下文学习
4. **规模效应**：scaling law在decoder-only上验证最充分

### 语言模型在多模态中的角色

```
输入序列: [图像tokens] [文本tokens]
         ↓
      语言模型
         ↓
输出序列: [生成的文本tokens]
```

语言模型需要：
- 理解视觉token携带的语义
- 结合视觉和文本信息进行推理
- 生成符合指令的回答

---

## 2.3 连接模块

连接模块是多模态模型中**最关键的设计**，负责将视觉特征"翻译"成语言模型能理解的表示。

### 方案1: Linear Projection (LLaVA)

```
视觉编码器输出          连接模块              语言模型输入
[v₁, v₂, ..., vₙ]  ──▶  W·vᵢ + b  ──▶  [v'₁, v'₂, ..., v'ₙ]
   (N×1024)           线性变换            (N×4096)
```

**实现代码**：
```python
class LinearProjector(nn.Module):
    def __init__(self, vision_dim=1024, llm_dim=4096):
        super().__init__()
        self.proj = nn.Sequential(
            nn.Linear(vision_dim, llm_dim),
            nn.GELU(),
            nn.Linear(llm_dim, llm_dim)
        )

    def forward(self, vision_features):
        return self.proj(vision_features)
```

**特点**：
- 最简单：一个线性层或两层MLP
- 优点：参数少、训练快
- 缺点：视觉token数量多（如576个），占用大量上下文

### 方案2: Q-Former (BLIP-2)

```
                    ┌─────────────────┐
Learnable Queries   │                 │
[q₁, q₂, ..., q₃₂] ─│   Q-Former      │
      (32个)        │  (交叉注意力)    │──▶ 32个输出token
                    │                 │
视觉特征            │                 │
[v₁, v₂, ..., vₙ]  ─│                 │
    (256个)         └─────────────────┘
```

**实现原理**：
```python
class QFormer(nn.Module):
    def __init__(self, num_queries=32, vision_dim=1024, hidden_dim=768):
        super().__init__()
        self.queries = nn.Parameter(torch.randn(1, num_queries, hidden_dim))
        self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads=12)
        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads=12)

    def forward(self, vision_features):
        # vision_features: (B, N, D)
        queries = self.queries.expand(vision_features.shape[0], -1, -1)

        # 交叉注意力：queries查询视觉特征
        queries = self.cross_attn(queries, vision_features, vision_features)

        # 自注意力
        queries = self.self_attn(queries, queries, queries)

        return queries  # (B, 32, D)
```

**特点**：
- 使用可学习的query去"提问"视觉特征
- **压缩**：将几百个视觉token压缩为32个
- 优点：大幅减少序列长度
- 缺点：可能丢失细节信息

### 方案3: Perceiver Resampler (Flamingo)

类似Q-Former，但结构更灵活：
- 使用交叉注意力层
- 可处理任意长度输入
- 支持多图像交错输入

### 方案4: 动态分辨率

```
高分辨率图像 (1344×896)
        │
        ▼
┌─────────────────────────┐
│ 切分成多个子图 + 缩略图  │
│                         │
│  ┌───┬───┬───┐          │
│  │ 1 │ 2 │ 3 │ 子图      │
│  ├───┼───┼───┤          │
│  │ 4 │ 5 │ 6 │          │
│  └───┴───┴───┘          │
│       + 缩略图           │
└───────────┬─────────────┘
            ▼
每个子图独立编码后拼接
```

---

## 2.4 典型架构对比

### LLaVA架构

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│ CLIP ViT-L  │ ──▶ │  MLP投影层   │ ──▶ │   Vicuna    │
│  视觉编码器  │     │  (2层线性)   │     │   语言模型   │
└─────────────┘     └─────────────┘     └─────────────┘
```

**特点**：
- 简单有效，易于复现
- 两阶段训练：预训练对齐 + 指令微调
- 视觉token数：576 (24×24)

### BLIP-2架构

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│  EVA-CLIP   │ ──▶ │  Q-Former   │ ──▶ │  FlanT5/    │
│ (冻结)      │     │ (可训练)    │     │  Vicuna     │
└─────────────┘     └─────────────┘     └─────────────┘
```

**特点**：
- 冻结视觉和语言模型，只训练Q-Former
- 参数效率高
- 视觉token数：32（大幅压缩）

### Flamingo架构

```
┌─────────────────────────────────────────┐
│             语言模型层                   │
│  ┌───────────────────────────────────┐  │
│  │        Self-Attention             │  │
│  └───────────────────────────────────┘  │
│  ┌───────────────────────────────────┐  │
│  │  Gated Cross-Attention (新增)     │◀─┼── 视觉特征
│  └───────────────────────────────────┘  │
│  ┌───────────────────────────────────┐  │
│  │           FFN                     │  │
│  └───────────────────────────────────┘  │
└─────────────────────────────────────────┘
```

**特点**：
- 在LLM中插入交叉注意力层
- 支持多图像交错输入
- few-shot能力强

### 架构对比总结

| 架构 | 连接方式 | 视觉token数 | 训练策略 |
|------|----------|-------------|----------|
| LLaVA | MLP投影 | 576 | 全参数微调 |
| BLIP-2 | Q-Former | 32 | 只训练Q-Former |
| Flamingo | 交叉注意力 | 可变 | 只训练新增层 |
| InternVL | 动态分辨率 | 动态 | 全参数训练 |

---

## 总结

多模态大模型的核心架构 = 视觉编码器 + 连接模块 + 语言模型

```
┌────────────┐    ┌────────────┐    ┌────────────┐
│   Vision   │    │  Connector │    │    LLM     │
│  Encoder   │ ──▶│  (Bridge)  │ ──▶│  (Brain)   │
│            │    │            │    │            │
│ 提取视觉特征│    │ 模态对齐转换│    │ 理解与生成 │
└────────────┘    └────────────┘    └────────────┘
```

选择哪种架构取决于：
- 计算资源限制
- 任务需求（细粒度理解 vs 快速响应）
- 训练数据规模
