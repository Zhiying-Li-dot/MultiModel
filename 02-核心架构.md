# 第二章：核心架构

多模态大模型的核心架构由三个关键组件构成：**视觉编码器**、**连接模块**、**语言模型**。本章将深入剖析每个组件的设计原理、实现细节和演进历程。

---

## 2.1 视觉编码器

视觉编码器负责将图像转换为语义丰富的特征表示，是多模态理解的基础。

### 2.1.1 Vision Transformer (ViT) 详解

#### 核心思想

ViT的革命性在于：**将图像视为一系列patch序列，直接用Transformer处理**，摒弃了CNN的归纳偏置（locality, translation equivariance）。

#### 完整架构图

```
输入图像 (224×224×3)
        │
        ▼
┌─────────────────────────────────────────────────────────┐
│                    Patch Embedding                       │
│  ┌─────────────────────────────────────────────────┐    │
│  │  1. 将图像分割为16×16的patches                   │    │
│  │     224÷16 = 14, 共14×14 = 196个patches         │    │
│  │                                                  │    │
│  │  2. 每个patch展平: 16×16×3 = 768维向量           │    │
│  │                                                  │    │
│  │  3. 线性投影到hidden_dim (如768或1024)           │    │
│  └─────────────────────────────────────────────────┘    │
└──────────────────────────┬──────────────────────────────┘
                           ▼
┌─────────────────────────────────────────────────────────┐
│                   添加特殊Token                          │
│  ┌─────────────────────────────────────────────────┐    │
│  │  [CLS] + [Patch_1, Patch_2, ..., Patch_196]     │    │
│  │                                                  │    │
│  │  序列长度: 1 + 196 = 197                         │    │
│  └─────────────────────────────────────────────────┘    │
└──────────────────────────┬──────────────────────────────┘
                           ▼
┌─────────────────────────────────────────────────────────┐
│                    位置编码                              │
│  ┌─────────────────────────────────────────────────┐    │
│  │  可学习的1D位置编码: (197, hidden_dim)           │    │
│  │  X = X + pos_embed                              │    │
│  └─────────────────────────────────────────────────┘    │
└──────────────────────────┬──────────────────────────────┘
                           ▼
┌─────────────────────────────────────────────────────────┐
│              Transformer Encoder × N层                   │
│  ┌─────────────────────────────────────────────────┐    │
│  │  每层包含:                                       │    │
│  │  ├── Layer Norm                                 │    │
│  │  ├── Multi-Head Self-Attention                  │    │
│  │  ├── Residual Connection                        │    │
│  │  ├── Layer Norm                                 │    │
│  │  ├── MLP (FFN)                                  │    │
│  │  └── Residual Connection                        │    │
│  └─────────────────────────────────────────────────┘    │
└──────────────────────────┬──────────────────────────────┘
                           ▼
┌─────────────────────────────────────────────────────────┐
│                      输出选择                            │
│  方式1: [CLS] token → 全局图像表示                       │
│  方式2: 所有patch token → 保留空间信息 (多模态常用)       │
└─────────────────────────────────────────────────────────┘
```

#### 完整代码实现

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat

class PatchEmbedding(nn.Module):
    """将图像分割为patches并嵌入"""

    def __init__(self, image_size=224, patch_size=16, in_channels=3, embed_dim=768):
        super().__init__()
        self.image_size = image_size
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2

        # 方式1: 使用卷积实现patch embedding（更高效）
        self.proj = nn.Conv2d(
            in_channels, embed_dim,
            kernel_size=patch_size, stride=patch_size
        )

        # 方式2: 使用线性层（概念上更清晰）
        # self.proj = nn.Linear(patch_size * patch_size * in_channels, embed_dim)

    def forward(self, x):
        """
        x: (B, C, H, W)
        return: (B, num_patches, embed_dim)
        """
        # 卷积方式: (B, C, H, W) -> (B, D, H/P, W/P) -> (B, D, N) -> (B, N, D)
        x = self.proj(x)  # (B, embed_dim, 14, 14)
        x = x.flatten(2)   # (B, embed_dim, 196)
        x = x.transpose(1, 2)  # (B, 196, embed_dim)
        return x


class MultiHeadSelfAttention(nn.Module):
    """多头自注意力机制"""

    def __init__(self, embed_dim=768, num_heads=12, dropout=0.0, bias=True):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.scale = self.head_dim ** -0.5

        # QKV投影
        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=bias)
        self.proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        x: (B, N, D)
        return: (B, N, D)
        """
        B, N, D = x.shape

        # 计算QKV
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, head_dim)
        q, k, v = qkv[0], qkv[1], qkv[2]

        # 注意力分数
        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, heads, N, N)

        if mask is not None:
            attn = attn.masked_fill(mask == 0, float('-inf'))

        attn = attn.softmax(dim=-1)
        attn = self.dropout(attn)

        # 加权求和
        x = (attn @ v).transpose(1, 2).reshape(B, N, D)
        x = self.proj(x)

        return x


class MLP(nn.Module):
    """前馈神经网络"""

    def __init__(self, embed_dim=768, hidden_dim=3072, dropout=0.0):
        super().__init__()
        self.fc1 = nn.Linear(embed_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, embed_dim)
        self.act = nn.GELU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        return x


class TransformerEncoderBlock(nn.Module):
    """Transformer编码器块"""

    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.0):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout)

    def forward(self, x):
        # Pre-Norm架构
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x


class VisionTransformer(nn.Module):
    """完整的Vision Transformer"""

    def __init__(
        self,
        image_size=224,
        patch_size=16,
        in_channels=3,
        embed_dim=768,
        depth=12,
        num_heads=12,
        mlp_ratio=4.0,
        dropout=0.0,
        num_classes=1000,  # 分类任务用
    ):
        super().__init__()

        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)
        num_patches = self.patch_embed.num_patches

        # [CLS] token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))

        # 位置编码
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))

        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout)
            for _ in range(depth)
        ])

        self.norm = nn.LayerNorm(embed_dim)

        # 分类头（多模态任务通常不需要）
        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()

        # 初始化
        self._init_weights()

    def _init_weights(self):
        # 位置编码初始化
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)

    def forward_features(self, x):
        """提取特征（多模态任务使用此方法）"""
        B = x.shape[0]

        # Patch embedding
        x = self.patch_embed(x)  # (B, N, D)

        # 添加[CLS] token
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # (B, N+1, D)

        # 添加位置编码
        x = x + self.pos_embed

        # Transformer blocks
        for block in self.blocks:
            x = block(x)

        x = self.norm(x)

        return x

    def forward(self, x, return_all_tokens=False):
        """
        return_all_tokens=False: 返回[CLS] token (分类任务)
        return_all_tokens=True: 返回所有token (多模态任务)
        """
        x = self.forward_features(x)

        if return_all_tokens:
            return x[:, 1:]  # 返回所有patch tokens, 不含[CLS]
        else:
            return self.head(x[:, 0])  # 返回[CLS]经过分类头


# ViT变体配置
VIT_CONFIGS = {
    'vit_base_patch16': {
        'patch_size': 16, 'embed_dim': 768, 'depth': 12, 'num_heads': 12
    },
    'vit_large_patch14': {
        'patch_size': 14, 'embed_dim': 1024, 'depth': 24, 'num_heads': 16
    },
    'vit_huge_patch14': {
        'patch_size': 14, 'embed_dim': 1280, 'depth': 32, 'num_heads': 16
    },
}
```

### 2.1.2 CLIP视觉编码器

CLIP (Contrastive Language-Image Pre-training) 使用对比学习训练视觉编码器，使其输出的特征天然对齐文本语义。

#### CLIP训练原理

```
┌─────────────────────────────────────────────────────────────────┐
│                    CLIP 对比学习训练                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Batch: N个图文对 {(I₁,T₁), (I₂,T₂), ..., (Iₙ,Tₙ)}             │
│                                                                 │
│  ┌──────────────┐                    ┌──────────────┐          │
│  │  Image       │                    │  Text        │          │
│  │  Encoder     │                    │  Encoder     │          │
│  │  (ViT)       │                    │  (Transformer)│          │
│  └──────┬───────┘                    └──────┬───────┘          │
│         │                                   │                   │
│         ▼                                   ▼                   │
│  ┌──────────────┐                    ┌──────────────┐          │
│  │  I₁ → v₁     │                    │  T₁ → t₁     │          │
│  │  I₂ → v₂     │                    │  T₂ → t₂     │          │
│  │  ...         │                    │  ...         │          │
│  │  Iₙ → vₙ     │                    │  Tₙ → tₙ     │          │
│  └──────┬───────┘                    └──────┬───────┘          │
│         │                                   │                   │
│         └─────────────┬─────────────────────┘                   │
│                       ▼                                         │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              相似度矩阵 S = V · T^T                      │   │
│  │                                                         │   │
│  │         t₁    t₂    t₃   ...   tₙ                       │   │
│  │    v₁ [ ✓     ✗     ✗         ✗  ]                      │   │
│  │    v₂ [ ✗     ✓     ✗         ✗  ]                      │   │
│  │    v₃ [ ✗     ✗     ✓         ✗  ]                      │   │
│  │    ...                                                   │   │
│  │    vₙ [ ✗     ✗     ✗         ✓  ]                      │   │
│  │                                                         │   │
│  │    对角线为正样本，其余为负样本                           │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  损失函数: L = (L_i2t + L_t2i) / 2                             │
│  其中: L_i2t = CrossEntropy(S / τ, labels)                     │
│       L_t2i = CrossEntropy(S^T / τ, labels)                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### CLIP实现代码

```python
class CLIPVisionEncoder(nn.Module):
    """CLIP视觉编码器"""

    def __init__(self, vit_config, projection_dim=512):
        super().__init__()
        self.vit = VisionTransformer(**vit_config, num_classes=0)

        # 投影到共享的embedding空间
        self.proj = nn.Linear(vit_config['embed_dim'], projection_dim)

    def forward(self, images, normalize=True):
        """
        images: (B, C, H, W)
        return: (B, projection_dim)
        """
        # 获取[CLS] token特征
        features = self.vit.forward_features(images)[:, 0]

        # 投影
        features = self.proj(features)

        # L2归一化（用于余弦相似度）
        if normalize:
            features = F.normalize(features, dim=-1)

        return features


class CLIPTextEncoder(nn.Module):
    """CLIP文本编码器"""

    def __init__(self, vocab_size, embed_dim=512, num_layers=12,
                 num_heads=8, max_length=77):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_embedding = nn.Parameter(torch.zeros(1, max_length, embed_dim))

        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads),
            num_layers=num_layers
        )

        self.ln_final = nn.LayerNorm(embed_dim)

    def forward(self, text_tokens, normalize=True):
        """
        text_tokens: (B, L)
        return: (B, embed_dim)
        """
        x = self.token_embedding(text_tokens) + self.pos_embedding[:, :text_tokens.shape[1]]

        # Causal mask for autoregressive
        mask = torch.triu(torch.ones(x.shape[1], x.shape[1]), diagonal=1).bool()
        x = self.transformer(x.transpose(0, 1), src_mask=mask).transpose(0, 1)

        x = self.ln_final(x)

        # 取[EOS] token的特征作为句子表示
        # 假设[EOS]在每个序列的最后
        x = x[torch.arange(x.shape[0]), text_tokens.argmax(dim=-1)]

        if normalize:
            x = F.normalize(x, dim=-1)

        return x


class CLIP(nn.Module):
    """完整的CLIP模型"""

    def __init__(self, vision_config, text_config, projection_dim=512, temperature=0.07):
        super().__init__()
        self.visual = CLIPVisionEncoder(vision_config, projection_dim)
        self.text = CLIPTextEncoder(**text_config)
        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1 / temperature)))

    def forward(self, images, texts):
        image_features = self.visual(images)
        text_features = self.text(texts)

        # 计算相似度
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.t()
        logits_per_text = logits_per_image.t()

        return logits_per_image, logits_per_text

    def contrastive_loss(self, logits_per_image, logits_per_text):
        batch_size = logits_per_image.shape[0]
        labels = torch.arange(batch_size, device=logits_per_image.device)

        loss_i2t = F.cross_entropy(logits_per_image, labels)
        loss_t2i = F.cross_entropy(logits_per_text, labels)

        return (loss_i2t + loss_t2i) / 2
```

### 2.1.3 SigLIP: CLIP的改进版

SigLIP使用Sigmoid损失替代Softmax，每个图文对独立计算，不需要全局负样本。

```python
class SigLIPLoss(nn.Module):
    """SigLIP损失函数"""

    def __init__(self, temperature=10.0, bias=-10.0):
        super().__init__()
        self.temperature = temperature
        self.bias = bias

    def forward(self, image_features, text_features):
        """
        image_features: (B, D) 归一化的图像特征
        text_features: (B, D) 归一化的文本特征
        """
        # 计算相似度矩阵
        logits = image_features @ text_features.t() * self.temperature + self.bias

        # 标签矩阵：对角线为1，其余为-1
        batch_size = logits.shape[0]
        labels = 2 * torch.eye(batch_size, device=logits.device) - 1

        # Sigmoid损失：每个元素独立
        loss = -F.logsigmoid(labels * logits).mean()

        return loss
```

**SigLIP vs CLIP对比**：

| 特性 | CLIP (Softmax) | SigLIP (Sigmoid) |
|------|----------------|------------------|
| 负样本 | 需要batch内所有负样本 | 每对独立计算 |
| Batch Size | 越大越好(32K) | 可以更小 |
| 分布式训练 | 需要all-gather | 更容易并行 |
| 效果 | 基准 | 略好于CLIP |

### 2.1.4 EVA: 结合MIM和CLIP

EVA (Exploring the Limits of Masked Visual Representation Learning at Scale) 结合了掩码图像建模(MIM)和CLIP目标。

```
EVA训练流程：

阶段1: Masked Image Modeling (MIM)
┌─────────────────────────────────────────────────────────────┐
│  1. 随机掩盖75%的patches                                     │
│  2. 用Encoder处理未掩盖的patches                             │
│  3. 用Decoder重建被掩盖的patches                             │
│  4. 重建目标：CLIP图像特征（而非像素）                        │
└─────────────────────────────────────────────────────────────┘

阶段2: CLIP Contrastive Learning
┌─────────────────────────────────────────────────────────────┐
│  在MIM预训练基础上，继续对比学习                              │
└─────────────────────────────────────────────────────────────┘
```

### 2.1.5 视觉编码器对比总结

| 编码器 | 预训练方式 | 特点 | Token数量 | 使用模型 |
|--------|-----------|------|-----------|----------|
| **ViT-B/16** | ImageNet监督 | 基础模型 | 196 | 早期研究 |
| **CLIP ViT-L/14** | 对比学习 | 语义对齐好 | 256 | LLaVA, Flamingo |
| **CLIP ViT-L/14@336** | 对比学习 | 更高分辨率 | 576 | LLaVA-1.5 |
| **SigLIP-SO400M** | Sigmoid对比 | 更强对齐 | 729 | PaliGemma |
| **EVA-CLIP** | MIM + 对比 | 视觉理解强 | 256 | BLIP-2, InternVL |
| **DINOv2** | 自监督 | 局部特征好 | 256 | 研究用途 |

---

## 2.2 语言模型

语言模型作为多模态系统的"大脑"，负责理解多模态信息并生成回复。

### 2.2.1 为什么选择Decoder-only架构

```
三种Transformer架构对比：

┌──────────────────────────────────────────────────────────────────┐
│  Encoder-only (BERT风格)                                        │
│  ┌──────────────────────────────────────────────────────┐       │
│  │  [CLS] T₁ T₂ T₃ [SEP]                                │       │
│  │    ↓   ↓  ↓  ↓   ↓                                   │       │
│  │  双向注意力：每个token可以看到所有其他token            │       │
│  │                                                      │       │
│  │  适合：理解任务（分类、NER等）                        │       │
│  │  不适合：生成任务                                     │       │
│  └──────────────────────────────────────────────────────┘       │
│                                                                  │
│  Encoder-Decoder (T5风格)                                       │
│  ┌──────────────────────────────────────────────────────┐       │
│  │  Encoder: 输入序列 → 双向注意力                       │       │
│  │  Decoder: 输出序列 → 单向注意力 + 交叉注意力          │       │
│  │                                                      │       │
│  │  适合：Seq2Seq任务（翻译、摘要）                      │       │
│  │  缺点：两套参数，效率较低                             │       │
│  └──────────────────────────────────────────────────────┘       │
│                                                                  │
│  Decoder-only (GPT风格) ← 多模态首选                            │
│  ┌──────────────────────────────────────────────────────┐       │
│  │  T₁ T₂ T₃ T₄ ...                                     │       │
│  │   ↓  ↓  ↓  ↓                                         │       │
│  │  单向注意力：每个token只能看到之前的tokens            │       │
│  │                                                      │       │
│  │  优点：                                              │       │
│  │  - 统一的生成范式                                    │       │
│  │  - 支持in-context learning                          │       │
│  │  - Scaling law验证充分                              │       │
│  │  - 视觉token可以自然地前置                           │       │
│  └──────────────────────────────────────────────────────┘       │
└──────────────────────────────────────────────────────────────────┘
```

### 2.2.2 常用语言模型详解

#### LLaMA系列架构

```python
class LlamaConfig:
    """LLaMA配置"""
    vocab_size: int = 32000
    hidden_size: int = 4096
    intermediate_size: int = 11008
    num_hidden_layers: int = 32
    num_attention_heads: int = 32
    num_key_value_heads: int = 32  # GQA: Grouped Query Attention
    max_position_embeddings: int = 4096
    rms_norm_eps: float = 1e-6
    rope_theta: float = 10000.0


class LlamaRMSNorm(nn.Module):
    """RMS Normalization (比LayerNorm更简单高效)"""

    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.eps = eps

    def forward(self, x):
        variance = x.pow(2).mean(-1, keepdim=True)
        x = x * torch.rsqrt(variance + self.eps)
        return self.weight * x


class LlamaRotaryEmbedding(nn.Module):
    """RoPE旋转位置编码"""

    def __init__(self, dim, max_position=4096, base=10000):
        super().__init__()
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq)

        # 预计算
        t = torch.arange(max_position)
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos())
        self.register_buffer("sin_cached", emb.sin())

    def forward(self, x, seq_len):
        return (
            self.cos_cached[:seq_len].to(x.dtype),
            self.sin_cached[:seq_len].to(x.dtype),
        )


def rotate_half(x):
    """RoPE辅助函数"""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2:]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    """应用RoPE"""
    cos = cos[position_ids].unsqueeze(1)
    sin = sin[position_ids].unsqueeze(1)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


class LlamaAttention(nn.Module):
    """LLaMA注意力层（支持GQA）"""

    def __init__(self, config):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads

        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)

        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim)

    def forward(self, hidden_states, attention_mask=None, position_ids=None):
        B, L, _ = hidden_states.shape

        # QKV投影
        query = self.q_proj(hidden_states)
        key = self.k_proj(hidden_states)
        value = self.v_proj(hidden_states)

        # 重塑为多头
        query = query.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        key = key.view(B, L, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value = value.view(B, L, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        # 应用RoPE
        cos, sin = self.rotary_emb(hidden_states, L)
        query, key = apply_rotary_pos_emb(query, key, cos, sin, position_ids)

        # GQA: 扩展key和value
        key = key.repeat_interleave(self.num_key_value_groups, dim=1)
        value = value.repeat_interleave(self.num_key_value_groups, dim=1)

        # 注意力计算
        attn_weights = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)

        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask

        attn_weights = F.softmax(attn_weights, dim=-1)
        attn_output = torch.matmul(attn_weights, value)

        attn_output = attn_output.transpose(1, 2).reshape(B, L, self.hidden_size)
        return self.o_proj(attn_output)


class LlamaMLP(nn.Module):
    """LLaMA MLP (SwiGLU激活)"""

    def __init__(self, config):
        super().__init__()
        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)

    def forward(self, x):
        # SwiGLU: gate * up
        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))


class LlamaDecoderLayer(nn.Module):
    """LLaMA解码器层"""

    def __init__(self, config):
        super().__init__()
        self.self_attn = LlamaAttention(config)
        self.mlp = LlamaMLP(config)
        self.input_layernorm = LlamaRMSNorm(config.hidden_size, config.rms_norm_eps)
        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, config.rms_norm_eps)

    def forward(self, hidden_states, attention_mask=None, position_ids=None):
        # Self Attention with Pre-Norm
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        hidden_states = self.self_attn(hidden_states, attention_mask, position_ids)
        hidden_states = residual + hidden_states

        # MLP with Pre-Norm
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states
```

### 2.2.3 LLM在多模态中的适配

```
多模态LLM输入格式:

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  [Vision Tokens] [Text Tokens]                                 │
│                                                                 │
│  ┌─────────────────────┐ ┌────────────────────────────────┐    │
│  │ <img> v₁ v₂ ... vₙ  │ │ User: 这是什么？\n Assistant:  │    │
│  │    (视觉token)       │ │        (文本token)              │    │
│  └─────────────────────┘ └────────────────────────────────┘    │
│                                                                 │
│  注意力mask:                                                    │
│  - 所有token可以看到视觉tokens                                  │
│  - 文本tokens使用因果mask (只能看到之前的token)                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 2.3 连接模块 (Connector/Projector)

连接模块是多模态模型中**最关键的设计决策**，决定了视觉信息如何"翻译"给语言模型。

### 2.3.1 方案对比概览

```
┌───────────────────────────────────────────────────────────────────────┐
│                        连接模块设计空间                                │
├───────────────────────────────────────────────────────────────────────┤
│                                                                       │
│  维度1: 是否压缩视觉token                                             │
│  ├─ 不压缩: 保留所有空间信息 (如576个token)                           │
│  │   代表: LLaVA (Linear/MLP投影)                                    │
│  │   优点: 信息无损                                                  │
│  │   缺点: 占用大量上下文长度                                        │
│  │                                                                   │
│  └─ 压缩: 减少token数量 (如32个token)                                │
│      代表: BLIP-2 (Q-Former), Flamingo (Perceiver)                   │
│      优点: 节省上下文                                                │
│      缺点: 可能丢失细节                                              │
│                                                                       │
│  维度2: 融合方式                                                      │
│  ├─ 前缀融合: 视觉token作为prefix，只需一次编码                       │
│  │   代表: LLaVA, BLIP-2                                             │
│  │                                                                   │
│  └─ 层间融合: 在LLM的每层或部分层注入视觉信息                         │
│      代表: Flamingo (Gated Cross-Attention)                          │
│      优点: 更深度的融合                                              │
│      缺点: 需要修改LLM结构                                           │
│                                                                       │
└───────────────────────────────────────────────────────────────────────┘
```

### 2.3.2 Linear/MLP Projection (LLaVA)

最简单直接的方式，用可学习的线性层对齐特征空间。

```python
class LLaVAProjector(nn.Module):
    """LLaVA风格的投影器"""

    def __init__(self, vision_hidden_size=1024, text_hidden_size=4096, projector_type='mlp'):
        super().__init__()

        if projector_type == 'linear':
            # 单层线性投影
            self.projector = nn.Linear(vision_hidden_size, text_hidden_size)

        elif projector_type == 'mlp':
            # 两层MLP (LLaVA-1.5使用)
            self.projector = nn.Sequential(
                nn.Linear(vision_hidden_size, text_hidden_size),
                nn.GELU(),
                nn.Linear(text_hidden_size, text_hidden_size)
            )

        elif projector_type == 'mlp_deep':
            # 更深的MLP
            self.projector = nn.Sequential(
                nn.Linear(vision_hidden_size, text_hidden_size),
                nn.GELU(),
                nn.Linear(text_hidden_size, text_hidden_size),
                nn.GELU(),
                nn.Linear(text_hidden_size, text_hidden_size)
            )

    def forward(self, vision_features):
        """
        vision_features: (B, N, vision_hidden_size) 来自ViT
        return: (B, N, text_hidden_size) 可直接输入LLM
        """
        return self.projector(vision_features)


class LLaVAMultimodalModel(nn.Module):
    """完整的LLaVA模型"""

    def __init__(self, vision_encoder, projector, llm):
        super().__init__()
        self.vision_encoder = vision_encoder
        self.projector = projector
        self.llm = llm

    def encode_images(self, images):
        """编码图像为LLM可理解的token"""
        # ViT编码
        vision_features = self.vision_encoder(images, return_all_tokens=True)
        # 投影
        vision_tokens = self.projector(vision_features)
        return vision_tokens

    def forward(self, images, input_ids, attention_mask=None, labels=None):
        """
        前向传播
        """
        # 获取视觉token
        vision_tokens = self.encode_images(images)  # (B, N_v, D)

        # 获取文本embedding
        text_embeds = self.llm.get_input_embeddings()(input_ids)  # (B, N_t, D)

        # 找到<image>占位符的位置，用视觉token替换
        # 简化版本：假设视觉token在最前面
        inputs_embeds = torch.cat([vision_tokens, text_embeds], dim=1)

        # LLM前向
        outputs = self.llm(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            labels=labels,
        )

        return outputs
```

### 2.3.3 Q-Former (BLIP-2)

使用可学习的Query通过交叉注意力"提问"视觉特征，实现token压缩。

```python
class QFormerLayer(nn.Module):
    """Q-Former的单层"""

    def __init__(self, hidden_dim=768, num_heads=12, cross_attention=True):
        super().__init__()

        # Self-Attention (queries之间)
        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)
        self.self_attn_norm = nn.LayerNorm(hidden_dim)

        # Cross-Attention (queries查询视觉特征)
        self.cross_attention = cross_attention
        if cross_attention:
            self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)
            self.cross_attn_norm = nn.LayerNorm(hidden_dim)

        # FFN
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.GELU(),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )
        self.ffn_norm = nn.LayerNorm(hidden_dim)

    def forward(self, queries, vision_features=None):
        """
        queries: (B, num_queries, D)
        vision_features: (B, N, D)
        """
        # Self-Attention
        residual = queries
        queries = self.self_attn_norm(queries)
        queries, _ = self.self_attn(queries, queries, queries)
        queries = residual + queries

        # Cross-Attention
        if self.cross_attention and vision_features is not None:
            residual = queries
            queries = self.cross_attn_norm(queries)
            queries, _ = self.cross_attn(queries, vision_features, vision_features)
            queries = residual + queries

        # FFN
        residual = queries
        queries = self.ffn_norm(queries)
        queries = self.ffn(queries)
        queries = residual + queries

        return queries


class QFormer(nn.Module):
    """完整的Q-Former"""

    def __init__(
        self,
        vision_hidden_size=1024,
        hidden_dim=768,
        num_queries=32,
        num_layers=12,
        num_heads=12,
        cross_attention_freq=2,  # 每隔几层加一个cross-attention
    ):
        super().__init__()

        # 可学习的query tokens
        self.queries = nn.Parameter(torch.zeros(1, num_queries, hidden_dim))
        nn.init.normal_(self.queries, std=0.02)

        # 视觉特征投影（如果维度不匹配）
        self.vision_proj = nn.Linear(vision_hidden_size, hidden_dim)

        # Q-Former layers
        self.layers = nn.ModuleList([
            QFormerLayer(
                hidden_dim=hidden_dim,
                num_heads=num_heads,
                cross_attention=(i % cross_attention_freq == 0)
            )
            for i in range(num_layers)
        ])

        self.norm = nn.LayerNorm(hidden_dim)

    def forward(self, vision_features):
        """
        vision_features: (B, N, vision_hidden_size) 来自ViT
        return: (B, num_queries, hidden_dim) 压缩后的视觉token
        """
        B = vision_features.shape[0]

        # 投影视觉特征
        vision_features = self.vision_proj(vision_features)

        # 扩展queries到batch
        queries = self.queries.expand(B, -1, -1)

        # 通过所有层
        for layer in self.layers:
            queries = layer(queries, vision_features)

        queries = self.norm(queries)

        return queries
```

### 2.3.4 Perceiver Resampler (Flamingo)

类似Q-Former，但设计更灵活，支持多图像处理。

```python
class PerceiverResampler(nn.Module):
    """Flamingo的Perceiver Resampler"""

    def __init__(
        self,
        vision_hidden_size=1024,
        hidden_dim=1024,
        num_latents=64,  # 类似Q-Former的num_queries
        num_layers=6,
        num_heads=16,
    ):
        super().__init__()

        # 可学习的latent tokens
        self.latents = nn.Parameter(torch.randn(num_latents, hidden_dim))

        # 时序位置编码（用于视频/多图像）
        self.time_pos_emb = nn.Parameter(torch.randn(1, 1, hidden_dim))

        # 投影层
        self.input_proj = nn.Linear(vision_hidden_size, hidden_dim)

        # Perceiver layers
        self.layers = nn.ModuleList([
            PerceiverLayer(hidden_dim, num_heads)
            for _ in range(num_layers)
        ])

        self.norm = nn.LayerNorm(hidden_dim)

    def forward(self, vision_features, time_indices=None):
        """
        vision_features: (B, T, N, D) 支持多帧/多图像
                        或 (B, N, D) 单图像
        time_indices: 时序索引
        """
        if vision_features.dim() == 3:
            vision_features = vision_features.unsqueeze(1)  # 添加时间维度

        B, T, N, D = vision_features.shape

        # 投影
        vision_features = self.input_proj(vision_features)

        # 添加时序位置编码
        if time_indices is not None:
            time_emb = self.time_pos_emb[:, time_indices]
            vision_features = vision_features + time_emb.unsqueeze(2)

        # 展平时间和空间维度
        vision_features = vision_features.reshape(B, T * N, -1)

        # 扩展latents
        latents = self.latents.unsqueeze(0).expand(B, -1, -1)

        # Perceiver layers
        for layer in self.layers:
            latents = layer(latents, vision_features)

        return self.norm(latents)


class PerceiverLayer(nn.Module):
    """Perceiver单层"""

    def __init__(self, hidden_dim, num_heads):
        super().__init__()

        # Cross attention: latents attend to vision
        self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)
        self.cross_norm = nn.LayerNorm(hidden_dim)

        # Self attention: latents attend to latents
        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)
        self.self_norm = nn.LayerNorm(hidden_dim)

        # FFN
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.GELU(),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )
        self.ffn_norm = nn.LayerNorm(hidden_dim)

    def forward(self, latents, vision_features):
        # Cross attention
        residual = latents
        latents = self.cross_norm(latents)
        latents, _ = self.cross_attn(latents, vision_features, vision_features)
        latents = residual + latents

        # Self attention
        residual = latents
        latents = self.self_norm(latents)
        latents, _ = self.self_attn(latents, latents, latents)
        latents = residual + latents

        # FFN
        residual = latents
        latents = self.ffn_norm(latents)
        latents = self.ffn(latents)
        latents = residual + latents

        return latents
```

### 2.3.5 Gated Cross-Attention (Flamingo)

在LLM的每层（或部分层）中插入门控交叉注意力。

```python
class GatedCrossAttentionBlock(nn.Module):
    """Flamingo的门控交叉注意力"""

    def __init__(self, hidden_dim, num_heads):
        super().__init__()

        self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)
        self.cross_attn_norm = nn.LayerNorm(hidden_dim)

        # 可学习的门控参数，初始化为0
        # 这样训练初期模型行为接近原始LLM
        self.gate = nn.Parameter(torch.zeros(1))

    def forward(self, text_features, vision_features):
        """
        text_features: (B, L, D) LLM隐藏状态
        vision_features: (B, N, D) 视觉特征
        """
        residual = text_features

        # Cross attention
        text_features = self.cross_attn_norm(text_features)
        cross_out, _ = self.cross_attn(text_features, vision_features, vision_features)

        # 门控融合
        text_features = residual + torch.tanh(self.gate) * cross_out

        return text_features


class FlamingoLayer(nn.Module):
    """在LLM层中插入Gated Cross-Attention"""

    def __init__(self, original_layer, hidden_dim, num_heads):
        super().__init__()
        self.original_layer = original_layer
        self.gated_cross_attn = GatedCrossAttentionBlock(hidden_dim, num_heads)

    def forward(self, hidden_states, vision_features=None, **kwargs):
        # 先进行交叉注意力（如果有视觉特征）
        if vision_features is not None:
            hidden_states = self.gated_cross_attn(hidden_states, vision_features)

        # 原始LLM层
        hidden_states = self.original_layer(hidden_states, **kwargs)

        return hidden_states
```

### 2.3.6 动态分辨率处理

为处理不同尺寸的高分辨率图像，现代模型采用动态切片策略。

```python
class DynamicResolutionProcessor:
    """动态分辨率处理器（LLaVA-NeXT, InternVL风格）"""

    def __init__(
        self,
        base_image_size=336,
        patch_size=14,
        max_tiles=6,
        min_tiles=1,
    ):
        self.base_size = base_image_size
        self.patch_size = patch_size
        self.max_tiles = max_tiles
        self.min_tiles = min_tiles

        # 预定义的分辨率网格选项
        self.grid_options = self._generate_grid_options()

    def _generate_grid_options(self):
        """生成所有可能的网格配置"""
        options = []
        for h in range(1, self.max_tiles + 1):
            for w in range(1, self.max_tiles + 1):
                if self.min_tiles <= h * w <= self.max_tiles:
                    options.append((h, w))
        return options

    def select_best_grid(self, image_width, image_height):
        """选择最佳网格配置"""
        aspect_ratio = image_width / image_height
        best_grid = (1, 1)
        best_waste = float('inf')

        for grid_h, grid_w in self.grid_options:
            # 计算使用此网格时的目标尺寸
            target_h = grid_h * self.base_size
            target_w = grid_w * self.base_size
            target_ratio = target_w / target_h

            # 计算需要的缩放
            if aspect_ratio > target_ratio:
                # 宽度为限制因素
                scale = target_w / image_width
            else:
                # 高度为限制因素
                scale = target_h / image_height

            # 计算缩放后的尺寸和浪费的像素
            scaled_w = image_width * scale
            scaled_h = image_height * scale
            waste = target_h * target_w - scaled_h * scaled_w

            if waste < best_waste:
                best_waste = waste
                best_grid = (grid_h, grid_w)

        return best_grid

    def process_image(self, image):
        """
        处理图像，返回子图列表

        image: PIL.Image
        return: list of PIL.Image (tiles + thumbnail)
        """
        width, height = image.size
        grid_h, grid_w = self.select_best_grid(width, height)

        # 计算目标尺寸
        target_h = grid_h * self.base_size
        target_w = grid_w * self.base_size

        # 调整图像尺寸
        resized = image.resize((target_w, target_h))

        tiles = []

        # 切分为子图
        for i in range(grid_h):
            for j in range(grid_w):
                left = j * self.base_size
                top = i * self.base_size
                right = left + self.base_size
                bottom = top + self.base_size

                tile = resized.crop((left, top, right, bottom))
                tiles.append(tile)

        # 添加全局缩略图
        thumbnail = image.resize((self.base_size, self.base_size))
        tiles.append(thumbnail)

        return tiles, (grid_h, grid_w)
```

---

## 2.4 典型架构深入分析

### 2.4.1 LLaVA 完整架构

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           LLaVA 架构                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  输入图像 (336×336)                                                     │
│       │                                                                 │
│       ▼                                                                 │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                    CLIP ViT-L/14@336                             │   │
│  │  ┌───────────────────────────────────────────────────────────┐   │   │
│  │  │  Patch Embedding: 336/14 = 24, 共24×24 = 576 patches      │   │   │
│  │  │  Transformer: 24层, hidden=1024, heads=16                 │   │   │
│  │  │  输出: (B, 576, 1024)                                     │   │   │
│  │  └───────────────────────────────────────────────────────────┘   │   │
│  │  状态: 冻结 (不参与训练)                                         │   │
│  └───────────────────────────────────────┬─────────────────────────┘   │
│                                          │                              │
│                                          ▼                              │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                      MLP Projector                               │   │
│  │  ┌───────────────────────────────────────────────────────────┐   │   │
│  │  │  Linear(1024, 4096)                                       │   │   │
│  │  │  GELU()                                                   │   │   │
│  │  │  Linear(4096, 4096)                                       │   │   │
│  │  │  输出: (B, 576, 4096)                                     │   │   │
│  │  └───────────────────────────────────────────────────────────┘   │   │
│  │  参数量: 1024×4096 + 4096×4096 ≈ 21M                            │   │
│  │  状态: 可训练                                                    │   │
│  └───────────────────────────────────────┬─────────────────────────┘   │
│                                          │                              │
│                                          ▼                              │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                     Vicuna-7B / 13B                              │   │
│  │  ┌───────────────────────────────────────────────────────────┐   │   │
│  │  │  输入: [vision_tokens(576)] + [text_tokens]               │   │   │
│  │  │  Transformer Decoder: 32层, hidden=4096, heads=32         │   │   │
│  │  │  自回归生成回复                                            │   │   │
│  │  └───────────────────────────────────────────────────────────┘   │   │
│  │  状态: 阶段1冻结, 阶段2可训练 (LoRA或全参数)                     │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘

LLaVA版本演进:
├── LLaVA-1.0: 基础版本
├── LLaVA-1.5: 更好的MLP, 更多数据
├── LLaVA-NeXT: 动态分辨率, 更大模型
└── LLaVA-OneVision: 统一图像/视频理解
```

### 2.4.2 BLIP-2 完整架构

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           BLIP-2 架构                                   │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  输入图像                                                               │
│       │                                                                 │
│       ▼                                                                 │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                     EVA-CLIP ViT-G                               │   │
│  │  ┌───────────────────────────────────────────────────────────┐   │   │
│  │  │  参数量: 1B                                               │   │   │
│  │  │  输出: (B, 257, 1408) 包含[CLS]                          │   │   │
│  │  └───────────────────────────────────────────────────────────┘   │   │
│  │  状态: 完全冻结                                                  │   │
│  └───────────────────────────────────────┬─────────────────────────┘   │
│                                          │                              │
│                                          ▼                              │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                        Q-Former                                  │   │
│  │  ┌───────────────────────────────────────────────────────────┐   │   │
│  │  │  Learnable Queries: 32个, 每个768维                       │   │   │
│  │  │  Transformer: 12层                                        │   │   │
│  │  │  - Self-Attention (queries内部)                           │   │   │
│  │  │  - Cross-Attention (queries查询视觉特征)                   │   │   │
│  │  │  输出: (B, 32, 768)                                       │   │   │
│  │  └───────────────────────────────────────────────────────────┘   │   │
│  │  参数量: ~188M                                                   │   │
│  │  状态: 可训练                                                    │   │
│  │                                                                  │   │
│  │  投影层: Linear(768, LLM_dim)                                   │   │
│  └───────────────────────────────────────┬─────────────────────────┘   │
│                                          │                              │
│                                          ▼                              │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                  FlanT5-XL / Vicuna                              │   │
│  │  ┌───────────────────────────────────────────────────────────┐   │   │
│  │  │  输入: [32个压缩的视觉token] + [text_tokens]              │   │   │
│  │  │  相比LLaVA节省: 576-32 = 544个token的上下文长度           │   │   │
│  │  └───────────────────────────────────────────────────────────┘   │   │
│  │  状态: 完全冻结                                                  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘

BLIP-2训练策略:
阶段1: Vision-Language Representation Learning
  - 训练Q-Former做图文对齐
  - 使用ITC, ITM, ITG三个目标

阶段2: Vision-to-Language Generative Learning
  - 连接冻结的LLM
  - 只训练Q-Former到LLM的投影层
```

### 2.4.3 架构对比总结

| 特性 | LLaVA | BLIP-2 | Flamingo | InternVL |
|------|-------|--------|----------|----------|
| **视觉编码器** | CLIP ViT-L | EVA-CLIP | NFNet | InternViT |
| **连接模块** | 2层MLP | Q-Former | Perceiver + Gated XAttn | QLLaMA |
| **语言模型** | Vicuna/LLaMA | FlanT5/Vicuna | Chinchilla | InternLM |
| **视觉token数** | 576 | 32 | 可变 | 动态 |
| **训练策略** | 两阶段全参数 | 只训Q-Former | 只训新增模块 | 全参数 |
| **多图像支持** | 需扩展 | 需扩展 | 原生支持 | 支持 |
| **开源情况** | 完全开源 | 完全开源 | 部分开源 | 完全开源 |

---

## 2.5 总结

```
多模态大模型架构设计的关键决策:

1. 视觉编码器选择
   ├─ 预训练方式: 对比学习(CLIP) vs 自监督(DINOv2) vs 混合(EVA)
   ├─ 模型规模: ViT-B vs ViT-L vs ViT-G
   └─ 是否冻结: 冻结(参数效率) vs 微调(更好性能)

2. 连接模块设计
   ├─ 复杂度: 简单投影 vs Q-Former vs 层间融合
   ├─ 压缩率: 保留所有token vs 压缩到固定数量
   └─ 融合位置: 仅输入层 vs 多层融合

3. 语言模型选择
   ├─ 架构: Decoder-only (主流)
   ├─ 规模: 7B → 13B → 34B → 70B
   └─ 微调策略: 冻结 vs LoRA vs 全参数

4. 分辨率处理
   ├─ 固定分辨率: 简单但可能丢失细节
   └─ 动态分辨率: 复杂但保留更多信息
```
