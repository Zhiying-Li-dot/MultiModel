# 第七章：前沿方向

## 7.1 视频理解

### 挑战

视频相比图像增加了**时间维度**，带来新的挑战：

```
图像: (H, W, C)           → N个视觉token
视频: (T, H, W, C)        → T×N个视觉token

问题:
1. Token数量爆炸: 30fps × 10秒 × 576 = 172,800 tokens
2. 时序建模: 需要理解动作的时间顺序
3. 长视频: 电影、讲座可能长达数小时
4. 计算成本: 显存和计算量剧增
```

### 解决方案

#### 方案1: 稀疏采样

```python
class SparseVideoSampler:
    """从长视频中稀疏采样关键帧"""

    def __init__(self, num_frames=8):
        self.num_frames = num_frames

    def uniform_sample(self, video):
        """均匀采样"""
        total_frames = len(video)
        indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)
        return video[indices]

    def keyframe_sample(self, video):
        """关键帧采样 (基于帧间差异)"""
        diffs = []
        for i in range(1, len(video)):
            diff = np.abs(video[i] - video[i-1]).mean()
            diffs.append((i, diff))

        # 选择差异最大的帧
        diffs.sort(key=lambda x: x[1], reverse=True)
        indices = sorted([0] + [d[0] for d in diffs[:self.num_frames-1]])

        return video[indices]
```

**优点**: 简单高效
**缺点**: 可能错过关键信息

#### 方案2: 时序聚合

```
┌─────────────────────────────────────────┐
│              时序聚合架构                │
├─────────────────────────────────────────┤
│                                         │
│  Frame 1   Frame 2   Frame 3   Frame N  │
│     │         │         │         │     │
│     ▼         ▼         ▼         ▼     │
│  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐    │
│  │ ViT │  │ ViT │  │ ViT │  │ ViT │    │
│  └──┬──┘  └──┬──┘  └──┬──┘  └──┬──┘    │
│     │         │         │         │     │
│     └────┬────┴────┬────┴────┬────┘     │
│          │         │         │          │
│          ▼         ▼         ▼          │
│     ┌────────────────────────────┐      │
│     │    Temporal Transformer    │      │
│     │    (建模帧间关系)           │      │
│     └─────────────┬──────────────┘      │
│                   │                      │
│                   ▼                      │
│            时序聚合特征                  │
│                                         │
└─────────────────────────────────────────┘
```

```python
class TemporalAggregator(nn.Module):
    """时序特征聚合"""

    def __init__(self, dim, num_frames, num_heads=8):
        super().__init__()
        self.temporal_embed = nn.Parameter(
            torch.randn(1, num_frames, dim)
        )
        self.temporal_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=dim, nhead=num_heads),
            num_layers=4
        )

    def forward(self, frame_features):
        """
        frame_features: (B, T, N, D)
        B: batch, T: frames, N: patches, D: dim
        """
        B, T, N, D = frame_features.shape

        # 每帧取[CLS]或平均池化
        frame_level = frame_features.mean(dim=2)  # (B, T, D)

        # 添加时序位置编码
        frame_level = frame_level + self.temporal_embed

        # 时序Transformer
        temporal_out = self.temporal_transformer(frame_level)

        return temporal_out
```

#### 方案3: 流式处理

适用于超长视频，维护滑动窗口记忆：

```python
class StreamingVideoProcessor:
    """流式视频处理"""

    def __init__(self, window_size=32, memory_size=128):
        self.window_size = window_size
        self.memory_size = memory_size
        self.memory_bank = None

    def process_chunk(self, frames, model):
        """处理视频块"""
        # 编码当前窗口
        features = model.encode_frames(frames)

        # 与记忆融合
        if self.memory_bank is not None:
            features = self.fuse_with_memory(features, self.memory_bank)

        # 更新记忆
        self.update_memory(features)

        return features

    def update_memory(self, features):
        """更新记忆库 (保留重要特征)"""
        if self.memory_bank is None:
            self.memory_bank = features
        else:
            # 合并并压缩
            combined = torch.cat([self.memory_bank, features], dim=1)
            # 保留最重要的memory_size个
            self.memory_bank = self.compress(combined, self.memory_size)
```

### 代表工作

| 模型 | 方法 | 特点 |
|------|------|------|
| Video-LLaVA | 均匀采样 + 时序池化 | 简单有效 |
| LLaMA-VID | Token压缩 | 长视频支持 |
| VideoChat | 时空注意力 | 对话交互 |
| Gemini 1.5 | 原生长上下文 | 百万token |

---

## 7.2 多模态生成

### 统一理解与生成

传统方案将理解和生成分开：
```
理解任务: 多模态LLM (LLaVA, GPT-4V)
生成任务: 扩散模型 (Stable Diffusion, DALL-E)
```

新趋势是**统一架构**：

```
┌────────────────────────────────────────────────┐
│              统一的Transformer                  │
│                                                │
│  输入: [文本token] [图像token] [生成token]      │
│                                                │
│  理解任务:                                      │
│  [图像] + "描述这张图片" → [文本描述]           │
│                                                │
│  生成任务:                                      │
│  "一只猫在月球上" + [MASK] → [图像token]        │
│                                                │
│  编辑任务:                                      │
│  [原图] + "把猫变成狗" → [新图像token]          │
│                                                │
└────────────────────────────────────────────────┘
```

### 实现方式

#### 离散Token统一

```python
class UnifiedMultimodalModel(nn.Module):
    """统一的多模态模型"""

    def __init__(self, vocab_size, image_codebook_size):
        super().__init__()
        # 统一词表: 文本token + 图像token
        total_vocab = vocab_size + image_codebook_size
        self.embedding = nn.Embedding(total_vocab, hidden_dim)
        self.transformer = TransformerDecoder(...)
        self.lm_head = nn.Linear(hidden_dim, total_vocab)

        # 图像编解码器 (VQ-VAE)
        self.image_tokenizer = VQImageTokenizer(codebook_size=image_codebook_size)

    def forward(self, input_ids, task="understand"):
        """
        input_ids: 混合的文本和图像token
        """
        embeds = self.embedding(input_ids)
        hidden = self.transformer(embeds)
        logits = self.lm_head(hidden)
        return logits

    def generate_image(self, text_prompt):
        """文生图"""
        text_tokens = self.tokenize_text(text_prompt)

        # 自回归生成图像token
        generated = []
        for _ in range(256):  # 16x16 图像token
            logits = self.forward(text_tokens + generated)
            next_token = logits[:, -1].argmax()
            generated.append(next_token)

        # 解码为图像
        image = self.image_tokenizer.decode(generated)
        return image
```

#### 连续特征 + 扩散

```python
class DiffusionMultimodalModel(nn.Module):
    """结合扩散模型的多模态生成"""

    def __init__(self):
        super().__init__()
        self.llm = LLaMA()
        self.diffusion = LatentDiffusion()
        self.connector = nn.Linear(llm_dim, diffusion_dim)

    def generate_image(self, text):
        # LLM理解文本
        text_features = self.llm.encode(text)

        # 转换为扩散模型条件
        condition = self.connector(text_features)

        # 扩散模型生成
        image = self.diffusion.sample(condition)

        return image
```

### 代表工作

| 模型 | 架构 | 能力 |
|------|------|------|
| **Chameleon** (Meta) | 离散token统一 | 理解+生成 |
| **Gemini** | 原生多模态 | 理解+生成 |
| **Emu2** | 连续+扩散 | 理解+生成 |
| **Show-o** | 离散+连续混合 | 统一架构 |

---

## 7.3 多模态Agent

### GUI操作Agent

让AI代理操作图形界面完成任务。

```
任务: "帮我在淘宝上搜索一本Python书"

Agent循环:
┌─────────────────────────────────────────────────┐
│                                                 │
│  1. 观察 (Observe)                              │
│     └── 截取屏幕截图                            │
│                                                 │
│  2. 思考 (Think)                                │
│     └── 分析当前界面状态                        │
│     └── 决定下一步动作                          │
│                                                 │
│  3. 行动 (Act)                                  │
│     └── 执行操作: 点击/输入/滚动                │
│                                                 │
│  4. 检查 (Check)                                │
│     └── 判断任务是否完成                        │
│     └── 如未完成，返回步骤1                     │
│                                                 │
└─────────────────────────────────────────────────┘
```

#### 实现框架

```python
class GUIAgent:
    """GUI操作Agent"""

    def __init__(self, vlm_model):
        self.vlm = vlm_model
        self.action_history = []

    def run(self, task):
        """执行任务"""
        max_steps = 20

        for step in range(max_steps):
            # 1. 观察
            screenshot = self.capture_screen()

            # 2. 思考
            action = self.think(screenshot, task)

            # 3. 检查是否完成
            if action['type'] == 'DONE':
                return True

            # 4. 执行动作
            self.execute(action)
            self.action_history.append(action)

        return False

    def think(self, screenshot, task):
        """使用VLM决定下一步动作"""
        prompt = f"""
        任务: {task}
        历史动作: {self.action_history[-5:]}

        请分析当前屏幕截图，决定下一步动作。

        可用动作:
        - CLICK(x, y): 点击坐标
        - TYPE(text): 输入文本
        - SCROLL(direction): 滚动页面
        - DONE: 任务完成

        输出格式: {{"type": "...", "params": ...}}
        """

        response = self.vlm.generate(screenshot, prompt)
        action = json.loads(response)
        return action

    def execute(self, action):
        """执行动作"""
        if action['type'] == 'CLICK':
            pyautogui.click(action['params']['x'], action['params']['y'])
        elif action['type'] == 'TYPE':
            pyautogui.typewrite(action['params']['text'])
        elif action['type'] == 'SCROLL':
            pyautogui.scroll(action['params']['direction'])
```

### 具身智能

将多模态大模型应用于机器人。

```
┌─────────────────────────────────────────────────┐
│                具身智能系统                      │
├─────────────────────────────────────────────────┤
│                                                 │
│  感知层                                         │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐           │
│  │  相机   │ │  雷达   │ │  触觉   │           │
│  └────┬────┘ └────┬────┘ └────┬────┘           │
│       │           │           │                 │
│       └───────────┴───────────┘                 │
│                   │                             │
│                   ▼                             │
│  ┌─────────────────────────────────────────┐   │
│  │          多模态大模型 (Brain)            │   │
│  │                                         │   │
│  │  - 场景理解                              │   │
│  │  - 任务规划                              │   │
│  │  - 动作生成                              │   │
│  └──────────────────┬──────────────────────┘   │
│                     │                           │
│                     ▼                           │
│  执行层                                         │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐           │
│  │ 机械臂  │ │  移动   │ │  夹爪   │           │
│  └─────────┘ └─────────┘ └─────────┘           │
│                                                 │
└─────────────────────────────────────────────────┘
```

#### 任务分解示例

```python
class EmbodiedAgent:
    """具身智能Agent"""

    def __init__(self, vlm, robot_controller):
        self.vlm = vlm
        self.robot = robot_controller

    def execute_task(self, task):
        """
        任务: "把桌上的苹果放到冰箱里"
        """
        # 1. 任务分解
        subtasks = self.decompose_task(task)
        # ["定位苹果", "移动到苹果位置", "抓取苹果",
        #  "定位冰箱", "打开冰箱", "放入苹果", "关闭冰箱"]

        # 2. 逐步执行
        for subtask in subtasks:
            self.execute_subtask(subtask)

    def decompose_task(self, task):
        """使用VLM分解任务"""
        # 观察环境
        observation = self.robot.get_observation()

        prompt = f"""
        任务: {task}
        当前观察: [图像]

        请将任务分解为可执行的子任务序列。
        """

        response = self.vlm.generate(observation['image'], prompt)
        subtasks = json.loads(response)
        return subtasks

    def execute_subtask(self, subtask):
        """执行子任务"""
        observation = self.robot.get_observation()

        # VLM生成低级动作
        prompt = f"""
        子任务: {subtask}
        当前观察: [图像]

        生成机器人动作参数:
        - move_to(x, y, z)
        - grasp(open/close)
        - rotate(angle)
        """

        action = self.vlm.generate(observation['image'], prompt)
        self.robot.execute(action)
```

### 应用场景

| 场景 | 描述 | 挑战 |
|------|------|------|
| **家庭服务** | 整理、清洁、烹饪 | 复杂环境、安全性 |
| **自动驾驶** | 感知、决策、规划 | 实时性、可靠性 |
| **工业机器人** | 装配、检测、分拣 | 精度、效率 |
| **医疗辅助** | 手术、康复、护理 | 安全性、合规性 |

---

## 7.4 其他前沿方向

### 任意分辨率/任意模态

```
目标: 一个模型处理任意输入
- 任意分辨率图像
- 任意长度视频
- 任意组合的多模态输入

代表: Fuyu, Pix2Struct, Unified-IO
```

### 3D理解

```
┌─────────────────────────────────────┐
│           3D多模态理解               │
├─────────────────────────────────────┤
│                                     │
│  输入模态:                          │
│  - 点云 (LiDAR)                     │
│  - 多视角图像                        │
│  - 深度图                           │
│  - RGB-D                            │
│                                     │
│  任务:                              │
│  - 3D物体检测                       │
│  - 场景理解                          │
│  - 空间问答                          │
│  - 3D生成                           │
│                                     │
└─────────────────────────────────────┘
```

### 多模态推理链

```
问题: "图中的物理现象违反了什么定律？"

推理链:
1. 识别图像内容: 一个球悬浮在半空中
2. 分析物理状态: 没有任何支撑物
3. 调用物理知识: 物体应该受重力下落
4. 得出结论: 违反了牛顿的重力定律

这需要:
- 视觉感知能力
- 物理常识
- 逻辑推理能力
- 知识整合能力
```

---

## 总结

| 方向 | 核心挑战 | 关键技术 |
|------|----------|----------|
| 视频理解 | 时序建模、长视频 | 稀疏采样、时序聚合、流式处理 |
| 多模态生成 | 统一架构 | 离散token、扩散模型 |
| GUI Agent | 界面理解、动作规划 | 屏幕理解、动作空间设计 |
| 具身智能 | 感知-决策-执行闭环 | 任务分解、动作生成 |

未来趋势：
1. **统一化**：一个模型处理所有模态和任务
2. **实时化**：更快的推理速度支持实时应用
3. **可靠化**：减少幻觉，提高可信度
4. **个性化**：适应不同用户和场景
