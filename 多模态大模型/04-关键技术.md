# 第四章：关键技术

## 4.1 视觉Token化

将图像转换为LLM可处理的token序列。

### 连续Token（主流方案）

```
图像 → ViT → 连续向量序列
              [v₁, v₂, ..., vₙ]
              每个vᵢ是高维连续向量 (如768维或1024维)
```

**实现方式**：
```python
def continuous_tokenization(image, vision_encoder, projector):
    # 1. ViT编码
    patch_features = vision_encoder(image)  # (B, 196, 1024)

    # 2. 投影到LLM空间
    vision_tokens = projector(patch_features)  # (B, 196, 4096)

    return vision_tokens
```

**特点**：
- 信息保留完整
- 与LLM的embedding维度对齐
- 需要投影层做空间转换

### 离散Token

```
图像 → VQ-VAE/VQGAN → 离散码本索引
                      [523, 128, 892, ...]
                      可直接用语言模型的词表处理
```

**实现原理**：
```python
class VQTokenizer:
    def __init__(self, codebook_size=8192, code_dim=256):
        # 码本：codebook_size个code_dim维向量
        self.codebook = nn.Embedding(codebook_size, code_dim)

    def encode(self, image):
        # 编码为连续特征
        z = self.encoder(image)  # (B, H, W, D)

        # 量化：找最近的码本向量
        distances = torch.cdist(z, self.codebook.weight)
        indices = distances.argmin(dim=-1)  # (B, H, W)

        return indices

    def decode(self, indices):
        # 从码本查找向量
        z_q = self.codebook(indices)

        # 解码为图像
        return self.decoder(z_q)
```

**代表工作**：
- VQGAN：用对抗训练的离散图像编码
- DALL-E的dVAE
- Chameleon：统一的离散token处理

### 对比

| 方式 | 优点 | 缺点 |
|------|------|------|
| 连续Token | 信息完整，效果好 | 需要特殊处理 |
| 离散Token | 统一处理，支持生成 | 可能损失信息 |

---

## 4.2 位置编码

### 1D位置编码（标准ViT）

```
将2D图像展平为1D序列后添加位置编码：

patch位置:  1    2    3    ...  196
           ↓    ↓    ↓         ↓
位置编码:  PE₁  PE₂  PE₃  ...  PE₁₉₆

问题：丢失了2D空间结构信息
```

**实现**：
```python
class LearnablePositionEncoding(nn.Module):
    def __init__(self, num_patches, dim):
        super().__init__()
        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, dim))

    def forward(self, x):
        return x + self.pos_embed
```

### 2D位置编码

保留行列信息：

```
图像patch网格：
    col 0  col 1  col 2  ...
row 0  (0,0)  (0,1)  (0,2)
row 1  (1,0)  (1,1)  (1,2)
row 2  (2,0)  (2,1)  (2,2)
  ...

2D位置编码：
PE(i,j) = PE_row(i) + PE_col(j)
```

**实现**：
```python
class Position2DEncoding(nn.Module):
    def __init__(self, height, width, dim):
        super().__init__()
        self.row_embed = nn.Parameter(torch.randn(height, dim // 2))
        self.col_embed = nn.Parameter(torch.randn(width, dim // 2))

    def forward(self, x, h, w):
        # 生成2D位置编码
        pos = torch.cat([
            self.row_embed[:h].unsqueeze(1).expand(-1, w, -1),
            self.col_embed[:w].unsqueeze(0).expand(h, -1, -1)
        ], dim=-1)

        return x + pos.flatten(0, 1)
```

### RoPE扩展到2D

RoPE (Rotary Position Embedding) 原本用于1D文本序列，可扩展到2D：

```python
def rope_2d(x, height, width):
    """
    x: (B, H*W, D) 特征
    """
    dim = x.shape[-1]
    half_dim = dim // 4

    # 为行和列分别计算RoPE
    row_pos = torch.arange(height).unsqueeze(1).expand(-1, width)
    col_pos = torch.arange(width).unsqueeze(0).expand(height, -1)

    # 计算旋转角度
    inv_freq = 1.0 / (10000 ** (torch.arange(half_dim) / half_dim))

    row_angles = row_pos.flatten()[:, None] * inv_freq
    col_angles = col_pos.flatten()[:, None] * inv_freq

    # 应用旋转
    x_row = apply_rotary(x[..., :dim//2], row_angles)
    x_col = apply_rotary(x[..., dim//2:], col_angles)

    return torch.cat([x_row, x_col], dim=-1)
```

**优势**：
- 天然支持外推到更大分辨率
- 相对位置编码，泛化性好

---

## 4.3 注意力机制

### 自注意力 (Self-Attention)

视觉token内部交互：

```
Q, K, V 都来自视觉特征

[v₁, v₂, v₃] → Self-Attn → [v'₁, v'₂, v'₃]
每个token可以关注所有其他token
```

**公式**：
```
Attention(Q, K, V) = softmax(QK^T / √d) · V
```

**实现**：
```python
class SelfAttention(nn.Module):
    def __init__(self, dim, num_heads=8):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3)
        self.proj = nn.Linear(dim, dim)

    def forward(self, x):
        B, N, C = x.shape

        # 计算Q, K, V
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)
        q, k, v = qkv.permute(2, 0, 3, 1, 4)

        # 注意力分数
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)

        # 加权求和
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)

        return self.proj(x)
```

### 交叉注意力 (Cross-Attention)

让文本去"查询"视觉信息：

```
Q: 来自文本特征
K, V: 来自视觉特征

文本token可以选择性地关注相关的视觉区域
```

**示例：问"图中的狗是什么颜色"**
```
文本query "颜色" 会重点关注图像中狗的区域
```

**实现**：
```python
class CrossAttention(nn.Module):
    def __init__(self, dim, num_heads=8):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5

        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Linear(dim, dim)
        self.v_proj = nn.Linear(dim, dim)
        self.out_proj = nn.Linear(dim, dim)

    def forward(self, text_features, vision_features):
        """
        text_features: (B, L, D) - Query来源
        vision_features: (B, N, D) - Key, Value来源
        """
        B, L, _ = text_features.shape
        N = vision_features.shape[1]

        # Q来自文本，K/V来自视觉
        q = self.q_proj(text_features)
        k = self.k_proj(vision_features)
        v = self.v_proj(vision_features)

        # Reshape for multi-head attention
        q = q.reshape(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)

        # 计算注意力
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)

        # 输出
        out = (attn @ v).transpose(1, 2).reshape(B, L, -1)

        return self.out_proj(out)
```

### Gated Cross-Attention (Flamingo)

带门控的交叉注意力，控制视觉信息的融入程度：

```python
class GatedCrossAttention(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.cross_attn = CrossAttention(dim)
        self.gate = nn.Parameter(torch.zeros(1))  # 初始化为0

    def forward(self, text_features, vision_features):
        # 门控：控制融入多少视觉信息
        cross_out = self.cross_attn(text_features, vision_features)
        return text_features + torch.tanh(self.gate) * cross_out
```

---

## 4.4 高分辨率处理

### 问题

ViT通常在224×224训练，如何处理高清图像？

- 直接resize会损失细节
- 直接增大输入，计算量剧增 (O(n²))

### 方案1: 位置编码插值

```python
def interpolate_pos_embed(pos_embed, new_size):
    """
    将位置编码插值到新尺寸
    pos_embed: (1, 196, D) for 14x14
    new_size: 目标尺寸，如28x28
    """
    # Reshape to 2D
    old_size = int(pos_embed.shape[1] ** 0.5)
    pos_embed_2d = pos_embed.reshape(1, old_size, old_size, -1)
    pos_embed_2d = pos_embed_2d.permute(0, 3, 1, 2)  # (1, D, H, W)

    # 双线性插值
    pos_embed_2d = F.interpolate(
        pos_embed_2d,
        size=(new_size, new_size),
        mode='bilinear',
        align_corners=False
    )

    # Reshape back
    pos_embed_2d = pos_embed_2d.permute(0, 2, 3, 1)
    return pos_embed_2d.reshape(1, new_size * new_size, -1)
```

### 方案2: 子图切片 (主流方案)

```
原图 (1008×672)
     │
     ▼
┌─────────────────────────────────────┐
│ 1. 计算最佳切分方案                  │
│    目标：每个子图尽量接近336×336     │
│    选择: 3×2 = 6块                  │
└─────────────────┬───────────────────┘
                  ▼
┌───────────────────────────────────────┐
│         切分成子图 + 全局缩略图        │
│                                       │
│  原图切片：                           │
│  ┌───────┬───────┬───────┐           │
│  │ 336×  │ 336×  │ 336×  │           │
│  │ 336   │ 336   │ 336   │           │
│  ├───────┼───────┼───────┤           │
│  │ 336×  │ 336×  │ 336×  │           │
│  │ 336   │ 336   │ 336   │           │
│  └───────┴───────┴───────┘           │
│                                       │
│  全局缩略图：                         │
│  ┌───────────────────────┐           │
│  │     336×336           │           │
│  │   (整图resize)        │           │
│  └───────────────────────┘           │
└─────────────────┬─────────────────────┘
                  ▼
┌───────────────────────────────────────┐
│         每个子图独立编码               │
│                                       │
│  子图1 → ViT → 576 tokens            │
│  子图2 → ViT → 576 tokens            │
│  ...                                  │
│  子图6 → ViT → 576 tokens            │
│  缩略图 → ViT → 576 tokens           │
│                                       │
│  总计: 7 × 576 = 4032 tokens         │
└───────────────────────────────────────┘
```

**实现代码**：
```python
class DynamicHighResProcessor:
    def __init__(self, base_size=336, max_tiles=6):
        self.base_size = base_size
        self.max_tiles = max_tiles

    def get_best_grid(self, width, height):
        """计算最佳切分网格"""
        aspect_ratio = width / height
        best_grid = (1, 1)
        best_diff = float('inf')

        for h in range(1, self.max_tiles + 1):
            for w in range(1, self.max_tiles + 1):
                if h * w > self.max_tiles:
                    continue
                grid_ratio = w / h
                diff = abs(grid_ratio - aspect_ratio)
                if diff < best_diff:
                    best_diff = diff
                    best_grid = (h, w)

        return best_grid

    def process(self, image):
        """处理高分辨率图像"""
        width, height = image.size
        grid_h, grid_w = self.get_best_grid(width, height)

        # 切分子图
        tile_h = height // grid_h
        tile_w = width // grid_w

        tiles = []
        for i in range(grid_h):
            for j in range(grid_w):
                tile = image.crop((
                    j * tile_w, i * tile_h,
                    (j + 1) * tile_w, (i + 1) * tile_h
                ))
                tile = tile.resize((self.base_size, self.base_size))
                tiles.append(tile)

        # 全局缩略图
        thumbnail = image.resize((self.base_size, self.base_size))
        tiles.append(thumbnail)

        return tiles, (grid_h, grid_w)
```

### 方案3: 动态分辨率 (NaViT)

不做固定切分，直接处理原始比例：

```python
class NaViTProcessor:
    """
    NaViT: 支持任意分辨率的ViT
    """
    def __init__(self, patch_size=14, max_tokens=1024):
        self.patch_size = patch_size
        self.max_tokens = max_tokens

    def process(self, image):
        width, height = image.size

        # 计算patch数量
        num_patches_w = width // self.patch_size
        num_patches_h = height // self.patch_size

        # 如果超过最大token数，进行缩放
        if num_patches_w * num_patches_h > self.max_tokens:
            scale = (self.max_tokens / (num_patches_w * num_patches_h)) ** 0.5
            new_w = int(num_patches_w * scale) * self.patch_size
            new_h = int(num_patches_h * scale) * self.patch_size
            image = image.resize((new_w, new_h))

        return image
```

---

## 总结

| 技术 | 核心思想 | 应用场景 |
|------|----------|----------|
| 视觉Token化 | 将图像转为token序列 | 所有多模态模型 |
| 位置编码 | 注入空间/顺序信息 | Transformer架构 |
| 自注意力 | 模态内部特征交互 | 视觉/文本编码 |
| 交叉注意力 | 模态间信息融合 | 多模态融合 |
| 高分辨率处理 | 保留图像细节 | 细粒度理解任务 |

这些技术是构建高效多模态模型的基础组件。
