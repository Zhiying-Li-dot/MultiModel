# 第六章：评估体系

## 6.1 主要基准测试

### 综合评测基准

| 基准 | 评测内容 | 规模 | 特点 |
|------|----------|------|------|
| **MMBench** | 综合多模态能力 | 3000+题 | 中文/英文双语 |
| **MME** | 感知+认知 | 2374题 | 细粒度能力分析 |
| **SEED-Bench** | 生成式评估 | 19K题 | 覆盖12个维度 |
| **MM-Vet** | 综合能力 | 200题 | GPT-4评分 |

### 视觉问答基准

| 基准 | 评测内容 | 规模 | 评测方式 |
|------|----------|------|----------|
| **VQAv2** | 开放域问答 | 1.1M问答 | 准确率匹配 |
| **GQA** | 组合推理 | 22M问答 | 准确率 |
| **OK-VQA** | 外部知识问答 | 14K问答 | 需要世界知识 |
| **A-OKVQA** | 增强知识问答 | 25K问答 | 多选+开放 |

### 专项能力基准

| 基准 | 评测能力 | 特点 |
|------|----------|------|
| **TextVQA** | 图像文字识别 | OCR能力 |
| **DocVQA** | 文档理解 | 文档场景 |
| **ChartQA** | 图表理解 | 数据可视化 |
| **InfoVQA** | 信息图理解 | 复杂图表 |
| **POPE** | 幻觉评估 | 物体存在性 |
| **MMMU** | 大学水平知识 | 多学科 |

---

## 6.2 能力维度

### 多模态能力图谱

```
多模态大模型能力

├── 感知能力 (Perception)
│   ├── 物体识别
│   │   └── "图中有什么物体？"
│   ├── 场景理解
│   │   └── "这是在什么地方？"
│   ├── 属性识别
│   │   └── 颜色、形状、大小、材质
│   ├── 数量计数
│   │   └── "图中有几只猫？"
│   └── 空间关系
│       └── "球在桌子的什么位置？"
│
├── 推理能力 (Reasoning)
│   ├── 逻辑推理
│   │   └── "如果A则B，图中显示A，所以..."
│   ├── 常识推理
│   │   └── "为什么这个人打伞？"
│   ├── 因果推理
│   │   └── "这个动作会导致什么结果？"
│   ├── 数学推理
│   │   └── "计算图中的总价格"
│   └── 时序推理
│       └── "这件事发生之前/之后..."
│
├── 知识能力 (Knowledge)
│   ├── 世界知识
│   │   └── "这是哪个国家的建筑？"
│   ├── 领域知识
│   │   └── "这是什么品种的狗？"
│   └── 文化知识
│       └── "这个节日叫什么？"
│
├── OCR能力
│   ├── 文字识别
│   │   └── 准确读取图中文字
│   ├── 文档理解
│   │   └── 理解文档结构和内容
│   ├── 表格解析
│   │   └── 提取表格数据
│   └── 公式识别
│       └── 数学/化学公式
│
└── 幻觉问题 (Hallucination)
    ├── 物体幻觉
    │   └── 声称存在不存在的物体
    ├── 属性幻觉
    │   └── 描述错误的属性
    └── 关系幻觉
        └── 描述错误的关系
```

### MMBench能力细分

```python
MMBENCH_CAPABILITIES = {
    "L1": {  # 一级能力
        "Perception": ["Existence", "Count", "Position", "Color",
                       "OCR", "Poster", "Celebrity", "Scene",
                       "Landmark", "Artwork", "Action"],
        "Reasoning": ["Attribute Reasoning", "Function Reasoning",
                      "Identity Reasoning", "Physical Reasoning",
                      "Social Reasoning", "Future Prediction"]
    },
    "L2": {  # 二级能力
        "Coarse Perception": ["Image Scene", "Instance Identity",
                              "Instance Attributes", "Instance Location",
                              "Instance Counting"],
        "Fine-grained Perception": ["OCR", "Celebrity", "Landmark"],
        "Attribute Reasoning": ["Physical Property", "Function",
                                "Identity", "Social Relation"],
        "Logic Reasoning": ["Future Prediction", "Structuralized
                            Image-text Understanding"]
    }
}
```

---

## 6.3 评估方法

### 选择题评估

最简单直接的评估方式：

```python
def evaluate_multiple_choice(model, questions):
    """
    选择题评估
    """
    correct = 0
    total = len(questions)

    for q in questions:
        image = q['image']
        question = q['question']
        options = q['options']  # ['A. xxx', 'B. xxx', 'C. xxx', 'D. xxx']
        answer = q['answer']    # 'A', 'B', 'C', or 'D'

        # 构建prompt
        prompt = f"{question}\n" + "\n".join(options) + "\nAnswer:"

        # 模型预测
        response = model.generate(image, prompt)

        # 提取答案
        predicted = extract_answer(response)  # 提取A/B/C/D

        if predicted == answer:
            correct += 1

    accuracy = correct / total
    return accuracy

def extract_answer(response):
    """从模型回复中提取选项"""
    # 方法1: 直接匹配
    for opt in ['A', 'B', 'C', 'D']:
        if opt in response.upper()[:10]:
            return opt

    # 方法2: 正则匹配
    import re
    match = re.search(r'[ABCD]', response.upper())
    if match:
        return match.group()

    return None
```

### 开放式评估 - 规则匹配

```python
def evaluate_vqa_accuracy(prediction, ground_truths):
    """
    VQAv2风格的准确率计算
    ground_truths: 10个标注者的答案
    """
    prediction = normalize_answer(prediction)

    # 计算匹配数
    matches = sum([1 for gt in ground_truths
                   if normalize_answer(gt) == prediction])

    # VQA accuracy公式: min(matches/3, 1)
    accuracy = min(matches / 3, 1.0)
    return accuracy

def normalize_answer(answer):
    """答案标准化"""
    answer = answer.lower().strip()
    # 去除标点
    answer = re.sub(r'[^\w\s]', '', answer)
    # 处理数字
    answer = convert_number_words(answer)  # "two" -> "2"
    # 去除冠词
    answer = re.sub(r'\b(a|an|the)\b', '', answer)
    return answer.strip()
```

### 开放式评估 - GPT-4评分

```python
def gpt4_evaluate(question, ground_truth, prediction):
    """
    使用GPT-4进行开放式问题评估
    """
    prompt = f"""You are an AI assistant that evaluates the quality of answers.

Question: {question}
Reference Answer: {ground_truth}
Model Answer: {prediction}

Evaluate the model answer on a scale of 0-10:
- 10: Perfect answer, matches reference completely
- 7-9: Correct but with minor issues
- 4-6: Partially correct
- 1-3: Mostly incorrect but has some relevant info
- 0: Completely wrong

Provide your score and brief justification.
Format: Score: X/10
Reason: ...
"""

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    # 解析分数
    text = response.choices[0].message.content
    score = extract_score(text)

    return score
```

### 幻觉评估 (POPE)

```python
def evaluate_pope(model, pope_data):
    """
    POPE: Polling-based Object Probing Evaluation
    评估模型是否会产生物体幻觉
    """
    results = {
        "accuracy": 0,
        "precision": 0,
        "recall": 0,
        "f1": 0,
        "yes_ratio": 0  # 模型回答"yes"的比例
    }

    tp, fp, tn, fn = 0, 0, 0, 0
    yes_count = 0

    for item in pope_data:
        image = item['image']
        question = item['question']  # "Is there a cat in the image?"
        label = item['label']  # "yes" or "no"

        response = model.generate(image, question)
        pred = "yes" if "yes" in response.lower() else "no"

        if pred == "yes":
            yes_count += 1

        if pred == "yes" and label == "yes":
            tp += 1
        elif pred == "yes" and label == "no":
            fp += 1  # 幻觉!
        elif pred == "no" and label == "no":
            tn += 1
        else:
            fn += 1

    total = len(pope_data)
    results["accuracy"] = (tp + tn) / total
    results["precision"] = tp / (tp + fp) if (tp + fp) > 0 else 0
    results["recall"] = tp / (tp + fn) if (tp + fn) > 0 else 0
    results["f1"] = 2 * results["precision"] * results["recall"] / \
                    (results["precision"] + results["recall"]) \
                    if (results["precision"] + results["recall"]) > 0 else 0
    results["yes_ratio"] = yes_count / total

    return results
```

---

## 6.4 评估实践

### 完整评估流程

```python
class MultimodalEvaluator:
    def __init__(self, model):
        self.model = model
        self.benchmarks = {}

    def load_benchmarks(self):
        """加载所有基准测试"""
        self.benchmarks = {
            "vqav2": load_vqav2(),
            "gqa": load_gqa(),
            "textvqa": load_textvqa(),
            "pope": load_pope(),
            "mmbench": load_mmbench(),
        }

    def run_evaluation(self):
        """运行完整评估"""
        results = {}

        for name, benchmark in self.benchmarks.items():
            print(f"Evaluating {name}...")

            if name == "pope":
                results[name] = self.evaluate_pope(benchmark)
            elif name == "mmbench":
                results[name] = self.evaluate_mmbench(benchmark)
            else:
                results[name] = self.evaluate_vqa(benchmark)

        return results

    def evaluate_vqa(self, data):
        """VQA类评估"""
        correct = 0
        for item in tqdm(data):
            pred = self.model.generate(item['image'], item['question'])
            acc = evaluate_vqa_accuracy(pred, item['answers'])
            correct += acc
        return correct / len(data)

    def generate_report(self, results):
        """生成评估报告"""
        report = "=" * 50 + "\n"
        report += "Multimodal Model Evaluation Report\n"
        report += "=" * 50 + "\n\n"

        for benchmark, score in results.items():
            if isinstance(score, dict):
                report += f"{benchmark}:\n"
                for k, v in score.items():
                    report += f"  {k}: {v:.4f}\n"
            else:
                report += f"{benchmark}: {score:.4f}\n"

        return report
```

### 评估结果示例

```
==================================================
Multimodal Model Evaluation Report
==================================================

VQAv2: 0.7823
GQA: 0.6234
TextVQA: 0.5812
POPE:
  accuracy: 0.8567
  precision: 0.8234
  recall: 0.8901
  f1: 0.8555
  yes_ratio: 0.5123
MMBench:
  overall: 0.6789
  perception: 0.7234
  reasoning: 0.6123
```

---

## 6.5 常见评测陷阱

### 1. 数据泄露

```
问题: 评测数据可能在预训练数据中出现
解决:
- 使用新构建的测试集
- 检查训练数据是否包含测试样本
- 使用held-out测试集
```

### 2. Prompt敏感性

```
问题: 不同prompt格式导致结果差异大
解决:
- 报告使用的prompt模板
- 测试多种prompt取平均
- 使用标准化的prompt格式
```

### 3. 答案提取

```python
# 不好的做法
response = "The answer is B because..."
answer = response[0]  # 可能提取错误

# 好的做法
def robust_extract(response):
    # 多种策略组合
    patterns = [
        r'answer is ([A-D])',
        r'([A-D])\.',
        r'^([A-D])',
    ]
    for p in patterns:
        match = re.search(p, response, re.IGNORECASE)
        if match:
            return match.group(1).upper()
    return None
```

### 4. 评估一致性

```
问题: 不同论文报告的结果不可比
原因:
- 使用不同的测试集split
- 不同的后处理方式
- 不同的评估脚本

解决:
- 使用官方评估脚本
- 报告详细的评估设置
- 开源评估代码
```

---

## 总结

| 评估维度 | 代表基准 | 评估方式 |
|----------|----------|----------|
| 综合能力 | MMBench, MME | 选择题 |
| 视觉问答 | VQAv2, GQA | 规则匹配 |
| 文字识别 | TextVQA, DocVQA | 规则匹配 |
| 幻觉检测 | POPE | 二分类 |
| 复杂推理 | MMMU | 选择题 |

评估最佳实践：
1. **多基准评估**：单一基准不能反映全面能力
2. **报告细节**：prompt、后处理、环境等
3. **避免过拟合**：不要针对测试集调参
4. **开源代码**：便于复现和比较
