# 第三章：训练范式

多模态大模型的训练是一个系统工程，涉及预训练、指令微调、对齐优化等多个阶段。本章将深入讲解每个阶段的目标、方法和实现细节。

---

## 3.1 训练流程总览

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        多模态大模型训练流程                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  阶段0: 单模态预训练 (通常使用现成模型)                              │   │
│  │  ├─ 视觉编码器: CLIP/SigLIP/EVA (在图文对上预训练)                  │   │
│  │  └─ 语言模型: LLaMA/Qwen/Mistral (在文本上预训练)                   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                     │                                       │
│                                     ▼                                       │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  阶段1: 多模态预训练 / 特征对齐                                      │   │
│  │  ├─ 目标: 让视觉特征能被语言模型理解                                 │   │
│  │  ├─ 数据: 大规模图文对 (CC3M, LAION等)                              │   │
│  │  ├─ 训练: 通常只训练连接模块                                        │   │
│  │  └─ 规模: 数十万到数百万样本                                        │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                     │                                       │
│                                     ▼                                       │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  阶段2: 指令微调 / 视觉指令学习                                      │   │
│  │  ├─ 目标: 让模型学会遵循指令完成视觉任务                             │   │
│  │  ├─ 数据: 高质量指令数据 (LLaVA-Instruct, ShareGPT4V等)            │   │
│  │  ├─ 训练: 连接模块 + LLM (可选视觉编码器)                           │   │
│  │  └─ 规模: 数十万到数百万样本                                        │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                     │                                       │
│                                     ▼                                       │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  阶段3: 对齐优化 (可选)                                              │   │
│  │  ├─ 目标: 减少幻觉，对齐人类偏好                                     │   │
│  │  ├─ 方法: RLHF / DPO / RLAIF                                        │   │
│  │  ├─ 数据: 人类偏好标注数据                                          │   │
│  │  └─ 规模: 数万到数十万样本                                          │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 3.2 预训练阶段详解

### 3.2.1 对比学习预训练 (CLIP风格)

对比学习是建立视觉-语言对齐的基础方法。

#### 核心思想

```
在一个batch中:
- 每张图像有且仅有一个匹配的文本描述
- 学习目标: 让匹配的图文对相似度高，不匹配的相似度低

可视化:
                    Text₁  Text₂  Text₃  Text₄  ...  Textₙ
              ┌─────────────────────────────────────────────┐
    Image₁    │  ✓      ✗      ✗      ✗          ✗   │
    Image₂    │  ✗      ✓      ✗      ✗          ✗   │
    Image₃    │  ✗      ✗      ✓      ✗          ✗   │
    Image₄    │  ✗      ✗      ✗      ✓          ✗   │
    ...       │  ...                                   │
    Imageₙ    │  ✗      ✗      ✗      ✗          ✓   │
              └─────────────────────────────────────────────┘

    ✓ 表示正样本对 (对角线)
    ✗ 表示负样本对 (其他位置)
```

#### 完整实现代码

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.cuda.amp import autocast, GradScaler

class CLIPTrainer:
    """CLIP风格对比学习训练器"""

    def __init__(
        self,
        vision_encoder,
        text_encoder,
        temperature=0.07,
        learnable_temperature=True,
        distributed=False,
    ):
        self.vision_encoder = vision_encoder
        self.text_encoder = text_encoder
        self.distributed = distributed

        # 温度参数
        if learnable_temperature:
            # 可学习的温度，初始化为1/0.07≈14.3
            self.logit_scale = nn.Parameter(torch.log(torch.tensor(1.0 / temperature)))
        else:
            self.logit_scale = torch.log(torch.tensor(1.0 / temperature))

    def gather_features(self, features):
        """在分布式训练中收集所有GPU上的特征"""
        if not self.distributed:
            return features

        # 收集所有进程的特征
        gathered = [torch.zeros_like(features) for _ in range(dist.get_world_size())]
        dist.all_gather(gathered, features)

        # 拼接
        gathered[dist.get_rank()] = features  # 保持梯度
        return torch.cat(gathered, dim=0)

    def compute_loss(self, image_features, text_features):
        """
        计算对比学习损失

        Args:
            image_features: (B, D) 归一化的图像特征
            text_features: (B, D) 归一化的文本特征

        Returns:
            loss: 标量损失
            metrics: 包含accuracy等指标的字典
        """
        # 分布式收集特征（增加负样本数量）
        if self.distributed:
            all_image_features = self.gather_features(image_features)
            all_text_features = self.gather_features(text_features)
        else:
            all_image_features = image_features
            all_text_features = text_features

        # 计算相似度矩阵
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ all_text_features.t()
        logits_per_text = logit_scale * text_features @ all_image_features.t()

        # 标签
        batch_size = image_features.shape[0]
        if self.distributed:
            # 分布式训练时，标签需要考虑rank偏移
            rank = dist.get_rank()
            labels = torch.arange(batch_size, device=image_features.device) + rank * batch_size
        else:
            labels = torch.arange(batch_size, device=image_features.device)

        # 对称损失
        loss_i2t = F.cross_entropy(logits_per_image, labels)
        loss_t2i = F.cross_entropy(logits_per_text, labels)
        loss = (loss_i2t + loss_t2i) / 2

        # 计算准确率（用于监控）
        with torch.no_grad():
            pred_i2t = logits_per_image.argmax(dim=-1)
            pred_t2i = logits_per_text.argmax(dim=-1)
            acc_i2t = (pred_i2t == labels).float().mean()
            acc_t2i = (pred_t2i == labels).float().mean()

        metrics = {
            'loss': loss.item(),
            'loss_i2t': loss_i2t.item(),
            'loss_t2i': loss_t2i.item(),
            'acc_i2t': acc_i2t.item(),
            'acc_t2i': acc_t2i.item(),
            'logit_scale': logit_scale.item(),
        }

        return loss, metrics

    def train_step(self, images, texts, optimizer, scaler=None):
        """单步训练"""
        optimizer.zero_grad()

        # 混合精度训练
        with autocast(enabled=scaler is not None):
            # 编码
            image_features = self.vision_encoder(images)
            text_features = self.text_encoder(texts)

            # L2归一化
            image_features = F.normalize(image_features, dim=-1)
            text_features = F.normalize(text_features, dim=-1)

            # 计算损失
            loss, metrics = self.compute_loss(image_features, text_features)

        # 反向传播
        if scaler is not None:
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            optimizer.step()

        return metrics


class InfoNCELoss(nn.Module):
    """InfoNCE损失的模块化实现"""

    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature

    def forward(self, query, key, labels=None):
        """
        Args:
            query: (B, D) 查询特征
            key: (B, D) 或 (N, D) 键特征
            labels: 可选，指定正样本索引
        """
        # 归一化
        query = F.normalize(query, dim=-1)
        key = F.normalize(key, dim=-1)

        # 相似度
        logits = query @ key.t() / self.temperature

        # 默认标签：对角线为正样本
        if labels is None:
            labels = torch.arange(query.shape[0], device=query.device)

        return F.cross_entropy(logits, labels)
```

#### 训练技巧

```python
# 1. 大Batch Size
# CLIP使用32K batch size，需要分布式训练
# 更多负样本 = 更难的对比任务 = 更好的表示

# 2. 学习率warmup
def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

# 3. 梯度裁剪
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 4. 权重衰减（不对bias和LayerNorm应用）
def get_parameter_groups(model, weight_decay=0.1):
    decay = []
    no_decay = []
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        if len(param.shape) == 1 or name.endswith(".bias"):
            no_decay.append(param)
        else:
            decay.append(param)
    return [
        {'params': decay, 'weight_decay': weight_decay},
        {'params': no_decay, 'weight_decay': 0.0}
    ]
```

### 3.2.2 生成式预训练

让模型学会根据图像生成描述文本。

```python
class GenerativePretraining:
    """生成式预训练"""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def prepare_inputs(self, images, captions):
        """
        准备输入

        格式: [IMG_TOKENS] Caption text
        """
        batch_size = images.shape[0]

        # 图像编码
        vision_features = self.model.vision_encoder(images)
        vision_tokens = self.model.connector(vision_features)  # (B, N_v, D)

        # 文本tokenize
        text_inputs = self.tokenizer(
            captions,
            padding=True,
            truncation=True,
            max_length=256,
            return_tensors='pt'
        )

        # 获取文本embedding
        text_embeds = self.model.llm.get_input_embeddings()(text_inputs['input_ids'])

        # 拼接: [vision_tokens] + [text_tokens]
        inputs_embeds = torch.cat([vision_tokens, text_embeds], dim=1)

        # 创建attention mask
        vision_mask = torch.ones(batch_size, vision_tokens.shape[1], device=images.device)
        attention_mask = torch.cat([vision_mask, text_inputs['attention_mask']], dim=1)

        # 创建labels (只对文本部分计算loss)
        # 视觉token部分的label设为-100 (忽略)
        vision_labels = torch.full(
            (batch_size, vision_tokens.shape[1]),
            -100,
            device=images.device
        )
        text_labels = text_inputs['input_ids'].clone()
        labels = torch.cat([vision_labels, text_labels], dim=1)

        return {
            'inputs_embeds': inputs_embeds,
            'attention_mask': attention_mask,
            'labels': labels,
        }

    def train_step(self, images, captions, optimizer):
        """训练一步"""
        optimizer.zero_grad()

        # 准备输入
        inputs = self.prepare_inputs(images, captions)

        # 前向传播
        outputs = self.model.llm(
            inputs_embeds=inputs['inputs_embeds'],
            attention_mask=inputs['attention_mask'],
            labels=inputs['labels'],
        )

        loss = outputs.loss

        # 反向传播
        loss.backward()
        optimizer.step()

        return {'loss': loss.item()}
```

### 3.2.3 图文匹配预训练 (ITM)

二分类任务判断图文是否匹配。

```python
class ImageTextMatchingHead(nn.Module):
    """图文匹配分类头"""

    def __init__(self, hidden_dim):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2)
        )

    def forward(self, multimodal_features):
        """
        multimodal_features: (B, L, D)
        取[CLS]或第一个token的特征进行分类
        """
        cls_features = multimodal_features[:, 0]
        return self.classifier(cls_features)


class ITMTrainer:
    """ITM训练"""

    def __init__(self, model, itm_head):
        self.model = model
        self.itm_head = itm_head

    def create_negative_pairs(self, images, texts, strategy='hard'):
        """创建负样本对"""
        batch_size = images.shape[0]

        if strategy == 'random':
            # 随机打乱文本
            neg_indices = torch.randperm(batch_size)
            neg_texts = [texts[i] for i in neg_indices]

        elif strategy == 'hard':
            # 难负例：选择相似但不匹配的
            with torch.no_grad():
                img_feat = self.model.encode_images(images)
                txt_feat = self.model.encode_texts(texts)
                similarity = img_feat @ txt_feat.t()

                # 排除对角线（正样本）
                similarity.fill_diagonal_(-float('inf'))

                # 选择最相似的作为难负例
                neg_indices = similarity.argmax(dim=1)
                neg_texts = [texts[i] for i in neg_indices]

        return neg_texts

    def train_step(self, images, texts, optimizer):
        """训练一步"""
        optimizer.zero_grad()

        batch_size = images.shape[0]

        # 创建负样本
        neg_texts = self.create_negative_pairs(images, texts)

        # 正样本
        pos_features = self.model.encode_multimodal(images, texts)
        pos_logits = self.itm_head(pos_features)

        # 负样本
        neg_features = self.model.encode_multimodal(images, neg_texts)
        neg_logits = self.itm_head(neg_features)

        # 拼接logits和标签
        logits = torch.cat([pos_logits, neg_logits], dim=0)
        labels = torch.cat([
            torch.ones(batch_size, dtype=torch.long),
            torch.zeros(batch_size, dtype=torch.long)
        ], dim=0).to(images.device)

        # 计算损失
        loss = F.cross_entropy(logits, labels)

        # 反向传播
        loss.backward()
        optimizer.step()

        # 计算准确率
        with torch.no_grad():
            preds = logits.argmax(dim=1)
            accuracy = (preds == labels).float().mean()

        return {'loss': loss.item(), 'accuracy': accuracy.item()}
```

---

## 3.3 指令微调阶段详解

### 3.3.1 指令微调的核心概念

指令微调让模型学会遵循人类指令完成各种视觉任务。

```
指令微调前:
  输入: [图像] + "dog"
  输出: "running in the park"  # 补全式

指令微调后:
  输入: [图像] + "请描述这张图片中的狗在做什么？"
  输出: "这只金毛犬正在公园的草地上快乐地奔跑。"  # 遵循指令
```

### 3.3.2 数据格式详解

#### 标准对话格式

```python
# LLaVA风格的数据格式
training_example = {
    "id": "unique_id_123",
    "image": "path/to/image.jpg",
    "conversations": [
        {
            "from": "human",
            "value": "<image>\n请详细描述这张图片的内容。"
        },
        {
            "from": "gpt",
            "value": "这张图片展示了一个阳光明媚的公园场景。在画面中央，一只金毛犬正在绿色的草地上奔跑。狗狗的毛发在阳光下闪闪发光，它的舌头伸在外面，看起来非常快乐。背景中可以看到一些树木和远处的长椅。"
        },
        {
            "from": "human",
            "value": "这只狗看起来是什么品种？它的情绪如何？"
        },
        {
            "from": "gpt",
            "value": "从外观判断，这是一只金毛寻回犬（Golden Retriever）。金毛犬的特征包括：\n\n1. 金黄色的浓密毛发\n2. 友善的面部表情\n3. 中大型体型\n\n关于它的情绪，可以从以下几点判断它非常开心：\n- 嘴巴张开，舌头伸出，这是狗狗"微笑"的表现\n- 尾巴高高翘起（虽然图中可能看不太清）\n- 奔跑的姿态轻快有活力\n- 耳朵自然放松"
        }
    ]
}

# 多图像对话格式
multi_image_example = {
    "id": "compare_123",
    "images": ["image1.jpg", "image2.jpg"],
    "conversations": [
        {
            "from": "human",
            "value": "<image>\n<image>\n请比较这两张图片的异同。"
        },
        {
            "from": "gpt",
            "value": "让我来比较这两张图片：\n\n**相同点：**\n- 都是户外场景\n- 都有动物出现\n\n**不同点：**\n- 第一张图是狗在草地上，第二张是猫在窗台上\n- 天气/光线不同..."
        }
    ]
}
```

#### Prompt模板设计

```python
class ConversationTemplate:
    """对话模板"""

    # Vicuna风格
    VICUNA_TEMPLATE = """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.

USER: {input}
ASSISTANT: {output}"""

    # LLaMA-2风格
    LLAMA2_TEMPLATE = """<s>[INST] <<SYS>>
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.
<</SYS>>

{input} [/INST] {output}</s>"""

    # ChatML风格 (Qwen等使用)
    CHATML_TEMPLATE = """<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
{input}<|im_end|>
<|im_start|>assistant
{output}<|im_end|>"""

    @staticmethod
    def format_conversation(conversations, template_type='vicuna'):
        """格式化对话"""
        formatted = []
        for i, turn in enumerate(conversations):
            role = turn['from']
            content = turn['value']

            if role == 'human':
                if template_type == 'vicuna':
                    formatted.append(f"USER: {content}")
                elif template_type == 'llama2':
                    formatted.append(f"[INST] {content} [/INST]")
            else:  # assistant/gpt
                if template_type == 'vicuna':
                    formatted.append(f"ASSISTANT: {content}")
                elif template_type == 'llama2':
                    formatted.append(f" {content}</s>")

        return '\n'.join(formatted)
```

### 3.3.3 训练实现

```python
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from PIL import Image
import json

class InstructionDataset(Dataset):
    """指令微调数据集"""

    def __init__(
        self,
        data_path,
        image_folder,
        tokenizer,
        image_processor,
        max_length=2048,
    ):
        self.data = json.load(open(data_path))
        self.image_folder = image_folder
        self.tokenizer = tokenizer
        self.image_processor = image_processor
        self.max_length = max_length

        # 特殊token
        self.image_token = "<image>"
        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # 加载图像
        image_path = os.path.join(self.image_folder, item['image'])
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.image_processor(image)

        # 处理对话
        conversations = item['conversations']
        input_ids, labels = self.process_conversations(conversations)

        return {
            'image': image_tensor,
            'input_ids': input_ids,
            'labels': labels,
            'attention_mask': (input_ids != self.tokenizer.pad_token_id).long(),
        }

    def process_conversations(self, conversations):
        """处理对话为模型输入格式"""
        input_ids = []
        labels = []

        for turn in conversations:
            role = turn['from']
            content = turn['value']

            # 替换<image>为特殊token
            if self.image_token in content:
                # 实际实现中，会在forward时用视觉token替换
                content = content.replace(self.image_token, "<image>" * 576)

            # tokenize
            if role == 'human':
                # 用户输入前加前缀
                text = f"USER: {content}\nASSISTANT:"
                tokens = self.tokenizer.encode(text, add_special_tokens=False)
                input_ids.extend(tokens)
                # 用户部分不计算loss
                labels.extend([-100] * len(tokens))

            else:  # assistant
                # 助手回复
                text = f" {content}</s>"
                tokens = self.tokenizer.encode(text, add_special_tokens=False)
                input_ids.extend(tokens)
                # 助手部分计算loss
                labels.extend(tokens)

        # 截断/填充
        if len(input_ids) > self.max_length:
            input_ids = input_ids[:self.max_length]
            labels = labels[:self.max_length]
        else:
            padding_length = self.max_length - len(input_ids)
            input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length
            labels = labels + [-100] * padding_length

        return torch.tensor(input_ids), torch.tensor(labels)


class InstructionTuningTrainer:
    """指令微调训练器"""

    def __init__(
        self,
        model,
        tokenizer,
        train_dataset,
        val_dataset=None,
        learning_rate=2e-5,
        batch_size=16,
        num_epochs=3,
        warmup_ratio=0.03,
        gradient_accumulation_steps=1,
        max_grad_norm=1.0,
        freeze_vision_encoder=True,
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset

        # 训练配置
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        self.warmup_ratio = warmup_ratio
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.max_grad_norm = max_grad_norm

        # 冻结视觉编码器
        if freeze_vision_encoder:
            for param in model.vision_encoder.parameters():
                param.requires_grad = False

        # 设置数据加载器
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True,
        )

        # 优化器
        self.optimizer = self._create_optimizer()

        # 学习率调度器
        total_steps = len(self.train_loader) * num_epochs // gradient_accumulation_steps
        warmup_steps = int(total_steps * warmup_ratio)
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )

    def _create_optimizer(self):
        """创建优化器，对不同参数组使用不同学习率"""
        # 区分参数组
        connector_params = []
        llm_params = []

        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue
            if 'connector' in name or 'projector' in name:
                connector_params.append(param)
            else:
                llm_params.append(param)

        param_groups = [
            {'params': connector_params, 'lr': self.learning_rate},
            {'params': llm_params, 'lr': self.learning_rate * 0.1},  # LLM用较小学习率
        ]

        return torch.optim.AdamW(param_groups, weight_decay=0.01)

    def train_epoch(self, epoch):
        """训练一个epoch"""
        self.model.train()
        total_loss = 0
        num_steps = 0

        progress_bar = tqdm(self.train_loader, desc=f"Epoch {epoch}")

        for step, batch in enumerate(progress_bar):
            # 移到GPU
            images = batch['image'].cuda()
            input_ids = batch['input_ids'].cuda()
            labels = batch['labels'].cuda()
            attention_mask = batch['attention_mask'].cuda()

            # 编码图像
            vision_tokens = self.model.encode_images(images)

            # 替换input中的image token为vision tokens
            inputs_embeds = self._prepare_inputs_embeds(
                input_ids, vision_tokens
            )

            # 前向传播
            outputs = self.model.llm(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                labels=labels,
            )

            loss = outputs.loss / self.gradient_accumulation_steps

            # 反向传播
            loss.backward()

            # 梯度累积
            if (step + 1) % self.gradient_accumulation_steps == 0:
                # 梯度裁剪
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.max_grad_norm
                )

                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()

            total_loss += loss.item() * self.gradient_accumulation_steps
            num_steps += 1

            # 更新进度条
            progress_bar.set_postfix({
                'loss': total_loss / num_steps,
                'lr': self.scheduler.get_last_lr()[0]
            })

        return total_loss / num_steps

    def _prepare_inputs_embeds(self, input_ids, vision_tokens):
        """
        准备输入embedding，将<image> token替换为实际的视觉token

        这是一个关键函数，需要正确处理token替换
        """
        batch_size = input_ids.shape[0]
        device = input_ids.device

        # 获取文本embedding
        text_embeds = self.model.llm.get_input_embeddings()(input_ids)

        # 找到每个样本中<image> token的位置
        image_token_id = self.tokenizer.convert_tokens_to_ids("<image>")

        new_embeds_list = []
        for b in range(batch_size):
            # 找到<image> token的位置
            image_positions = (input_ids[b] == image_token_id).nonzero(as_tuple=True)[0]

            if len(image_positions) == 0:
                new_embeds_list.append(text_embeds[b])
                continue

            # 假设图像token连续出现
            start_pos = image_positions[0].item()
            end_pos = image_positions[-1].item() + 1

            # 拼接: [前部分文本] + [视觉token] + [后部分文本]
            new_embeds = torch.cat([
                text_embeds[b, :start_pos],
                vision_tokens[b],
                text_embeds[b, end_pos:]
            ], dim=0)

            new_embeds_list.append(new_embeds)

        # 填充到相同长度
        max_len = max(e.shape[0] for e in new_embeds_list)
        padded_embeds = torch.zeros(
            batch_size, max_len, text_embeds.shape[-1],
            device=device, dtype=text_embeds.dtype
        )
        for b, embeds in enumerate(new_embeds_list):
            padded_embeds[b, :embeds.shape[0]] = embeds

        return padded_embeds

    def train(self):
        """完整训练流程"""
        best_loss = float('inf')

        for epoch in range(self.num_epochs):
            train_loss = self.train_epoch(epoch)

            # 验证
            if self.val_dataset is not None:
                val_loss = self.validate()
                print(f"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}")

                # 保存最佳模型
                if val_loss < best_loss:
                    best_loss = val_loss
                    self.save_checkpoint(f"best_model.pt")
            else:
                print(f"Epoch {epoch}: train_loss={train_loss:.4f}")

            # 每个epoch保存
            self.save_checkpoint(f"checkpoint_epoch_{epoch}.pt")

    def save_checkpoint(self, path):
        """保存checkpoint"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
        }, path)
```

### 3.3.4 LLaVA两阶段训练详解

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        LLaVA 两阶段训练                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  阶段1: 预训练对齐 (Feature Alignment)                                       │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  目标: 让投影层学会将视觉特征"翻译"为LLM可理解的表示                  │   │
│  │                                                                     │   │
│  │  数据:                                                              │   │
│  │  ├─ CC3M过滤后的558K图文对                                          │   │
│  │  └─ 简单描述任务: 给图像生成caption                                 │   │
│  │                                                                     │   │
│  │  训练设置:                                                          │   │
│  │  ├─ 冻结: CLIP ViT-L/14@336 (视觉编码器)                           │   │
│  │  ├─ 冻结: Vicuna-7B/13B (语言模型)                                  │   │
│  │  ├─ 训练: 只训练MLP投影层 (~20M参数)                                │   │
│  │  ├─ 学习率: 1e-3 (较大，因为只训练投影层)                           │   │
│  │  ├─ Batch size: 256                                                │   │
│  │  ├─ Epochs: 1                                                      │   │
│  │  └─ 耗时: 约4小时 (8×A100)                                         │   │
│  │                                                                     │   │
│  │  数据格式示例:                                                      │   │
│  │  {                                                                  │   │
│  │    "image": "00001.jpg",                                            │   │
│  │    "conversations": [                                               │   │
│  │      {"from": "human", "value": "<image>\nProvide a brief..."},    │   │
│  │      {"from": "gpt", "value": "A golden retriever running..."}     │   │
│  │    ]                                                                │   │
│  │  }                                                                  │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                     │                                       │
│                                     ▼                                       │
│  阶段2: 视觉指令微调 (Visual Instruction Tuning)                            │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  目标: 让模型学会遵循各种视觉指令                                    │   │
│  │                                                                     │   │
│  │  数据 (LLaVA-1.5使用665K混合数据):                                   │   │
│  │  ├─ LLaVA-Instruct-158K: GPT-4生成的多轮对话                        │   │
│  │  ├─ VQA-v2: 视觉问答                                                │   │
│  │  ├─ GQA: 组合推理                                                   │   │
│  │  ├─ OKVQA: 知识问答                                                 │   │
│  │  ├─ OCRVQA: OCR问答                                                 │   │
│  │  ├─ TextCaps: 文字描述                                              │   │
│  │  ├─ RefCOCO: 指代理解                                               │   │
│  │  └─ ShareGPT: 纯文本对话(保持语言能力)                              │   │
│  │                                                                     │   │
│  │  训练设置:                                                          │   │
│  │  ├─ 冻结: CLIP ViT-L/14@336                                        │   │
│  │  ├─ 训练: MLP投影层 + Vicuna全参数                                  │   │
│  │  ├─ 学习率: 2e-5 (较小，保护预训练知识)                             │   │
│  │  ├─ Batch size: 128                                                │   │
│  │  ├─ Epochs: 1                                                      │   │
│  │  └─ 耗时: 约20小时 (8×A100)                                        │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 3.4 RLHF / 对齐优化

### 3.4.1 为什么需要对齐

```
问题1: 幻觉 (Hallucination)
  用户: 图中有几只猫？
  模型: 图中有3只猫。  # 实际只有2只

问题2: 不遵循指令
  用户: 简短回答，图中是什么动物？
  模型: 这是一张非常有趣的图片...（长篇大论）

问题3: 有害内容
  用户: 如何用这张图制作虚假新闻？
  模型: 你可以...（提供有害指导）

解决方案: 通过人类反馈进行对齐
```

### 3.4.2 RLHF完整流程

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           RLHF 完整流程                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Step 1: 收集人类偏好数据                                                    │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  输入: 图像 + 问题                                                  │   │
│  │       [狗的图片] + "描述这张图片"                                   │   │
│  │                                                                     │   │
│  │  模型生成多个候选回答:                                              │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │ Response A: "图中有一只可爱的金毛犬在公园里跑步。"           │   │   │
│  │  │                                                             │   │   │
│  │  │ Response B: "这是一只狗。它在外面。看起来很开心。"           │   │   │
│  │  │                                                             │   │   │
│  │  │ Response C: "图片显示一只金毛猎犬在草地上。不对，              │   │   │
│  │  │             是两只狗在玩耍。" (有幻觉)                       │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │                                                                     │   │
│  │  人类标注员对比评分: A > B > C                                      │   │
│  │  - A最好: 描述准确、流畅、有细节                                   │   │
│  │  - B次之: 正确但过于简单                                           │   │
│  │  - C最差: 包含幻觉信息                                             │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                     │                                       │
│                                     ▼                                       │
│  Step 2: 训练奖励模型 (Reward Model)                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  架构: 多模态模型 + 标量输出头                                      │   │
│  │  输入: (图像, 问题, 回答)                                           │   │
│  │  输出: 标量分数 r ∈ ℝ (越高越好)                                    │   │
│  │                                                                     │   │
│  │  训练目标: Bradley-Terry模型                                        │   │
│  │  对于偏好对 (chosen, rejected):                                     │   │
│  │                                                                     │   │
│  │  L = -log(σ(r(chosen) - r(rejected)))                              │   │
│  │                                                                     │   │
│  │  直觉: 让chosen的得分比rejected高                                   │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                     │                                       │
│                                     ▼                                       │
│  Step 3: PPO强化学习优化                                                    │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  目标函数:                                                          │   │
│  │  J(θ) = E[r(response)] - β · KL(π_θ || π_ref)                      │   │
│  │                                                                     │   │
│  │  其中:                                                              │   │
│  │  - r(response): 奖励模型给的分数                                    │   │
│  │  - KL: 与参考模型的KL散度 (防止偏离太远)                            │   │
│  │  - β: KL惩罚系数 (通常0.01-0.1)                                    │   │
│  │                                                                     │   │
│  │  PPO具体步骤:                                                       │   │
│  │  1. 用当前策略生成回答                                              │   │
│  │  2. 用奖励模型计算奖励                                              │   │
│  │  3. 计算优势函数 (GAE)                                              │   │
│  │  4. 更新策略 (带clipping的策略梯度)                                 │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.4.3 RLHF实现代码

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical

class RewardModel(nn.Module):
    """奖励模型"""

    def __init__(self, base_model, hidden_dim=4096):
        super().__init__()
        self.base_model = base_model

        # 奖励头：输出标量分数
        self.reward_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, images, input_ids, attention_mask):
        """
        返回每个样本的奖励分数
        """
        # 获取模型的最后隐藏状态
        outputs = self.base_model(
            images=images,
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
        )

        # 取最后一个token的隐藏状态
        last_hidden = outputs.hidden_states[-1]
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_indices = torch.arange(last_hidden.shape[0], device=last_hidden.device)
        last_token_hidden = last_hidden[batch_indices, sequence_lengths]

        # 计算奖励
        reward = self.reward_head(last_token_hidden).squeeze(-1)

        return reward


class RewardModelTrainer:
    """奖励模型训练器"""

    def __init__(self, reward_model, learning_rate=1e-5):
        self.model = reward_model
        self.optimizer = torch.optim.AdamW(
            reward_model.parameters(),
            lr=learning_rate
        )

    def compute_loss(self, chosen_rewards, rejected_rewards):
        """
        计算奖励模型损失

        Args:
            chosen_rewards: (B,) 被选中(更好)的回答的奖励
            rejected_rewards: (B,) 被拒绝(更差)的回答的奖励

        Returns:
            loss: 标量损失
        """
        # Bradley-Terry损失
        loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()

        # 额外的正则化：防止奖励值过大
        reg_loss = 0.001 * (chosen_rewards.pow(2).mean() + rejected_rewards.pow(2).mean())

        return loss + reg_loss

    def train_step(self, batch):
        """训练一步"""
        self.optimizer.zero_grad()

        # 计算chosen样本的奖励
        chosen_rewards = self.model(
            images=batch['images'],
            input_ids=batch['chosen_input_ids'],
            attention_mask=batch['chosen_attention_mask'],
        )

        # 计算rejected样本的奖励
        rejected_rewards = self.model(
            images=batch['images'],
            input_ids=batch['rejected_input_ids'],
            attention_mask=batch['rejected_attention_mask'],
        )

        # 损失
        loss = self.compute_loss(chosen_rewards, rejected_rewards)

        loss.backward()
        self.optimizer.step()

        # 计算准确率
        with torch.no_grad():
            accuracy = (chosen_rewards > rejected_rewards).float().mean()

        return {
            'loss': loss.item(),
            'accuracy': accuracy.item(),
            'chosen_reward_mean': chosen_rewards.mean().item(),
            'rejected_reward_mean': rejected_rewards.mean().item(),
        }


class PPOTrainer:
    """PPO训练器"""

    def __init__(
        self,
        policy_model,
        ref_model,
        reward_model,
        tokenizer,
        learning_rate=1e-6,
        kl_coef=0.1,
        clip_range=0.2,
        value_coef=0.5,
        max_grad_norm=1.0,
    ):
        self.policy = policy_model
        self.ref_model = ref_model
        self.reward_model = reward_model
        self.tokenizer = tokenizer

        self.kl_coef = kl_coef
        self.clip_range = clip_range
        self.value_coef = value_coef
        self.max_grad_norm = max_grad_norm

        # 冻结参考模型和奖励模型
        for param in ref_model.parameters():
            param.requires_grad = False
        for param in reward_model.parameters():
            param.requires_grad = False

        self.optimizer = torch.optim.AdamW(
            policy_model.parameters(),
            lr=learning_rate
        )

    @torch.no_grad()
    def generate_responses(self, images, prompts, max_length=256):
        """生成回答"""
        input_ids = self.tokenizer(
            prompts,
            return_tensors='pt',
            padding=True,
        ).input_ids.cuda()

        # 生成
        outputs = self.policy.generate(
            images=images,
            input_ids=input_ids,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            return_dict_in_generate=True,
            output_scores=True,
        )

        return outputs.sequences, outputs.scores

    @torch.no_grad()
    def compute_rewards_and_kl(self, images, input_ids, attention_mask):
        """计算奖励和KL散度"""
        # 奖励模型分数
        rewards = self.reward_model(images, input_ids, attention_mask)

        # 计算策略模型的log概率
        policy_outputs = self.policy(
            images=images,
            input_ids=input_ids,
            attention_mask=attention_mask,
        )
        policy_logprobs = self._get_logprobs(policy_outputs.logits, input_ids)

        # 计算参考模型的log概率
        ref_outputs = self.ref_model(
            images=images,
            input_ids=input_ids,
            attention_mask=attention_mask,
        )
        ref_logprobs = self._get_logprobs(ref_outputs.logits, input_ids)

        # KL散度
        kl = policy_logprobs - ref_logprobs

        return rewards, kl, policy_logprobs

    def _get_logprobs(self, logits, labels):
        """从logits计算log概率"""
        # Shift
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()

        # Log softmax
        log_probs = F.log_softmax(shift_logits, dim=-1)

        # Gather
        labels_log_probs = torch.gather(
            log_probs,
            dim=-1,
            index=shift_labels.unsqueeze(-1)
        ).squeeze(-1)

        return labels_log_probs

    def ppo_step(self, batch):
        """PPO更新一步"""
        images = batch['images']
        prompts = batch['prompts']

        # 1. 生成回答
        response_ids, _ = self.generate_responses(images, prompts)

        # 2. 准备输入
        full_input_ids = response_ids
        attention_mask = (full_input_ids != self.tokenizer.pad_token_id).long()

        # 3. 计算旧的log概率（用于重要性采样）
        with torch.no_grad():
            rewards, kl, old_logprobs = self.compute_rewards_and_kl(
                images, full_input_ids, attention_mask
            )

        # 4. 计算优势 (这里简化为 reward - baseline)
        # 完整PPO应该用GAE计算优势
        baseline = rewards.mean()
        advantages = rewards - baseline - self.kl_coef * kl.sum(dim=-1)

        # 5. PPO更新
        for _ in range(4):  # 通常更新多次
            # 当前策略的log概率
            policy_outputs = self.policy(
                images=images,
                input_ids=full_input_ids,
                attention_mask=attention_mask,
            )
            new_logprobs = self._get_logprobs(policy_outputs.logits, full_input_ids)

            # 重要性比率
            ratio = torch.exp(new_logprobs.sum(dim=-1) - old_logprobs.sum(dim=-1))

            # Clipped surrogate objective
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_range, 1 + self.clip_range) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()

            # 更新
            self.optimizer.zero_grad()
            policy_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
            self.optimizer.step()

        return {
            'policy_loss': policy_loss.item(),
            'reward_mean': rewards.mean().item(),
            'kl_mean': kl.sum(dim=-1).mean().item(),
        }
```

### 3.4.4 DPO: 简化的对齐方法

DPO (Direct Preference Optimization) 直接优化偏好，不需要训练奖励模型。

```python
class DPOTrainer:
    """DPO训练器"""

    def __init__(
        self,
        policy_model,
        ref_model,
        tokenizer,
        beta=0.1,
        learning_rate=1e-6,
    ):
        self.policy = policy_model
        self.ref_model = ref_model
        self.tokenizer = tokenizer
        self.beta = beta

        # 冻结参考模型
        for param in ref_model.parameters():
            param.requires_grad = False

        self.optimizer = torch.optim.AdamW(
            policy_model.parameters(),
            lr=learning_rate
        )

    def compute_logprobs(self, model, images, input_ids, attention_mask, labels):
        """计算序列的log概率"""
        outputs = model(
            images=images,
            input_ids=input_ids,
            attention_mask=attention_mask,
        )

        logits = outputs.logits[:, :-1]
        labels = labels[:, 1:]

        log_probs = F.log_softmax(logits, dim=-1)
        selected_log_probs = torch.gather(
            log_probs,
            dim=-1,
            index=labels.unsqueeze(-1)
        ).squeeze(-1)

        # 只计算response部分的log概率
        # 需要一个mask来区分prompt和response
        return selected_log_probs.sum(dim=-1)

    def dpo_loss(self, images, chosen_ids, chosen_mask, rejected_ids, rejected_mask, chosen_labels, rejected_labels):
        """
        DPO损失函数

        L_DPO = -log σ(β * (log π(y_w|x)/π_ref(y_w|x) - log π(y_l|x)/π_ref(y_l|x)))

        其中:
        - y_w: chosen (赢的回答)
        - y_l: rejected (输的回答)
        - π: 策略模型
        - π_ref: 参考模型
        """
        # 策略模型的log概率
        policy_chosen_logps = self.compute_logprobs(
            self.policy, images, chosen_ids, chosen_mask, chosen_labels
        )
        policy_rejected_logps = self.compute_logprobs(
            self.policy, images, rejected_ids, rejected_mask, rejected_labels
        )

        # 参考模型的log概率
        with torch.no_grad():
            ref_chosen_logps = self.compute_logprobs(
                self.ref_model, images, chosen_ids, chosen_mask, chosen_labels
            )
            ref_rejected_logps = self.compute_logprobs(
                self.ref_model, images, rejected_ids, rejected_mask, rejected_labels
            )

        # DPO损失
        chosen_rewards = self.beta * (policy_chosen_logps - ref_chosen_logps)
        rejected_rewards = self.beta * (policy_rejected_logps - ref_rejected_logps)

        loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()

        # 额外指标
        with torch.no_grad():
            reward_margin = (chosen_rewards - rejected_rewards).mean()
            accuracy = (chosen_rewards > rejected_rewards).float().mean()

        return loss, {
            'loss': loss.item(),
            'reward_margin': reward_margin.item(),
            'accuracy': accuracy.item(),
            'chosen_reward': chosen_rewards.mean().item(),
            'rejected_reward': rejected_rewards.mean().item(),
        }

    def train_step(self, batch):
        """训练一步"""
        self.optimizer.zero_grad()

        loss, metrics = self.dpo_loss(
            images=batch['images'],
            chosen_ids=batch['chosen_input_ids'],
            chosen_mask=batch['chosen_attention_mask'],
            rejected_ids=batch['rejected_input_ids'],
            rejected_mask=batch['rejected_attention_mask'],
            chosen_labels=batch['chosen_labels'],
            rejected_labels=batch['rejected_labels'],
        )

        loss.backward()
        self.optimizer.step()

        return metrics
```

### 3.4.5 RLHF vs DPO 对比

| 特性 | RLHF (PPO) | DPO |
|------|------------|-----|
| **复杂度** | 高（需要4个模型） | 低（只需2个模型） |
| **模型** | 策略、参考、奖励、值函数 | 策略、参考 |
| **训练稳定性** | 较难调参 | 更稳定 |
| **计算资源** | 更多 | 更少 |
| **效果** | 略好（充分调优后） | 接近RLHF |
| **应用场景** | 大规模对齐 | 快速迭代 |

---

## 3.5 高效训练技术

### 3.5.1 LoRA微调

```python
import torch
import torch.nn as nn
import math

class LoRALayer(nn.Module):
    """LoRA层"""

    def __init__(
        self,
        in_features,
        out_features,
        rank=8,
        alpha=16,
        dropout=0.0,
    ):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank

        # 低秩分解矩阵
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))

        # 初始化
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)

        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

    def forward(self, x):
        """
        LoRA前向: Wx + (BA)x * scaling
        """
        return self.dropout(x @ self.lora_A.T @ self.lora_B.T) * self.scaling


class LinearWithLoRA(nn.Module):
    """带LoRA的线性层"""

    def __init__(self, linear_layer, rank=8, alpha=16):
        super().__init__()
        self.linear = linear_layer
        self.lora = LoRALayer(
            linear_layer.in_features,
            linear_layer.out_features,
            rank=rank,
            alpha=alpha,
        )

        # 冻结原始层
        for param in self.linear.parameters():
            param.requires_grad = False

    def forward(self, x):
        return self.linear(x) + self.lora(x)


def apply_lora_to_model(model, rank=8, alpha=16, target_modules=['q_proj', 'v_proj']):
    """给模型添加LoRA"""
    for name, module in model.named_modules():
        if any(target in name for target in target_modules):
            if isinstance(module, nn.Linear):
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]
                parent = model.get_submodule(parent_name)
                setattr(parent, child_name, LinearWithLoRA(module, rank, alpha))

    return model
```

### 3.5.2 梯度检查点

```python
from torch.utils.checkpoint import checkpoint

class TransformerBlockWithCheckpoint(nn.Module):
    """带梯度检查点的Transformer块"""

    def __init__(self, block, use_checkpoint=True):
        super().__init__()
        self.block = block
        self.use_checkpoint = use_checkpoint

    def forward(self, x, **kwargs):
        if self.use_checkpoint and self.training:
            return checkpoint(self.block, x, **kwargs)
        else:
            return self.block(x, **kwargs)
```

### 3.5.3 混合精度训练

```python
from torch.cuda.amp import autocast, GradScaler

class MixedPrecisionTrainer:
    def __init__(self, model, optimizer):
        self.model = model
        self.optimizer = optimizer
        self.scaler = GradScaler()

    def train_step(self, batch):
        self.optimizer.zero_grad()

        # 自动混合精度
        with autocast():
            outputs = self.model(**batch)
            loss = outputs.loss

        # 缩放损失并反向传播
        self.scaler.scale(loss).backward()

        # 更新参数
        self.scaler.step(self.optimizer)
        self.scaler.update()

        return loss.item()
```

---

## 3.6 总结

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        训练范式总结                                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  阶段       │ 目标           │ 数据规模    │ 训练参数      │ 学习率        │
│  ─────────────────────────────────────────────────────────────────────────  │
│  预训练     │ 模态对齐       │ 数百万-亿级 │ 编码器+投影层 │ 1e-4 ~ 1e-3   │
│  指令微调   │ 指令遵循       │ 数十万-百万 │ 投影层+LLM    │ 1e-5 ~ 2e-5   │
│  RLHF/DPO  │ 人类对齐       │ 数万-数十万 │ 全模型/部分   │ 1e-6 ~ 1e-5   │
│                                                                             │
│  关键技巧:                                                                   │
│  1. 渐进式解冻: 先训简单模块，再解冻复杂模块                                  │
│  2. 学习率差异: 预训练组件用小学习率                                          │
│  3. 数据质量: 高质量数据比数量更重要                                          │
│  4. 混合数据: 指令微调时混合多种任务数据                                      │
│  5. 高效方法: LoRA/梯度检查点/混合精度                                        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```
