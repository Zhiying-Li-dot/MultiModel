# 第三章：文本生成

## 3.1 语言模型基础

### 语言模型定义

语言模型是对文本序列的概率分布建模：

```
P(w_1, w_2, ..., w_n) = P(w_1) × P(w_2|w_1) × P(w_3|w_1,w_2) × ... × P(w_n|w_1,...,w_{n-1})

自回归分解：每个词的概率取决于之前所有词
```

### 发展历程

```
N-gram (统计方法)
    │
    │  局限：无法捕捉长距离依赖
    ▼
RNN/LSTM (2010s)
    │
    │  局限：顺序计算慢，长序列梯度消失
    ▼
Transformer (2017)
    │
    │  突破：并行计算，自注意力机制
    ▼
GPT系列 (2018-)
    │
    │  规模化：参数量从1.17亿到1.76万亿
    ▼
LLaMA/开源模型 (2023-)
        高效开源大模型
```

---

## 3.2 Transformer架构

### 核心组件

```
          ┌─────────────────────────────────────────┐
          │            Transformer Block             │
          └─────────────────────────────────────────┘

输入 x ──────────────────────────────────────────────────►
   │                                                      │
   │     ┌───────────────────┐                           │
   ├────►│ Multi-Head Self   │                           │
   │     │    Attention      │                           │
   │     └─────────┬─────────┘                           │
   │               │                                     │
   │               ▼                                     │
   │     ┌───────────────────┐                           │
   └────►│    Add & Norm     │◄──────────────────────────┘
         └─────────┬─────────┘
                   │
                   ▼
         ┌───────────────────┐
   ┌────►│  Feed Forward     │
   │     │     Network       │
   │     └─────────┬─────────┘
   │               │
   │               ▼
   │     ┌───────────────────┐
   └────►│    Add & Norm     │──────────────────────────► 输出
         └───────────────────┘
```

### Self-Attention

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Attention(Q, K, V) = softmax(QK^T / √d_k) V

    Q: Query (B, H, N, d_k)
    K: Key   (B, H, N, d_k)
    V: Value (B, H, N, d_v)
    """
    d_k = Q.size(-1)

    # 计算注意力分数
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

    # 应用mask（用于因果语言模型）
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # Softmax归一化
    attn_weights = F.softmax(scores, dim=-1)

    # 加权求和
    output = torch.matmul(attn_weights, V)

    return output, attn_weights


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=768, n_heads=12):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        B, N, _ = x.shape

        # 线性变换并分头
        Q = self.W_q(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)

        # 注意力计算
        out, _ = scaled_dot_product_attention(Q, K, V, mask)

        # 合并多头
        out = out.transpose(1, 2).contiguous().view(B, N, -1)

        return self.W_o(out)
```

### Causal Mask (因果掩码)

```
用于自回归生成，确保每个位置只能看到之前的位置

位置:    1   2   3   4   5
      ┌───┬───┬───┬───┬───┐
    1 │ 1 │ 0 │ 0 │ 0 │ 0 │  位置1只能看位置1
      ├───┼───┼───┼───┼───┤
    2 │ 1 │ 1 │ 0 │ 0 │ 0 │  位置2能看位置1,2
      ├───┼───┼───┼───┼───┤
    3 │ 1 │ 1 │ 1 │ 0 │ 0 │  位置3能看位置1,2,3
      ├───┼───┼───┼───┼───┤
    4 │ 1 │ 1 │ 1 │ 1 │ 0 │  ...
      ├───┼───┼───┼───┼───┤
    5 │ 1 │ 1 │ 1 │ 1 │ 1 │
      └───┴───┴───┴───┴───┘

def create_causal_mask(seq_len):
    return torch.tril(torch.ones(seq_len, seq_len))
```

---

## 3.3 GPT架构

### GPT模型结构

```
              ┌─────────────────────────────────────┐
              │           GPT 架构                  │
              └─────────────────────────────────────┘

输入: "The cat sat on"
         │
         ▼
┌─────────────────────────────┐
│     Token Embedding         │  词汇表映射
│   + Position Embedding      │  位置编码
└───────────────┬─────────────┘
                │
                ▼
┌─────────────────────────────┐
│  Transformer Decoder Block  │ ×N层
│  (Causal Self-Attention)    │
└───────────────┬─────────────┘
                │
                ▼
┌─────────────────────────────┐
│      Layer Norm             │
└───────────────┬─────────────┘
                │
                ▼
┌─────────────────────────────┐
│    Linear (vocab_size)      │  输出词汇表大小
└───────────────┬─────────────┘
                │
                ▼
            Softmax
                │
                ▼
输出: P("the"|context), P("mat"|context), ...
      下一个词的概率分布
```

### GPT实现

```python
class GPT(nn.Module):
    def __init__(self, vocab_size, d_model=768, n_layers=12, n_heads=12, max_len=1024):
        super().__init__()

        # Embeddings
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_len, d_model)

        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads) for _ in range(n_layers)
        ])

        self.ln_f = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)

        # 权重共享
        self.token_embedding.weight = self.head.weight

    def forward(self, x, targets=None):
        B, T = x.shape

        # Embeddings
        tok_emb = self.token_embedding(x)
        pos_emb = self.position_embedding(torch.arange(T, device=x.device))
        x = tok_emb + pos_emb

        # Transformer blocks
        for block in self.blocks:
            x = block(x)

        x = self.ln_f(x)
        logits = self.head(x)

        # 计算损失
        loss = None
        if targets is not None:
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1)
            )

        return logits, loss


class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.attn = CausalSelfAttention(d_model, n_heads)
        self.ln2 = nn.LayerNorm(d_model)
        self.mlp = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model),
        )

    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x
```

### GPT系列演进

| 模型 | 参数量 | 发布时间 | 特点 |
|------|--------|----------|------|
| GPT-1 | 117M | 2018.6 | 首次预训练+微调范式 |
| GPT-2 | 1.5B | 2019.2 | 零样本能力涌现 |
| GPT-3 | 175B | 2020.6 | In-context Learning |
| GPT-3.5 | - | 2022.11 | ChatGPT基础 |
| GPT-4 | ~1.76T (MoE) | 2023.3 | 多模态，最强能力 |

---

## 3.4 解码策略

### 贪婪解码 (Greedy Decoding)

```python
def greedy_decode(model, prompt, max_length=100):
    """每步选择概率最高的token"""
    tokens = tokenize(prompt)

    for _ in range(max_length):
        logits = model(tokens)
        next_token = logits[:, -1, :].argmax(dim=-1)
        tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)

        if next_token == EOS_TOKEN:
            break

    return tokens

# 优点: 确定性，速度快
# 缺点: 容易重复，缺乏多样性
```

### 温度采样 (Temperature Sampling)

```python
def temperature_sample(logits, temperature=1.0):
    """
    temperature < 1: 更确定性（分布更尖锐）
    temperature = 1: 原始分布
    temperature > 1: 更随机（分布更平坦）
    """
    logits = logits / temperature
    probs = F.softmax(logits, dim=-1)
    next_token = torch.multinomial(probs, num_samples=1)
    return next_token

# temperature=0.7 通常是好的默认值
```

### Top-K 采样

```python
def top_k_sample(logits, k=50):
    """只从概率最高的k个token中采样"""
    # 找到top-k
    values, indices = logits.topk(k)

    # 将其他位置设为-inf
    logits_filtered = torch.full_like(logits, float('-inf'))
    logits_filtered.scatter_(1, indices, values)

    probs = F.softmax(logits_filtered, dim=-1)
    next_token = torch.multinomial(probs, num_samples=1)
    return next_token
```

### Top-P (Nucleus) 采样

```python
def top_p_sample(logits, p=0.9):
    """选择累积概率达到p的最小token集合"""
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

    # 找到累积概率超过p的位置
    sorted_indices_to_remove = cumulative_probs > p
    # 保留第一个超过p的token
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = False

    # 过滤
    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
    logits[indices_to_remove] = float('-inf')

    probs = F.softmax(logits, dim=-1)
    next_token = torch.multinomial(probs, num_samples=1)
    return next_token

# Top-P比Top-K更灵活，根据分布自适应调整
```

### 解码策略对比

```
输入: "The weather today is"

贪婪解码: "The weather today is sunny and warm and sunny and warm and..."
         (容易陷入重复)

Temperature=0.3: "The weather today is sunny and pleasant."
                 (保守但连贯)

Temperature=1.5: "The weather today is mysteriously purple with dancing clouds!"
                 (创意但可能不连贯)

Top-P=0.9: "The weather today is beautiful with clear skies."
           (平衡质量和多样性)
```

### 常用参数组合

| 场景 | Temperature | Top-P | Top-K |
|------|-------------|-------|-------|
| 代码生成 | 0.2-0.4 | 0.9 | - |
| 创意写作 | 0.8-1.0 | 0.95 | - |
| 对话 | 0.7 | 0.9 | 50 |
| 事实问答 | 0.3 | 0.85 | - |

---

## 3.5 Prompt Engineering

### Prompt结构

```
┌─────────────────────────────────────────────────────┐
│                  Prompt 结构                        │
├─────────────────────────────────────────────────────┤
│                                                     │
│  System Prompt (系统提示)                           │
│  ├─ 定义AI角色和行为                               │
│  └─ "You are a helpful assistant..."               │
│                                                     │
│  Context (上下文)                                   │
│  ├─ 背景信息                                       │
│  └─ 相关文档、示例等                               │
│                                                     │
│  Few-shot Examples (示例)                          │
│  ├─ 输入-输出对                                    │
│  └─ 展示期望的格式和风格                           │
│                                                     │
│  User Query (用户问题)                              │
│  └─ 实际的任务或问题                               │
│                                                     │
│  Output Format (输出格式)                           │
│  └─ 指定期望的响应格式                             │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 常用Prompt技术

#### 1. Zero-shot Prompting

```
Prompt: "Translate the following English text to French: 'Hello, how are you?'"

Output: "Bonjour, comment allez-vous?"

# 不提供示例，直接给任务
```

#### 2. Few-shot Prompting

```
Prompt:
"""
Classify the sentiment of the following reviews:

Review: "This movie was amazing!" -> Positive
Review: "I hated every minute of it." -> Negative
Review: "The acting was brilliant but the plot was confusing." -> Mixed

Review: "The food was delicious and the service was excellent." ->
"""

Output: "Positive"

# 提供几个示例，模型学习模式
```

#### 3. Chain-of-Thought (CoT)

```
Prompt:
"""
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.
Each can has 3 tennis balls. How many tennis balls does he have now?

A: Let me think step by step.
Roger started with 5 balls.
He bought 2 cans × 3 balls = 6 balls.
Total = 5 + 6 = 11 balls.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and
bought 6 more, how many apples do they have?

A: Let me think step by step.
"""

Output:
"The cafeteria started with 23 apples.
They used 20 apples for lunch, leaving 23 - 20 = 3 apples.
Then they bought 6 more, so 3 + 6 = 9 apples."

# 引导模型展示推理过程，提高准确率
```

#### 4. Self-Consistency

```
# 多次采样，取多数投票结果
responses = []
for _ in range(5):
    response = model.generate(prompt, temperature=0.7)
    responses.append(extract_answer(response))

final_answer = majority_vote(responses)

# 提高推理任务的稳定性
```

### Prompt优化技巧

```python
# 好的Prompt
good_prompt = """
You are an expert Python developer. Your task is to review code and suggest improvements.

Rules:
1. Focus on code quality, not style preferences
2. Explain why each change is beneficial
3. Provide corrected code snippets

Code to review:
```python
def calc(x,y):
    return x+y
```

Please review this code and suggest improvements.
"""

# 不好的Prompt
bad_prompt = "Review this code: def calc(x,y): return x+y"

# 好的Prompt特点：
# - 明确角色
# - 清晰的规则和约束
# - 结构化的输入
# - 明确的期望输出
```

---

## 3.6 In-Context Learning

### 定义

```
In-Context Learning (ICL): 模型通过在prompt中提供的示例来"学习"任务，
无需更新模型参数。

┌─────────────────────────────────────────┐
│           传统微调 vs ICL               │
├─────────────────────────────────────────┤
│                                         │
│  传统微调:                              │
│  训练数据 ──► 更新模型参数 ──► 推理     │
│                                         │
│  ICL:                                   │
│  示例 + 问题 ──► 直接推理（参数不变）   │
│                                         │
└─────────────────────────────────────────┘
```

### ICL的工作原理

```
模型如何"学习"？

1. 模式识别：从示例中识别输入输出的映射模式
2. 格式对齐：学习期望的输出格式
3. 任务理解：通过示例理解任务本质

研究发现：
- 示例的格式比内容更重要
- 即使示例答案是错的，格式对齐也能帮助
- 示例数量通常3-5个效果最好
```

### 示例选择策略

```python
def select_examples(query, example_pool, k=5, method='similarity'):
    """选择与query最相关的示例"""

    if method == 'random':
        return random.sample(example_pool, k)

    elif method == 'similarity':
        # 基于嵌入相似度选择
        query_emb = get_embedding(query)
        scores = []
        for ex in example_pool:
            ex_emb = get_embedding(ex['input'])
            score = cosine_similarity(query_emb, ex_emb)
            scores.append(score)
        top_k_indices = np.argsort(scores)[-k:]
        return [example_pool[i] for i in top_k_indices]

    elif method == 'diverse':
        # 选择多样化的示例
        selected = []
        for _ in range(k):
            best_ex = max(example_pool, key=lambda x: diversity_score(x, selected))
            selected.append(best_ex)
        return selected
```

---

## 3.7 检索增强生成 (RAG)

### RAG架构

```
                    ┌─────────────────────────────────────┐
                    │              RAG 流程               │
                    └─────────────────────────────────────┘

用户问题: "What is the capital of France?"
     │
     ▼
┌─────────────┐      相似度搜索      ┌─────────────────┐
│   Query     │─────────────────────►│  Vector Store   │
│  Encoder    │                      │  (知识库)       │
└─────────────┘                      │  ┌───────────┐  │
                                     │  │ Doc 1     │  │
                                     │  │ Doc 2     │  │
                                     │  │ Doc 3 ✓   │  │
                                     │  │ ...       │  │
                                     │  └───────────┘  │
                                     └────────┬────────┘
                                              │
                                     检索到的文档
                                              │
                                              ▼
                              ┌─────────────────────────────┐
                              │  Prompt = 问题 + 检索文档   │
                              │                             │
                              │  "Based on the following    │
                              │   context: [Doc 3 content]  │
                              │   Answer: What is the       │
                              │   capital of France?"       │
                              └──────────────┬──────────────┘
                                             │
                                             ▼
                              ┌─────────────────────────────┐
                              │           LLM               │
                              └──────────────┬──────────────┘
                                             │
                                             ▼
                              答案: "The capital of France is Paris."
```

### RAG实现

```python
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

class RAGSystem:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.vector_store = None
        self.llm = ChatOpenAI()

    def build_index(self, documents):
        """构建向量索引"""
        # 文档切分
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = splitter.split_documents(documents)

        # 构建向量存储
        self.vector_store = FAISS.from_documents(chunks, self.embeddings)

    def retrieve(self, query, k=3):
        """检索相关文档"""
        docs = self.vector_store.similarity_search(query, k=k)
        return docs

    def generate(self, query):
        """RAG生成"""
        # 1. 检索
        docs = self.retrieve(query)
        context = "\n".join([doc.page_content for doc in docs])

        # 2. 构建prompt
        prompt = f"""Based on the following context, answer the question.

Context:
{context}

Question: {query}

Answer:"""

        # 3. 生成
        response = self.llm.generate(prompt)
        return response
```

### RAG优化技术

```
1. Chunking策略
   ├─ 固定大小切分
   ├─ 语义切分（按段落/章节）
   └─ 滑动窗口（重叠切分）

2. 检索优化
   ├─ 混合检索（向量+关键词）
   ├─ 重排序（Reranker）
   └─ 查询扩展

3. 生成优化
   ├─ 多轮检索
   ├─ 自我反思
   └─ 引用标注
```

---

## 总结

| 技术 | 用途 | 要点 |
|------|------|------|
| Transformer | 基础架构 | Self-Attention、并行计算 |
| GPT | 文本生成 | 自回归、Causal Mask |
| 解码策略 | 控制输出 | Temperature、Top-P、Top-K |
| Prompt工程 | 任务引导 | 结构化、Few-shot、CoT |
| ICL | 零样本学习 | 示例驱动、无需微调 |
| RAG | 知识增强 | 检索+生成、减少幻觉 |

**学习重点：**
1. 理解Transformer和Self-Attention
2. 掌握不同解码策略的适用场景
3. 学会设计有效的Prompt
4. 了解RAG的原理和实现
