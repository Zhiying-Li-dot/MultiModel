# 第四章：音频生成

## 4.1 音频基础知识

### 音频表示

```
音频信号的三种主要表示形式：

1. 波形 (Waveform)
   ┌─────────────────────────────────────┐
   │    ∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿∿     │
   │  原始音频采样点，时域表示            │
   │  采样率: 16kHz/22kHz/44.1kHz       │
   └─────────────────────────────────────┘

2. 频谱图 (Spectrogram)
   ┌─────────────────────────────────────┐
   │  频率 ▲                             │
   │       │ ████  ████                  │
   │       │ █████████ ███               │
   │       │ █████████████████           │
   │       └──────────────────► 时间     │
   │  短时傅里叶变换(STFT)结果           │
   └─────────────────────────────────────┘

3. 梅尔频谱图 (Mel Spectrogram)
   ┌─────────────────────────────────────┐
   │  梅尔频率 ▲                         │
   │          │ 更符合人耳感知           │
   │          │ 低频分辨率高，高频分辨率低│
   │          └─────────────────► 时间   │
   │  TTS模型最常用的输入/输出格式       │
   └─────────────────────────────────────┘
```

### 音频处理代码

```python
import librosa
import numpy as np

class AudioProcessor:
    def __init__(self, sample_rate=22050, n_fft=1024, hop_length=256, n_mels=80):
        self.sample_rate = sample_rate
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels

    def load_audio(self, path):
        """加载音频文件"""
        wav, sr = librosa.load(path, sr=self.sample_rate)
        return wav

    def wav_to_mel(self, wav):
        """波形转梅尔频谱"""
        mel = librosa.feature.melspectrogram(
            y=wav,
            sr=self.sample_rate,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            n_mels=self.n_mels
        )
        mel_db = librosa.power_to_db(mel, ref=np.max)
        return mel_db

    def mel_to_wav(self, mel_db):
        """梅尔频谱转波形 (近似重建)"""
        mel = librosa.db_to_power(mel_db)
        wav = librosa.feature.inverse.mel_to_audio(
            mel,
            sr=self.sample_rate,
            n_fft=self.n_fft,
            hop_length=self.hop_length
        )
        return wav
```

---

## 4.2 语音合成 (Text-to-Speech)

### TTS发展历程

```
传统方法 (拼接合成、参数合成)
    │
    │  质量差，机械感
    ▼
Tacotron (2017)
    │
    │  端到端，注意力机制
    ▼
Tacotron 2 (2018)
    │
    │  + WaveNet声码器，接近人声
    ▼
FastSpeech (2019)
    │
    │  并行生成，速度快
    ▼
VITS (2021)
    │
    │  端到端，VAE+Flow+GAN
    ▼
Vall-E / XTTS (2023)
    │
    │  零样本语音克隆
    ▼
当前: ChatTTS, Fish-Speech, CosyVoice
```

### TTS基本架构

```
            ┌─────────────────────────────────────────────┐
            │           典型TTS架构（两阶段）              │
            └─────────────────────────────────────────────┘

文本: "Hello, world!"
      │
      ▼
┌──────────────────┐
│  Text Encoder    │  文本转音素/字符嵌入
│  + G2P (可选)    │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  Acoustic Model  │  Tacotron/FastSpeech
│  (声学模型)      │  生成梅尔频谱
└────────┬─────────┘
         │
         ▼
    Mel Spectrogram (80 x T)
         │
         ▼
┌──────────────────┐
│    Vocoder       │  WaveNet/HiFi-GAN
│   (声码器)       │  梅尔频谱转波形
└────────┬─────────┘
         │
         ▼
    音频波形
```

### Tacotron 2

```
              ┌─────────────────────────────────────────┐
              │          Tacotron 2 架构                │
              └─────────────────────────────────────────┘

文本字符 ──► Character Embedding
                    │
                    ▼
            ┌───────────────┐
            │   Encoder     │  3层卷积 + BiLSTM
            │  (文本编码)   │
            └───────┬───────┘
                    │
                    ▼ Encoder输出
                    │
     ┌──────────────┼──────────────┐
     │              │              │
     │         Attention           │
     │       (Location-Sensitive)  │
     │              │              │
     └──────────────┼──────────────┘
                    │
                    ▼ Context向量
            ┌───────────────┐
            │   Decoder     │  2层LSTM
            │  (自回归)     │  每步生成1帧Mel
            └───────┬───────┘
                    │
                    ▼
              Mel Spectrogram
                    │
                    ▼
            ┌───────────────┐
            │   PostNet     │  5层卷积，细化Mel
            └───────┬───────┘
                    │
                    ▼
             最终Mel输出
```

### FastSpeech 2

```
FastSpeech: 非自回归，并行生成

文本 ──► Encoder ──► 时长预测器 ──► 扩展 ──► Decoder ──► Mel
                      │
                   预测每个音素的持续时长

优点:
1. 并行生成，速度快（比Tacotron快10-20倍）
2. 稳定，不会跳词或重复
3. 可控时长/语速

实现要点:
- 需要时长标注进行训练（MFA对齐）
- 使用变分方法或对齐算法
```

```python
class FastSpeech2(nn.Module):
    def __init__(self, vocab_size, d_model=256, n_layers=4):
        super().__init__()

        self.encoder = Encoder(vocab_size, d_model, n_layers)

        # 变分适配器
        self.duration_predictor = VariancePredictor(d_model)
        self.pitch_predictor = VariancePredictor(d_model)
        self.energy_predictor = VariancePredictor(d_model)

        self.length_regulator = LengthRegulator()
        self.decoder = Decoder(d_model, n_layers)
        self.mel_linear = nn.Linear(d_model, 80)

    def forward(self, text, durations=None, pitch=None, energy=None):
        # 编码
        enc_out = self.encoder(text)

        # 预测变化量
        dur_pred = self.duration_predictor(enc_out)
        pitch_pred = self.pitch_predictor(enc_out)
        energy_pred = self.energy_predictor(enc_out)

        if durations is None:
            durations = dur_pred.round().long()

        # 长度调节（扩展到Mel帧数）
        expanded = self.length_regulator(enc_out, durations)

        # 解码
        dec_out = self.decoder(expanded)
        mel = self.mel_linear(dec_out)

        return mel, dur_pred, pitch_pred, energy_pred
```

### 声码器 (Vocoder)

```
声码器: 将Mel频谱转换为音频波形

主流声码器:

1. WaveNet (2016)
   - 自回归，每次生成一个采样点
   - 质量最高，但极慢

2. WaveGlow (2018)
   - Flow-based，并行生成
   - 质量好，速度中等

3. HiFi-GAN (2020) ⭐ 最常用
   - GAN-based，并行生成
   - 质量接近WaveNet，速度快1000倍

4. Vocos (2023)
   - 最新，更快更好
```

```python
class HiFiGANGenerator(nn.Module):
    """HiFi-GAN生成器"""
    def __init__(self, in_channels=80, upsample_rates=[8, 8, 2, 2]):
        super().__init__()

        self.conv_pre = nn.Conv1d(in_channels, 512, 7, padding=3)

        self.ups = nn.ModuleList()
        ch = 512
        for rate in upsample_rates:
            self.ups.append(
                nn.ConvTranspose1d(ch, ch//2, rate*2, rate, rate//2)
            )
            ch = ch // 2

        self.conv_post = nn.Conv1d(ch, 1, 7, padding=3)

    def forward(self, mel):
        x = self.conv_pre(mel)

        for up in self.ups:
            x = F.leaky_relu(x, 0.1)
            x = up(x)
            # + ResBlock...

        x = F.leaky_relu(x)
        x = self.conv_post(x)
        x = torch.tanh(x)

        return x
```

---

## 4.3 语音克隆

### 零样本语音克隆

```
          ┌─────────────────────────────────────────────────┐
          │          零样本语音克隆流程                      │
          └─────────────────────────────────────────────────┘

参考音频 (3-10秒)
      │
      ▼
┌──────────────┐
│Speaker Encoder│  提取说话人特征（音色、风格）
└──────┬───────┘
       │
       ▼ Speaker Embedding
       │
       ├─────────────────────────────────────────┐
       │                                         │
文本 ──┴──► TTS Model ◄───────────────────────────┘
                │
                ▼
        克隆目标说话人的音色说出新文本
```

### VALL-E架构

```
VALL-E: 将TTS视为语言模型任务

训练数据: 60000小时音频

文本 + 参考音频 ──► 离散化音频Token ──► 自回归预测

特点:
1. 使用Neural Codec (如EnCodec) 将音频离散化
2. 将TTS转化为token序列预测任务
3. 只需3秒参考音频即可克隆

音频离散化:
音频波形 ──► EnCodec Encoder ──► 8个Codebook的离散码
                                  (类似VQ-VAE)
```

### 实用语音克隆工具

```python
# 使用Coqui XTTS
from TTS.api import TTS

# 加载模型
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2")

# 克隆语音
tts.tts_to_file(
    text="Hello, this is a cloned voice!",
    speaker_wav="reference_audio.wav",  # 参考音频
    language="en",
    file_path="output.wav"
)
```

---

## 4.4 音乐生成

### 音乐生成模型

```
模型演进:

MuseNet (OpenAI, 2019)
    │  Transformer，MIDI生成
    ▼
Jukebox (OpenAI, 2020)
    │  直接生成原始音频
    │  VQ-VAE + Transformer
    ▼
MusicLM (Google, 2023)
    │  文本到音乐
    │  音频Token + 语言模型
    ▼
MusicGen (Meta, 2023) ⭐
    │  开源，效果好
    │  单个Transformer解码器
    ▼
Suno / Udio (2024)
    高质量AI音乐生成服务
```

### MusicGen架构

```
              ┌─────────────────────────────────────────┐
              │          MusicGen 架构                  │
              └─────────────────────────────────────────┘

文本描述: "An upbeat electronic dance track with heavy bass"
      │
      ▼
┌────────────────┐
│ Text Encoder   │  T5 Encoder
│    (冻结)      │
└───────┬────────┘
        │ 文本嵌入
        │
        ▼
┌────────────────────────────────────────────────┐
│           Transformer Decoder                   │
│  (自回归生成音频Token)                          │
│                                                 │
│  使用延迟模式 (Delay Pattern) 处理多码本        │
└───────────────────────┬─────────────────────────┘
                        │
                        ▼ 音频Token序列
┌────────────────────────────────────────────────┐
│           EnCodec Decoder                       │
│  (音频Token → 波形)                             │
└───────────────────────┬─────────────────────────┘
                        │
                        ▼
                   生成的音乐
```

```python
# 使用MusicGen
from audiocraft.models import MusicGen

model = MusicGen.get_pretrained('facebook/musicgen-medium')
model.set_generation_params(duration=30)  # 30秒

# 文本生成音乐
wav = model.generate(['happy rock song with electric guitar'])

# 旋律条件生成
melody_wav, sr = torchaudio.load('melody.wav')
wav = model.generate_with_chroma(
    descriptions=['energetic EDM'],
    melody_wavs=melody_wav,
    melody_sample_rate=sr
)
```

### 音乐表示方法

```
1. MIDI表示
   ┌─────────────────────────────────────┐
   │ Note: C4, Velocity: 80, Duration: 0.5s
   │ Note: E4, Velocity: 70, Duration: 0.5s
   │ ...
   └─────────────────────────────────────┘
   优点: 结构化，可编辑
   缺点: 无法表示音色、表情

2. 频谱/波形
   ┌─────────────────────────────────────┐
   │ 直接对音频信号建模
   │ 保留完整的音频信息
   └─────────────────────────────────────┘
   优点: 保真度高
   缺点: 序列长，计算量大

3. 离散音频Token (主流)
   ┌─────────────────────────────────────┐
   │ 使用Neural Codec压缩
   │ 音频 ──► [t1, t2, t3, ...]
   │ 类似文本token，可用Transformer建模
   └─────────────────────────────────────┘
```

---

## 4.5 音频理解

### 语音识别 (ASR)

```
自动语音识别 (Automatic Speech Recognition)

音频 ──► ASR模型 ──► 文本

主流模型:
- Whisper (OpenAI): 多语言，开源，最常用
- Wav2Vec 2.0: 自监督预训练
- Conformer: 卷积+Transformer
```

```python
# 使用Whisper
import whisper

model = whisper.load_model("base")  # tiny/base/small/medium/large
result = model.transcribe("audio.mp3")

print(result["text"])  # 识别文本
print(result["segments"])  # 带时间戳的分段
```

### 音频分类与标注

```
音频理解任务:

1. 音频分类
   音频 ──► 类别 (音乐/语音/环境音)

2. 音频事件检测
   音频 ──► 检测事件及时间 (狗叫@2.3s, 车鸣@5.1s)

3. 说话人识别
   音频 ──► 说话人身份

4. 情感识别
   语音 ──► 情感 (开心/悲伤/愤怒)
```

---

## 总结

| 任务 | 代表模型 | 特点 |
|------|----------|------|
| TTS | FastSpeech 2 + HiFi-GAN | 快速、稳定 |
| 语音克隆 | XTTS, VALL-E | 零样本，3秒参考 |
| 音乐生成 | MusicGen | 开源，文生音乐 |
| 语音识别 | Whisper | 多语言，鲁棒 |

**学习重点：**
1. 理解音频的不同表示形式（波形、Mel频谱）
2. TTS两阶段架构：声学模型 + 声码器
3. 了解语音克隆的Speaker Embedding方法
4. 音乐生成中的离散音频Token方法
