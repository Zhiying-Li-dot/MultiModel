# 第二章：图像生成

图像生成是AIGC最核心的领域，本章详细介绍GAN、VAE、Diffusion三大主流方法。

## 2.1 GAN (生成对抗网络)

### 基本原理

```
                    ┌─────────────────────────────────────┐
                    │            GAN 框架                 │
                    └─────────────────────────────────────┘

随机噪声 z ──────►  Generator (G)  ──────► 生成图像 G(z)
                        │                      │
                        │                      ▼
                        │              ┌──────────────┐
                        │              │ Discriminator│
                        │              │     (D)      │
                        │              └──────┬───────┘
                        │                     │
                        │            真/假判断 D(x)
                        │                     │
真实图像 x ─────────────────────────────────►─┘

目标：G 生成以假乱真的图像，D 区分真假
博弈：G 想骗过 D，D 想识破 G
```

### 损失函数

```python
# GAN的MinMax博弈目标
# min_G max_D V(D,G) = E[log D(x)] + E[log(1 - D(G(z)))]

def gan_loss(real_images, fake_images, discriminator):
    """
    判别器损失: 最大化区分真假的能力
    生成器损失: 最大化骗过判别器的能力
    """
    # 判别器损失
    real_loss = F.binary_cross_entropy(discriminator(real_images), ones)
    fake_loss = F.binary_cross_entropy(discriminator(fake_images), zeros)
    d_loss = real_loss + fake_loss

    # 生成器损失
    g_loss = F.binary_cross_entropy(discriminator(fake_images), ones)

    return d_loss, g_loss
```

### GAN实现

```python
import torch
import torch.nn as nn

class Generator(nn.Module):
    """DCGAN生成器"""
    def __init__(self, latent_dim=100, channels=3):
        super().__init__()
        self.main = nn.Sequential(
            # 输入: latent_dim x 1 x 1
            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(True),
            # 512 x 4 x 4
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            # 256 x 8 x 8
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            # 128 x 16 x 16
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            # 64 x 32 x 32
            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),
            nn.Tanh()
            # channels x 64 x 64
        )

    def forward(self, z):
        return self.main(z.view(-1, z.size(1), 1, 1))


class Discriminator(nn.Module):
    """DCGAN判别器"""
    def __init__(self, channels=3):
        super().__init__()
        self.main = nn.Sequential(
            # channels x 64 x 64
            nn.Conv2d(channels, 64, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # 64 x 32 x 32
            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            # 128 x 16 x 16
            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            # 256 x 8 x 8
            nn.Conv2d(256, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            # 512 x 4 x 4
            nn.Conv2d(512, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.main(x).view(-1, 1)
```

### GAN训练循环

```python
def train_gan(generator, discriminator, dataloader, epochs=100):
    g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    criterion = nn.BCELoss()

    for epoch in range(epochs):
        for real_images, _ in dataloader:
            batch_size = real_images.size(0)
            real_labels = torch.ones(batch_size, 1)
            fake_labels = torch.zeros(batch_size, 1)

            # ========== 训练判别器 ==========
            d_optimizer.zero_grad()

            # 真实图像
            outputs = discriminator(real_images)
            d_loss_real = criterion(outputs, real_labels)

            # 生成图像
            z = torch.randn(batch_size, 100)
            fake_images = generator(z)
            outputs = discriminator(fake_images.detach())
            d_loss_fake = criterion(outputs, fake_labels)

            d_loss = d_loss_real + d_loss_fake
            d_loss.backward()
            d_optimizer.step()

            # ========== 训练生成器 ==========
            g_optimizer.zero_grad()

            outputs = discriminator(fake_images)
            g_loss = criterion(outputs, real_labels)  # 希望被判为真

            g_loss.backward()
            g_optimizer.step()
```

### GAN变体演进

```
GAN (2014)
 │
 ├── DCGAN (2015) ─────── 引入CNN，稳定训练
 │
 ├── WGAN (2017) ──────── Wasserstein距离，解决模式崩塌
 │
 ├── Progressive GAN (2017) ── 渐进式训练，高分辨率
 │
 ├── StyleGAN (2018) ───── 风格控制，高质量人脸
 │    │
 │    ├── StyleGAN2 (2019) ── 改进artifacts
 │    │
 │    └── StyleGAN3 (2021) ── 解决alias问题
 │
 └── BigGAN (2018) ─────── 大规模训练，ImageNet生成
```

### StyleGAN架构

```
         Mapping Network              Synthesis Network
         ┌───────────────┐
z ──────►│ 8层全连接网络  │────► w (样式向量)
         └───────────────┘           │
                                     ▼
              ┌──────────────────────────────────────┐
              │ 4x4 ──► 8x8 ──► 16x16 ──► ... ──► 1024x1024 │
              │  ↑       ↑       ↑                    │
              │  w       w       w    (AdaIN注入样式) │
              │  ↑       ↑       ↑                    │
              │ noise   noise   noise (加入随机细节)  │
              └──────────────────────────────────────┘

特点：
1. Mapping Network将z映射到w空间，解耦属性
2. AdaIN在每层注入样式信息
3. 噪声注入增加随机细节（如发丝、皮肤纹理）
```

---

## 2.2 VAE (变分自编码器)

### 基本原理

```
        ┌─────────────────────────────────────────────┐
        │                VAE 架构                     │
        └─────────────────────────────────────────────┘

输入图像 x ──► Encoder ──► μ, σ ──► z = μ + σ·ε ──► Decoder ──► 重建图像 x'
                           │           │
                        均值 方差    重参数化技巧
                           │        (ε ~ N(0,1))
                           │
              KL散度损失: 使z接近标准正态分布 N(0,1)
```

### VAE损失函数

```python
def vae_loss(x, x_recon, mu, log_var):
    """
    VAE损失 = 重建损失 + KL散度

    重建损失: 让输出接近输入
    KL散度: 让隐变量分布接近标准正态分布
    """
    # 重建损失
    recon_loss = F.mse_loss(x_recon, x, reduction='sum')

    # KL散度: KL(q(z|x) || p(z))
    # = -0.5 * sum(1 + log(σ²) - μ² - σ²)
    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())

    return recon_loss + kl_loss
```

### VAE实现

```python
class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        super().__init__()

        # Encoder
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_var = nn.Linear(hidden_dim, latent_dim)

        # Decoder
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        h = F.relu(self.fc1(x))
        mu = self.fc_mu(h)
        log_var = self.fc_var(h)
        return mu, log_var

    def reparameterize(self, mu, log_var):
        """重参数化技巧，使采样可导"""
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h))

    def forward(self, x):
        mu, log_var = self.encode(x)
        z = self.reparameterize(mu, log_var)
        x_recon = self.decode(z)
        return x_recon, mu, log_var
```

### VQ-VAE (向量量化VAE)

```
VQ-VAE: 用离散码本代替连续隐变量

输入 ──► Encoder ──► z_e ──► 查找最近码本向量 ──► z_q ──► Decoder ──► 输出
                             │
                      Codebook (K个向量)
                      [e_1, e_2, ..., e_K]

优点：
1. 离散表示更紧凑
2. 避免后验坍塌问题
3. 是DALL-E 1、Stable Diffusion的基础
```

```python
class VectorQuantizer(nn.Module):
    def __init__(self, num_embeddings=512, embedding_dim=64):
        super().__init__()
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)

    def forward(self, z_e):
        # z_e: (B, C, H, W)
        B, C, H, W = z_e.shape
        z_e_flat = z_e.permute(0, 2, 3, 1).reshape(-1, C)  # (B*H*W, C)

        # 计算距离，找最近的码本向量
        distances = torch.cdist(z_e_flat, self.embedding.weight)
        indices = distances.argmin(dim=1)  # (B*H*W,)

        # 量化
        z_q_flat = self.embedding(indices)  # (B*H*W, C)
        z_q = z_q_flat.reshape(B, H, W, C).permute(0, 3, 1, 2)

        # 直通估计器 (Straight-Through Estimator)
        z_q = z_e + (z_q - z_e).detach()

        return z_q, indices
```

---

## 2.3 Diffusion Models (扩散模型) ⭐

扩散模型是当前最主流的图像生成方法，Stable Diffusion、DALL-E 2/3、Midjourney都基于此。

### 基本原理

```
前向过程 (加噪)                      反向过程 (去噪)
───────────────────►              ◄───────────────────
x_0 → x_1 → x_2 → ... → x_T      x_T → ... → x_2 → x_1 → x_0
原图    逐步加高斯噪声     纯噪声   纯噪声   神经网络预测并去噪    生成图

前向: q(x_t | x_{t-1}) = N(x_t; √(1-β_t)·x_{t-1}, β_t·I)
反向: p_θ(x_{t-1} | x_t) = N(x_{t-1}; μ_θ(x_t, t), Σ_θ(x_t, t))
```

### 关键公式

```
1. 前向过程（一步到位公式）：
   x_t = √ᾱ_t · x_0 + √(1-ᾱ_t) · ε
   其中 ᾱ_t = ∏(1-β_i)，ε ~ N(0, I)

2. 训练目标（预测噪声）：
   L = E[||ε - ε_θ(x_t, t)||²]
   即训练网络预测加入的噪声

3. 采样过程：
   x_{t-1} = (1/√α_t)(x_t - (β_t/√(1-ᾱ_t))·ε_θ(x_t,t)) + σ_t·z
```

### DDPM实现

```python
import torch
import torch.nn as nn
import numpy as np

class DiffusionModel:
    def __init__(self, model, timesteps=1000, beta_start=0.0001, beta_end=0.02):
        self.model = model  # UNet
        self.timesteps = timesteps

        # 定义噪声调度
        self.betas = torch.linspace(beta_start, beta_end, timesteps)
        self.alphas = 1 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)

    def q_sample(self, x_0, t, noise=None):
        """前向加噪: x_t = √ᾱ_t · x_0 + √(1-ᾱ_t) · ε"""
        if noise is None:
            noise = torch.randn_like(x_0)

        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)
        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)

        return sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise

    def p_losses(self, x_0, t):
        """计算训练损失"""
        noise = torch.randn_like(x_0)
        x_t = self.q_sample(x_0, t, noise)

        # 预测噪声
        predicted_noise = self.model(x_t, t)

        # MSE损失
        loss = F.mse_loss(predicted_noise, noise)
        return loss

    @torch.no_grad()
    def p_sample(self, x_t, t):
        """单步去噪采样"""
        # 预测噪声
        predicted_noise = self.model(x_t, t)

        # 计算均值
        alpha = self.alphas[t]
        alpha_cumprod = self.alphas_cumprod[t]
        beta = self.betas[t]

        mean = (1 / torch.sqrt(alpha)) * (
            x_t - (beta / torch.sqrt(1 - alpha_cumprod)) * predicted_noise
        )

        # 添加噪声（除了最后一步）
        if t > 0:
            noise = torch.randn_like(x_t)
            sigma = torch.sqrt(beta)
            return mean + sigma * noise
        return mean

    @torch.no_grad()
    def sample(self, shape):
        """完整采样过程"""
        # 从纯噪声开始
        x = torch.randn(shape)

        # 逐步去噪
        for t in reversed(range(self.timesteps)):
            x = self.p_sample(x, torch.tensor([t]))

        return x
```

### UNet架构

```
Diffusion模型使用UNet预测噪声：

输入: 带噪图像 x_t, 时间步 t
                                      Skip Connection
     ┌────────────────────────────────────────────────────┐
     │                                                    │
     ▼                                                    │
┌─────────┐    ┌─────────┐    ┌─────────┐         ┌─────────┐
│ Down 1  │───►│ Down 2  │───►│ Down 3  │───►...  │ Middle  │
│ 64→128  │    │ 128→256 │    │ 256→512 │         │  Block  │
└─────────┘    └─────────┘    └─────────┘         └────┬────┘
     │              │              │                    │
     │              │              │                    ▼
     │              │              │              ┌─────────┐
     │              │              └─────────────►│  Up 3   │
     │              │                             │ 512→256 │
     │              │                             └────┬────┘
     │              │                                  │
     │              └────────────────────────────────►─┤
     │                                            ┌─────────┐
     │                                            │  Up 2   │
     │                                            │ 256→128 │
     │                                            └────┬────┘
     │                                                 │
     └────────────────────────────────────────────────►┤
                                                  ┌─────────┐
                                                  │  Up 1   │
                                                  │ 128→64  │
                                                  └────┬────┘
                                                       │
                                                       ▼
                                                  预测噪声 ε

时间嵌入 t 通过正弦位置编码注入到每个ResBlock
```

```python
class UNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, time_dim=256):
        super().__init__()
        self.time_mlp = nn.Sequential(
            SinusoidalPositionEmbeddings(time_dim),
            nn.Linear(time_dim, time_dim),
            nn.GELU(),
            nn.Linear(time_dim, time_dim),
        )

        # Encoder
        self.down1 = DownBlock(in_channels, 64, time_dim)
        self.down2 = DownBlock(64, 128, time_dim)
        self.down3 = DownBlock(128, 256, time_dim)

        # Middle
        self.middle = MiddleBlock(256, time_dim)

        # Decoder
        self.up3 = UpBlock(256 + 256, 128, time_dim)
        self.up2 = UpBlock(128 + 128, 64, time_dim)
        self.up1 = UpBlock(64 + 64, 64, time_dim)

        self.final = nn.Conv2d(64, out_channels, 1)

    def forward(self, x, t):
        t_emb = self.time_mlp(t)

        # Encoder
        x1 = self.down1(x, t_emb)
        x2 = self.down2(x1, t_emb)
        x3 = self.down3(x2, t_emb)

        # Middle
        x = self.middle(x3, t_emb)

        # Decoder with skip connections
        x = self.up3(torch.cat([x, x3], dim=1), t_emb)
        x = self.up2(torch.cat([x, x2], dim=1), t_emb)
        x = self.up1(torch.cat([x, x1], dim=1), t_emb)

        return self.final(x)
```

---

## 2.4 Stable Diffusion

Stable Diffusion是最流行的开源文生图模型。

### 架构概览

```
                    Stable Diffusion 架构
     ┌──────────────────────────────────────────────────────┐
     │                                                      │
     │  文本: "a cat sitting on a sofa"                     │
     │         │                                            │
     │         ▼                                            │
     │  ┌──────────────┐                                    │
     │  │ Text Encoder │  CLIP Text Encoder                 │
     │  │   (冻结)     │                                    │
     │  └──────┬───────┘                                    │
     │         │ 文本嵌入                                    │
     │         ▼                                            │
     │  ┌──────────────────────────────────────────┐        │
     │  │              Latent Space                 │        │
     │  │  ┌─────────────────────────────────────┐ │        │
     │  │  │  噪声 z_T ──► UNet去噪 ──► 潜码 z_0  │ │        │
     │  │  │      ↑                               │ │        │
     │  │  │   时间步 t                           │ │        │
     │  │  │      ↑                               │ │        │
     │  │  │   文本条件 (Cross Attention)         │ │        │
     │  │  └─────────────────────────────────────┘ │        │
     │  └──────────────────┬───────────────────────┘        │
     │                     │                                │
     │                     ▼                                │
     │  ┌──────────────────────────────────────────┐        │
     │  │           VAE Decoder                    │        │
     │  │         (潜码 → 图像)                    │        │
     │  └──────────────────┬───────────────────────┘        │
     │                     │                                │
     │                     ▼                                │
     │              生成的图像 (512x512)                    │
     └──────────────────────────────────────────────────────┘

关键点：
1. 在潜空间(64x64x4)进行扩散，而非像素空间(512x512x3)
2. 使用预训练的VAE进行编解码
3. 使用CLIP文本编码器理解提示词
4. UNet通过Cross Attention融合文本条件
```

### 核心组件

```python
from diffusers import StableDiffusionPipeline

# 三大核心组件
class StableDiffusion:
    def __init__(self):
        # 1. VAE: 图像 ↔ 潜空间
        self.vae = AutoencoderKL()  # 压缩8倍: 512x512 → 64x64

        # 2. Text Encoder: 文本 → 嵌入
        self.text_encoder = CLIPTextModel()  # 77 tokens × 768 dim

        # 3. UNet: 预测噪声
        self.unet = UNet2DConditionModel()

        # 调度器
        self.scheduler = DDPMScheduler()

    def encode_prompt(self, prompt):
        """编码文本提示"""
        tokens = self.tokenizer(prompt, max_length=77, padding="max_length")
        text_embeddings = self.text_encoder(tokens)[0]  # (1, 77, 768)
        return text_embeddings

    def encode_image(self, image):
        """图像编码到潜空间"""
        latents = self.vae.encode(image).latent_dist.sample()
        latents = latents * 0.18215  # 缩放因子
        return latents

    def decode_latents(self, latents):
        """潜空间解码到图像"""
        latents = latents / 0.18215
        image = self.vae.decode(latents).sample
        return image
```

### 采样过程

```python
@torch.no_grad()
def generate(self, prompt, num_steps=50, guidance_scale=7.5):
    # 1. 编码文本
    text_embeddings = self.encode_prompt(prompt)
    uncond_embeddings = self.encode_prompt("")  # 无条件嵌入

    # Classifier-Free Guidance: 同时计算有条件和无条件
    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])

    # 2. 初始化随机噪声
    latents = torch.randn((1, 4, 64, 64))

    # 3. 设置时间步
    self.scheduler.set_timesteps(num_steps)

    # 4. 去噪循环
    for t in self.scheduler.timesteps:
        # 扩展latents用于CFG
        latent_input = torch.cat([latents] * 2)

        # 预测噪声
        noise_pred = self.unet(latent_input, t, text_embeddings).sample

        # Classifier-Free Guidance
        noise_uncond, noise_cond = noise_pred.chunk(2)
        noise_pred = noise_uncond + guidance_scale * (noise_cond - noise_uncond)

        # 更新latents
        latents = self.scheduler.step(noise_pred, t, latents).prev_sample

    # 5. 解码到图像
    image = self.decode_latents(latents)
    return image
```

### Cross Attention机制

```python
class CrossAttention(nn.Module):
    """
    UNet中的Cross Attention层
    将文本条件注入到图像特征中
    """
    def __init__(self, dim, context_dim=768, heads=8):
        super().__init__()
        self.heads = heads
        self.scale = (dim // heads) ** -0.5

        self.to_q = nn.Linear(dim, dim)
        self.to_k = nn.Linear(context_dim, dim)
        self.to_v = nn.Linear(context_dim, dim)
        self.to_out = nn.Linear(dim, dim)

    def forward(self, x, context):
        """
        x: 图像特征 (B, H*W, dim)
        context: 文本嵌入 (B, 77, 768)
        """
        B, N, _ = x.shape

        q = self.to_q(x)      # Query来自图像
        k = self.to_k(context)  # Key来自文本
        v = self.to_v(context)  # Value来自文本

        # 多头注意力
        q = q.view(B, N, self.heads, -1).transpose(1, 2)
        k = k.view(B, -1, self.heads, -1).transpose(1, 2)
        v = v.view(B, -1, self.heads, -1).transpose(1, 2)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)

        out = (attn @ v).transpose(1, 2).reshape(B, N, -1)
        return self.to_out(out)
```

---

## 2.5 条件控制技术

### ControlNet

```
ControlNet: 添加额外条件控制（边缘、深度、姿态等）

原始SD UNet (锁定)              ControlNet (可训练)
    │                               │
    │  ┌─────────────────┐          │  ┌─────────────────┐
    │  │   Encoder 1    │          │  │  Encoder 1 Copy │
    │  └────────┬───────┘          │  └────────┬────────┘
    │           │                   │           │
    │           ▼                   │           ▼
    │  ┌─────────────────┐          │  ┌─────────────────┐
    │  │   Encoder 2    │◄─────────┼──│  Encoder 2 Copy │
    │  └────────┬───────┘   Zero   │  └────────┬────────┘
    │           │          Conv    │           │
    │           ▼                   │           ▼ + 条件图
    │  ┌─────────────────┐          │  ┌─────────────────┐
    │  │   Encoder 3    │◄─────────┼──│  Encoder 3 Copy │
    │  └────────┬───────┘          │  └────────┬────────┘
    │           │                   │           │
    │           ▼                   │           ▼
    │      ...                      │       ...

条件类型:
- Canny边缘
- 深度图
- 人体姿态
- 语义分割
- 涂鸦/草图
```

### IP-Adapter

```
IP-Adapter: 使用参考图像控制生成风格

参考图像 ──► Image Encoder (CLIP) ──► Image Embeddings
                                           │
                                           ▼
文本提示 ──► Text Encoder ──► Text Embeddings ──┬──► Cross Attention
                                               │
                         Image Embeddings ─────┘
                         (通过新增的Cross Attention层)

可以实现:
- 风格迁移
- 人脸保持
- 物体参考
```

---

## 2.6 采样加速

### DDIM

```python
class DDIMSampler:
    """
    DDIM: 确定性采样，可大幅减少步数
    原始DDPM需要1000步，DDIM可以20-50步
    """
    def sample(self, model, shape, num_steps=50):
        # 选择子序列时间步
        timesteps = np.linspace(0, 999, num_steps).astype(int)

        x = torch.randn(shape)

        for i in range(len(timesteps) - 1, -1, -1):
            t = timesteps[i]
            t_prev = timesteps[i-1] if i > 0 else 0

            # 预测噪声
            eps = model(x, t)

            # DDIM更新公式
            alpha_t = self.alphas_cumprod[t]
            alpha_prev = self.alphas_cumprod[t_prev]

            # 预测x_0
            x0_pred = (x - torch.sqrt(1 - alpha_t) * eps) / torch.sqrt(alpha_t)

            # 计算x_{t-1}
            x = torch.sqrt(alpha_prev) * x0_pred + \
                torch.sqrt(1 - alpha_prev) * eps

        return x
```

### 其他加速方法

| 方法 | 步数 | 原理 |
|------|------|------|
| DDPM | 1000 | 原始方法 |
| DDIM | 20-50 | 确定性采样 |
| DPM-Solver | 10-25 | ODE求解器 |
| LCM | 4-8 | 一致性蒸馏 |
| SDXL Turbo | 1-4 | 对抗蒸馏 |

---

## 总结

| 模型 | 原理 | 优点 | 缺点 | 代表作 |
|------|------|------|------|--------|
| GAN | 对抗训练 | 快速、高质量 | 训练不稳定 | StyleGAN |
| VAE | 变分推断 | 稳定、可解释 | 生成模糊 | VQ-VAE |
| Diffusion | 去噪过程 | 最高质量 | 采样慢 | Stable Diffusion |

**学习重点：**
1. 理解Diffusion的前向/反向过程
2. 掌握Stable Diffusion的架构
3. 了解Cross Attention如何融合文本条件
4. 熟悉CFG（Classifier-Free Guidance）
