# 第五章：视频生成

## 5.1 视频生成概述

### 视频生成的挑战

```
视频生成比图像生成更难的原因：

1. 数据量大
   ┌─────────────────────────────────────┐
   │ 图像: 512×512×3 = 786K pixels       │
   │ 视频: 512×512×3×30fps×10s = 235M    │
   │                                     │
   │ 视频数据量是图像的 300倍+           │
   └─────────────────────────────────────┘

2. 时间一致性
   ┌─────────────────────────────────────┐
   │ 帧与帧之间必须保持连贯              │
   │ 物体运动要符合物理规律              │
   │ 不能出现闪烁、跳变                  │
   └─────────────────────────────────────┘

3. 长程依赖
   ┌─────────────────────────────────────┐
   │ 开头的物体要在结尾保持一致          │
   │ 动作要有因果关系                    │
   │ 场景变化要合理                      │
   └─────────────────────────────────────┘
```

### 视频生成发展历程

```
GAN时代 (2016-2021)
├── VGAN (2016): 视频GAN开创
├── MoCoGAN (2018): 运动-内容分离
└── DVD-GAN (2019): 高分辨率视频

Diffusion时代 (2022-现在)
├── Video Diffusion (2022): 扩散模型用于视频
├── Make-A-Video (Meta, 2022)
├── Imagen Video (Google, 2022)
├── Gen-1/Gen-2 (Runway, 2023)
├── Pika (2023)
└── Sora (OpenAI, 2024) ⭐ 革命性突破

关键突破:
- 从GAN到Diffusion
- 从短视频(几秒)到长视频(分钟级)
- 从低分辨率到高分辨率
- 从简单动作到复杂场景理解
```

---

## 5.2 视频生成基础架构

### 视频表示方法

```
1. 帧序列表示
   ┌─────────────────────────────────────┐
   │ [Frame1, Frame2, Frame3, ...]       │
   │  H×W×3   H×W×3   H×W×3              │
   │                                     │
   │ 直接处理每一帧，简单但计算量大      │
   └─────────────────────────────────────┘

2. 3D卷积表示
   ┌─────────────────────────────────────┐
   │ 视频作为3D张量: T×H×W×C             │
   │                                     │
   │ 使用3D卷积同时建模时空              │
   └─────────────────────────────────────┘

3. 潜空间表示 (主流)
   ┌─────────────────────────────────────┐
   │ 视频 ──► 3D VAE ──► 压缩的潜码      │
   │ T×H×W×3 ──►  t×h×w×c (压缩8-16倍)  │
   │                                     │
   │ 在潜空间进行扩散，大幅降低计算量    │
   └─────────────────────────────────────┘

4. Token序列表示
   ┌─────────────────────────────────────┐
   │ 视频 ──► VQ-VAE ──► 离散Token序列   │
   │                                     │
   │ 可以用Transformer自回归生成         │
   └─────────────────────────────────────┘
```

### 时间建模方法

```
方法1: 分离式时空注意力

┌──────────────────────────────────────────────┐
│  Spatial Attention (空间)                    │
│  ┌───┐ ┌───┐ ┌───┐                          │
│  │ F1│ │ F2│ │ F3│  每帧独立做空间注意力    │
│  └───┘ └───┘ └───┘                          │
│    ↓     ↓     ↓                            │
│  Temporal Attention (时间)                   │
│  ┌───────────────┐                          │
│  │ F1 ─ F2 ─ F3 │  跨帧做时间注意力         │
│  └───────────────┘                          │
└──────────────────────────────────────────────┘

方法2: 3D全注意力

┌──────────────────────────────────────────────┐
│  所有token同时做时空注意力                    │
│  计算量大，但建模能力强                       │
│                                              │
│  O(T²×H²×W²) 复杂度                          │
└──────────────────────────────────────────────┘

方法3: 因果时间注意力 (Causal)

┌──────────────────────────────────────────────┐
│  F1 → F2 → F3 → F4                          │
│  每帧只能看到之前的帧                        │
│  适合自回归生成，保证时间因果                │
└──────────────────────────────────────────────┘
```

---

## 5.3 Video Diffusion Model

### 基本架构

```
          ┌─────────────────────────────────────────────────────┐
          │          Video Diffusion Model 架构                 │
          └─────────────────────────────────────────────────────┘

文本: "A cat running in a garden"
      │
      ▼
┌──────────────┐
│ Text Encoder │  CLIP/T5
└──────┬───────┘
       │
       ▼
┌──────────────────────────────────────────────────────────────┐
│                    3D VAE Encoder                            │
│  输入视频 T×H×W×3 ──► 潜码 t×h×w×c                           │
└──────────────────────────────┬───────────────────────────────┘
                               │
                               ▼
┌──────────────────────────────────────────────────────────────┐
│                      3D UNet                                 │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │  Space-Time Attention Blocks                            │ │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐     │ │
│  │  │Spatial Attn │→│Temporal Attn│→│Cross Attn   │      │ │
│  │  │ (帧内)      │  │ (帧间)      │  │(文本条件)   │      │ │
│  │  └─────────────┘  └─────────────┘  └─────────────┘     │ │
│  └─────────────────────────────────────────────────────────┘ │
└──────────────────────────────┬───────────────────────────────┘
                               │
                               ▼
┌──────────────────────────────────────────────────────────────┐
│                    3D VAE Decoder                            │
│  潜码 t×h×w×c ──► 输出视频 T×H×W×3                           │
└──────────────────────────────────────────────────────────────┘
```

### 3D UNet实现

```python
class VideoUNet(nn.Module):
    """视频扩散模型的3D UNet"""
    def __init__(self, in_channels=4, out_channels=4, time_dim=256, context_dim=768):
        super().__init__()

        self.time_embed = nn.Sequential(
            SinusoidalPositionEmbeddings(time_dim),
            nn.Linear(time_dim, time_dim * 4),
            nn.SiLU(),
            nn.Linear(time_dim * 4, time_dim),
        )

        # Encoder blocks
        self.down_blocks = nn.ModuleList([
            SpatioTemporalDownBlock(in_channels, 64, time_dim, context_dim),
            SpatioTemporalDownBlock(64, 128, time_dim, context_dim),
            SpatioTemporalDownBlock(128, 256, time_dim, context_dim),
        ])

        # Middle block
        self.mid_block = SpatioTemporalMidBlock(256, time_dim, context_dim)

        # Decoder blocks
        self.up_blocks = nn.ModuleList([
            SpatioTemporalUpBlock(256 + 256, 128, time_dim, context_dim),
            SpatioTemporalUpBlock(128 + 128, 64, time_dim, context_dim),
            SpatioTemporalUpBlock(64 + 64, 64, time_dim, context_dim),
        ])

        self.out = nn.Conv3d(64, out_channels, 3, padding=1)

    def forward(self, x, t, context):
        """
        x: (B, C, T, H, W) 带噪视频潜码
        t: (B,) 时间步
        context: (B, N, D) 文本嵌入
        """
        t_emb = self.time_embed(t)

        # Encoder
        skips = []
        for block in self.down_blocks:
            x = block(x, t_emb, context)
            skips.append(x)

        # Middle
        x = self.mid_block(x, t_emb, context)

        # Decoder
        for block, skip in zip(self.up_blocks, reversed(skips)):
            x = torch.cat([x, skip], dim=1)
            x = block(x, t_emb, context)

        return self.out(x)


class SpatioTemporalBlock(nn.Module):
    """时空注意力块"""
    def __init__(self, dim, time_dim, context_dim):
        super().__init__()

        # 空间注意力
        self.spatial_attn = SpatialSelfAttention(dim)

        # 时间注意力
        self.temporal_attn = TemporalSelfAttention(dim)

        # 文本交叉注意力
        self.cross_attn = CrossAttention(dim, context_dim)

        # FFN
        self.ffn = FeedForward(dim)

        # Time embedding projection
        self.time_proj = nn.Linear(time_dim, dim)

    def forward(self, x, t_emb, context):
        B, C, T, H, W = x.shape

        # 时间嵌入
        t_emb = self.time_proj(t_emb)[:, :, None, None, None]
        x = x + t_emb

        # 空间注意力 (每帧独立)
        x = rearrange(x, 'b c t h w -> (b t) (h w) c')
        x = x + self.spatial_attn(x)
        x = rearrange(x, '(b t) (h w) c -> b c t h w', b=B, t=T, h=H, w=W)

        # 时间注意力 (每个空间位置)
        x = rearrange(x, 'b c t h w -> (b h w) t c')
        x = x + self.temporal_attn(x)
        x = rearrange(x, '(b h w) t c -> b c t h w', b=B, h=H, w=W)

        # 交叉注意力
        x = rearrange(x, 'b c t h w -> b (t h w) c')
        x = x + self.cross_attn(x, context)
        x = rearrange(x, 'b (t h w) c -> b c t h w', t=T, h=H, w=W)

        # FFN
        x = x + self.ffn(x)

        return x
```

---

## 5.4 Sora技术解析

### Sora核心创新

```
Sora的关键技术点：

1. 可变分辨率和时长
   ┌─────────────────────────────────────────────────────┐
   │ 传统方法: 固定分辨率/时长训练                        │
   │ Sora: 原生分辨率训练，支持任意尺寸                   │
   │                                                     │
   │ 使用"Patch"而非固定网格，类似ViT                    │
   │ 视频被分解为时空patches                             │
   └─────────────────────────────────────────────────────┘

2. Diffusion Transformer (DiT)
   ┌─────────────────────────────────────────────────────┐
   │ 将Transformer用于扩散模型                           │
   │ 代替传统的UNet架构                                  │
   │                                                     │
   │ 更好的扩展性，参数量可达数十亿                      │
   └─────────────────────────────────────────────────────┘

3. 时空压缩
   ┌─────────────────────────────────────────────────────┐
   │ 视频先经过时空VAE压缩                               │
   │ 将高维视频转换为低维潜空间表示                      │
   │                                                     │
   │ 降低计算量，保持质量                                │
   └─────────────────────────────────────────────────────┘

4. 数据规模
   ┌─────────────────────────────────────────────────────┐
   │ 大规模高质量视频数据训练                            │
   │ 学习了丰富的世界知识和物理规律                      │
   └─────────────────────────────────────────────────────┘
```

### Sora架构推测

```
              ┌─────────────────────────────────────────────┐
              │           Sora 架构 (推测)                  │
              └─────────────────────────────────────────────┘

输入: 文本描述 / 图像 / 视频
      │
      ▼
┌────────────────────────────────────────────────────────────┐
│                   条件编码                                 │
│  文本 ──► T5/CLIP ──► 文本嵌入                             │
│  图像 ──► VAE ──► 图像潜码                                 │
└────────────────────────────┬───────────────────────────────┘
                             │
                             ▼
┌────────────────────────────────────────────────────────────┐
│              Spacetime Latent Patches                      │
│                                                            │
│  视频 ──► 3D VAE ──► 潜空间 ──► Patch化 ──► Token序列      │
│                                                            │
│  将视频转换为可变长度的patch token序列                     │
└────────────────────────────┬───────────────────────────────┘
                             │
                             ▼
┌────────────────────────────────────────────────────────────┐
│              Diffusion Transformer (DiT)                   │
│                                                            │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  [Patch1] [Patch2] ... [PatchN]                     │   │
│  │     ↓        ↓           ↓                          │   │
│  │  ┌─────────────────────────────────────────────┐   │   │
│  │  │        Transformer Layers × N               │   │   │
│  │  │  - Self Attention (全部patch)               │   │   │
│  │  │  - Cross Attention (文本条件)               │   │   │
│  │  │  - FFN                                      │   │   │
│  │  └─────────────────────────────────────────────┘   │   │
│  │     ↓        ↓           ↓                          │   │
│  │  预测噪声/速度                                      │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                            │
└────────────────────────────┬───────────────────────────────┘
                             │
                             ▼
┌────────────────────────────────────────────────────────────┐
│                    3D VAE Decoder                          │
│                                                            │
│  潜空间patches ──► 重组 ──► 3D VAE解码 ──► 视频            │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

### DiT (Diffusion Transformer)

```python
class DiTBlock(nn.Module):
    """Diffusion Transformer Block"""
    def __init__(self, dim, n_heads, mlp_ratio=4.0):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim, elementwise_affine=False)
        self.attn = Attention(dim, n_heads)
        self.norm2 = nn.LayerNorm(dim, elementwise_affine=False)
        self.mlp = MLP(dim, int(dim * mlp_ratio))

        # AdaLN-Zero: 自适应层归一化
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(),
            nn.Linear(dim, 6 * dim)
        )

    def forward(self, x, c):
        """
        x: (B, N, D) patch tokens
        c: (B, D) condition embedding (时间步+文本)
        """
        # AdaLN参数
        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = \
            self.adaLN_modulation(c).chunk(6, dim=-1)

        # Attention with AdaLN
        x = x + gate_msa.unsqueeze(1) * self.attn(
            modulate(self.norm1(x), shift_msa, scale_msa)
        )

        # MLP with AdaLN
        x = x + gate_mlp.unsqueeze(1) * self.mlp(
            modulate(self.norm2(x), shift_mlp, scale_mlp)
        )

        return x


def modulate(x, shift, scale):
    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)
```

---

## 5.5 实用视频生成

### 开源视频生成模型

```
1. AnimateDiff
   - 基于Stable Diffusion
   - 插入运动模块，将图像模型转为视频模型
   - 可复用SD的各种LoRA、ControlNet

2. Stable Video Diffusion (SVD)
   - Stability AI官方视频模型
   - 图像转视频 (Image-to-Video)
   - 14-25帧，576×1024分辨率

3. Open-Sora
   - 开源复现Sora架构
   - DiT + 3D VAE
   - 持续更新中

4. CogVideoX
   - 智谱AI开源
   - 中文友好
```

### 使用Stable Video Diffusion

```python
from diffusers import StableVideoDiffusionPipeline
from PIL import Image

# 加载模型
pipe = StableVideoDiffusionPipeline.from_pretrained(
    "stabilityai/stable-video-diffusion-img2vid-xt",
    torch_dtype=torch.float16
)
pipe = pipe.to("cuda")

# 加载输入图像
image = Image.open("input_image.png").resize((1024, 576))

# 生成视频
frames = pipe(
    image,
    num_frames=25,          # 帧数
    decode_chunk_size=8,    # 解码批次大小
    motion_bucket_id=127,   # 运动程度 (0-255)
    noise_aug_strength=0.02 # 噪声增强
).frames[0]

# 保存视频
from diffusers.utils import export_to_video
export_to_video(frames, "output.mp4", fps=7)
```

### 使用AnimateDiff

```python
from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler

# 加载运动模块
adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2")

# 加载基础SD模型 + 运动适配器
pipe = AnimateDiffPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    motion_adapter=adapter
)
pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to("cuda")

# 生成视频
output = pipe(
    prompt="A astronaut riding a horse on mars",
    negative_prompt="bad quality, worst quality",
    num_frames=16,
    guidance_scale=7.5,
    num_inference_steps=25,
)

frames = output.frames[0]
export_to_video(frames, "astronaut.mp4")
```

---

## 5.6 视频编辑

### 视频编辑任务

```
1. 文本引导编辑
   输入视频 + "把狗换成猫" ──► 编辑后视频

2. 风格迁移
   输入视频 + 风格参考 ──► 风格化视频

3. 视频补全
   部分遮挡的视频 ──► 补全后的视频

4. 视频超分
   低分辨率视频 ──► 高分辨率视频
```

### TokenFlow

```
TokenFlow: 保持时间一致性的视频编辑

原理：
1. 用DDIM Inversion获取原视频的噪声
2. 在去噪过程中，传播相邻帧的特征
3. 保持帧间一致性

流程：
原视频 ──► DDIM Inversion ──► 噪声序列
                                 │
编辑prompt                       │
    │                            │
    ▼                            ▼
修改后的生成 ◄───────────── 特征传播 (TokenFlow)
```

---

## 总结

| 技术 | 代表模型 | 特点 |
|------|----------|------|
| Video Diffusion | SVD | 图生视频，开源 |
| DiT架构 | Sora | 可扩展，高质量 |
| 运动模块 | AnimateDiff | 复用SD生态 |
| 视频编辑 | TokenFlow | 保持一致性 |

**学习重点：**
1. 理解视频生成的三大挑战（数据量、时间一致性、长程依赖）
2. 掌握时空注意力的设计方法
3. 了解Sora的核心技术（Patch、DiT、可变分辨率）
4. 实践开源视频生成工具
