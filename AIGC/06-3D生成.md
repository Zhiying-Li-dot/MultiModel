# 第六章：3D内容生成

## 6.1 3D生成概述

### 3D表示方法

```
┌─────────────────────────────────────────────────────────────┐
│                    3D表示方法对比                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. 网格 (Mesh)                                            │
│     ┌───────────────┐                                      │
│     │  顶点 + 面片   │  游戏、动画行业标准                   │
│     │  V + F        │  需要显式几何                         │
│     └───────────────┘                                      │
│                                                             │
│  2. 点云 (Point Cloud)                                     │
│     ┌───────────────┐                                      │
│     │  无序点集合    │  激光雷达扫描格式                     │
│     │  (x,y,z,rgb)  │  无拓扑信息                          │
│     └───────────────┘                                      │
│                                                             │
│  3. 体素 (Voxel)                                           │
│     ┌───────────────┐                                      │
│     │  3D像素网格   │  规则结构，易于CNN处理                │
│     │  N³ grid     │  内存消耗大                          │
│     └───────────────┘                                      │
│                                                             │
│  4. 隐式表示 (Implicit)                                    │
│     ┌───────────────┐                                      │
│     │  神经网络     │  NeRF、SDF                           │
│     │  f(x,y,z)→?  │  连续表示，任意分辨率                 │
│     └───────────────┘                                      │
│                                                             │
│  5. 3D高斯 (3D Gaussian Splatting)                        │
│     ┌───────────────┐                                      │
│     │  高斯椭球集合 │  最新方法，实时渲染                   │
│     │  位置+协方差  │  质量高，速度快                       │
│     └───────────────┘                                      │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 3D生成任务分类

```
3D生成任务：

1. 单图重建 (Image-to-3D)
   单张图片 ──► 3D模型

2. 多视图重建
   多张图片 ──► 3D模型

3. 文本生成3D (Text-to-3D)
   文本描述 ──► 3D模型

4. 3D编辑
   3D模型 + 指令 ──► 编辑后的3D模型

5. 3D补全
   部分3D ──► 完整3D
```

---

## 6.2 NeRF (神经辐射场)

### 基本原理

```
NeRF: Neural Radiance Field

核心思想: 用神经网络表示3D场景

输入: 3D坐标 (x,y,z) + 观察方向 (θ,φ)
输出: 颜色 (r,g,b) + 密度 (σ)

       (x, y, z, θ, φ)
             │
             ▼
      ┌──────────────┐
      │   MLP网络    │
      │  (8层256维)  │
      └──────┬───────┘
             │
             ▼
       (r, g, b, σ)
         颜色   密度

体渲染方程:
C(r) = ∫ T(t)·σ(t)·c(t) dt

其中 T(t) = exp(-∫σ(s)ds) 是透射率
```

### NeRF实现

```python
import torch
import torch.nn as nn

class NeRF(nn.Module):
    def __init__(self, pos_dim=3, dir_dim=3, hidden_dim=256, n_layers=8):
        super().__init__()

        # 位置编码
        self.pos_encoder = PositionalEncoder(pos_dim, L=10)  # 60维
        self.dir_encoder = PositionalEncoder(dir_dim, L=4)   # 24维

        # MLP层
        self.layers = nn.ModuleList()
        in_dim = self.pos_encoder.output_dim  # 60

        for i in range(n_layers):
            if i == 4:  # skip connection
                self.layers.append(nn.Linear(in_dim + self.pos_encoder.output_dim, hidden_dim))
            else:
                self.layers.append(nn.Linear(in_dim if i == 0 else hidden_dim, hidden_dim))

        # 密度输出
        self.sigma_layer = nn.Linear(hidden_dim, 1)

        # 颜色输出（与方向相关）
        self.feature_layer = nn.Linear(hidden_dim, hidden_dim)
        self.color_layer = nn.Sequential(
            nn.Linear(hidden_dim + self.dir_encoder.output_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 3),
            nn.Sigmoid()
        )

    def forward(self, pos, direction):
        """
        pos: (B, 3) 空间坐标
        direction: (B, 3) 观察方向
        """
        # 位置编码
        pos_encoded = self.pos_encoder(pos)
        dir_encoded = self.dir_encoder(direction)

        # MLP forward
        x = pos_encoded
        for i, layer in enumerate(self.layers):
            if i == 4:
                x = torch.cat([x, pos_encoded], dim=-1)
            x = F.relu(layer(x))

        # 密度（与方向无关）
        sigma = F.relu(self.sigma_layer(x))

        # 颜色（与方向相关）
        feature = self.feature_layer(x)
        color = self.color_layer(torch.cat([feature, dir_encoded], dim=-1))

        return color, sigma


class PositionalEncoder(nn.Module):
    """位置编码: 将低维坐标映射到高维"""
    def __init__(self, input_dim, L=10):
        super().__init__()
        self.L = L
        self.output_dim = input_dim * (2 * L)

    def forward(self, x):
        # γ(p) = (sin(2^0·π·p), cos(2^0·π·p), ..., sin(2^L·π·p), cos(2^L·π·p))
        encodings = []
        for i in range(self.L):
            freq = 2 ** i * torch.pi
            encodings.append(torch.sin(freq * x))
            encodings.append(torch.cos(freq * x))
        return torch.cat(encodings, dim=-1)
```

### 体渲染

```python
def volume_rendering(rays_o, rays_d, model, near=0, far=1, n_samples=64):
    """
    体渲染：沿光线积分
    rays_o: (B, 3) 光线起点
    rays_d: (B, 3) 光线方向
    """
    # 采样点
    t_vals = torch.linspace(near, far, n_samples)
    z_vals = rays_o[..., None, :] + rays_d[..., None, :] * t_vals[..., None]
    # z_vals: (B, n_samples, 3)

    # 查询网络
    flat_z = z_vals.reshape(-1, 3)
    flat_d = rays_d[:, None, :].expand_as(z_vals).reshape(-1, 3)
    colors, sigmas = model(flat_z, flat_d)

    colors = colors.reshape(-1, n_samples, 3)
    sigmas = sigmas.reshape(-1, n_samples, 1)

    # 计算权重
    deltas = t_vals[1:] - t_vals[:-1]
    deltas = torch.cat([deltas, torch.tensor([1e10])])

    alpha = 1 - torch.exp(-sigmas * deltas)
    transmittance = torch.cumprod(1 - alpha + 1e-10, dim=1)
    transmittance = torch.cat([torch.ones_like(transmittance[:, :1]), transmittance[:, :-1]], dim=1)

    weights = alpha * transmittance

    # 积分得到颜色
    rgb = (weights * colors).sum(dim=1)

    return rgb
```

### NeRF变体

```
NeRF发展历程：

NeRF (2020)
 │  原始版本，训练慢，每场景需要独立训练
 │
 ├── Instant-NGP (2022)
 │    哈希编码，训练加速100倍
 │
 ├── Mip-NeRF (2021)
 │    抗锯齿，多尺度
 │
 ├── NeRF-W (2021)
 │    处理野外光照变化
 │
 └── 3D Gaussian Splatting (2023) ⭐
     超越NeRF，实时渲染
```

---

## 6.3 3D Gaussian Splatting

### 基本原理

```
3D Gaussian Splatting (3DGS)

核心思想: 用3D高斯椭球集合表示场景

每个高斯由以下参数定义:
- 位置 μ ∈ R³
- 协方差 Σ ∈ R³×³ (定义形状)
- 不透明度 α ∈ [0,1]
- 球谐系数 (颜色，视角相关)

渲染: 将3D高斯投影到2D，按深度排序，alpha混合

优势:
1. 实时渲染 (100+ FPS)
2. 显式表示，易于编辑
3. 训练速度快
```

### 3DGS渲染流程

```
┌─────────────────────────────────────────────────────────────┐
│              3D Gaussian Splatting 渲染                     │
└─────────────────────────────────────────────────────────────┘

3D高斯集合 {μᵢ, Σᵢ, αᵢ, SHᵢ}
         │
         ▼
┌─────────────────────┐
│  1. 视锥剔除        │  去除相机外的高斯
└─────────┬───────────┘
          │
          ▼
┌─────────────────────┐
│  2. 投影到2D        │  3D高斯 → 2D高斯
│                     │  Σ' = JWΣWᵀJᵀ
└─────────┬───────────┘
          │
          ▼
┌─────────────────────┐
│  3. 排序            │  按深度从前到后排序
│     (Radix Sort)    │  GPU并行排序
└─────────┬───────────┘
          │
          ▼
┌─────────────────────┐
│  4. Alpha混合       │  Cᵢ = cᵢαᵢ∏(1-αⱼ)
│     (Splatting)     │  j<i
└─────────┬───────────┘
          │
          ▼
      渲染图像
```

### 3DGS实现要点

```python
class Gaussian3D:
    """3D高斯参数"""
    def __init__(self, n_points):
        # 位置
        self.positions = nn.Parameter(torch.randn(n_points, 3))

        # 协方差 (用四元数+缩放表示)
        self.rotations = nn.Parameter(torch.zeros(n_points, 4))  # 四元数
        self.rotations.data[:, 0] = 1  # 初始化为单位四元数
        self.scales = nn.Parameter(torch.ones(n_points, 3) * 0.01)

        # 不透明度
        self.opacities = nn.Parameter(torch.zeros(n_points, 1))

        # 球谐系数 (用于视角相关的颜色)
        self.sh_coeffs = nn.Parameter(torch.randn(n_points, 16, 3))  # 4阶球谐

    def get_covariance(self):
        """从四元数和缩放计算协方差矩阵"""
        R = quaternion_to_rotation_matrix(self.rotations)
        S = torch.diag_embed(self.scales)
        # Σ = RSSᵀRᵀ
        return R @ S @ S.transpose(-1, -2) @ R.transpose(-1, -2)


def render_gaussians(gaussians, camera, image_size):
    """渲染3D高斯到图像"""
    # 1. 计算2D投影
    means_2d = project_to_2d(gaussians.positions, camera)
    cov_2d = compute_2d_covariance(gaussians, camera)

    # 2. 计算每个像素的颜色
    colors = compute_sh_color(gaussians.sh_coeffs, camera.direction)

    # 3. 排序和混合
    sorted_indices = sort_by_depth(gaussians.positions, camera)

    # 4. 可微分光栅化
    image = rasterize(means_2d, cov_2d, colors, gaussians.opacities, sorted_indices)

    return image
```

### 自适应密度控制

```
3DGS训练中的自适应调整：

1. 克隆 (Clone)
   - 对于梯度大但尺度小的高斯
   - 复制并小幅移动

2. 分裂 (Split)
   - 对于梯度大且尺度大的高斯
   - 分裂成两个较小的高斯

3. 剔除 (Prune)
   - 对于不透明度很低的高斯
   - 或尺度过大的高斯

策略:
if gradient > threshold and scale < threshold:
    clone()
elif gradient > threshold and scale > threshold:
    split()
elif opacity < threshold:
    prune()
```

---

## 6.4 Text-to-3D

### Score Distillation Sampling (SDS)

```
DreamFusion的核心方法：利用2D扩散模型指导3D生成

思路：
1. 从不同视角渲染3D表示
2. 用预训练的2D扩散模型评估渲染图像
3. 将梯度传回3D表示进行优化

SDS Loss:
∇_θ L_SDS = E[w(t)(ε_φ(x_t; y, t) - ε) ∂x/∂θ]

其中:
- θ: 3D表示的参数 (NeRF/3DGS)
- x: 渲染的图像
- ε_φ: 预训练扩散模型预测的噪声
- y: 文本条件
```

### DreamFusion流程

```
              ┌─────────────────────────────────────────────┐
              │           DreamFusion 流程                  │
              └─────────────────────────────────────────────┘

文本: "a DSLR photo of a peacock on a surfboard"
                    │
                    ▼
        ┌─────────────────────────┐
        │  随机初始化NeRF/3DGS    │
        └───────────┬─────────────┘
                    │
    ┌───────────────┴───────────────┐
    │         优化循环              │
    │  ┌────────────────────────┐  │
    │  │ 1. 随机采样相机视角    │  │
    │  │    θ, φ ~ Uniform      │  │
    │  └───────────┬────────────┘  │
    │              │               │
    │              ▼               │
    │  ┌────────────────────────┐  │
    │  │ 2. 渲染图像 x          │  │
    │  │    x = render(NeRF, cam)│  │
    │  └───────────┬────────────┘  │
    │              │               │
    │              ▼               │
    │  ┌────────────────────────┐  │
    │  │ 3. 加噪声              │  │
    │  │    x_t = √ᾱ·x + √(1-ᾱ)·ε│  │
    │  └───────────┬────────────┘  │
    │              │               │
    │              ▼               │
    │  ┌────────────────────────┐  │
    │  │ 4. 扩散模型预测噪声    │  │
    │  │    ε_pred = UNet(x_t, t, text)│
    │  └───────────┬────────────┘  │
    │              │               │
    │              ▼               │
    │  ┌────────────────────────┐  │
    │  │ 5. 计算SDS梯度并更新   │  │
    │  │    θ = θ - lr·∇SDS     │  │
    │  └────────────────────────┘  │
    │                              │
    └──────────────────────────────┘
                    │
                    ▼
            生成的3D模型
```

### Text-to-3D模型演进

```
DreamFusion (2022)
│  NeRF + SDS
│  问题: 多视角不一致，Janus问题
│
├── Magic3D (2023)
│    两阶段：粗糙NeRF → 精细Mesh
│    更高分辨率
│
├── ProlificDreamer (2023)
│    VSD (Variational Score Distillation)
│    解决过饱和问题
│
├── DreamGaussian (2023)
│    3DGS + SDS
│    速度快 (几分钟)
│
└── 最新趋势
    - 多视图扩散模型 (Zero-1-to-3, MVDream)
    - 直接生成3D Token
    - 大规模3D数据训练
```

### 实用Text-to-3D

```python
# 使用threestudio框架
# https://github.com/threestudio-project/threestudio

# 配置文件示例
config = {
    "prompt": "a delicious hamburger",
    "guidance_type": "stable-diffusion",
    "geometry_type": "implicit-volume",  # 或 "gaussian-splatting"
    "material_type": "diffuse-with-point-light-material",
    "renderer_type": "nerf-volume-renderer",
    "guidance_scale": 100,
    "num_iterations": 10000,
}

# 运行
# python launch.py --config configs/dreamfusion-sd.yaml prompt="a hamburger"
```

---

## 6.5 单图3D重建

### Zero-1-to-3

```
Zero-1-to-3: 单图生成新视角

原理: 将视角变换作为条件输入扩散模型

输入: 单张图片 + 目标视角 (R, T)
输出: 目标视角下的图像

训练数据: Objaverse (大规模3D数据集)

流程:
输入图像 + 相对相机变换 ──► 微调的SD ──► 新视角图像

可以结合NeRF/3DGS进行3D重建
```

### 最新方法: LRM/Instant3D

```
Large Reconstruction Model (LRM):

思路: 端到端的Transformer直接预测3D

输入图像
    │
    ▼
┌──────────────────┐
│  Vision Encoder  │  提取图像特征
│    (DINOv2)      │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  Transformer     │  将图像特征转换为
│  (自回归/交叉)   │  3D表示（triplane）
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  NeRF Decoder    │  从triplane解码
│                  │  得到3D模型
└────────┬─────────┘
         │
         ▼
    3D模型

特点:
- 前馈推理，无需优化
- 5秒内完成
- 泛化能力强
```

---

## 6.6 3D资产生成工具

### 实用工具列表

```
开源工具:

1. threestudio
   - 统一框架，支持多种Text-to-3D方法
   - DreamFusion, Magic3D, ProlificDreamer等

2. gaussian-splatting
   - 官方3DGS实现
   - 从多视图图像重建

3. instant-ngp
   - NVIDIA快速NeRF
   - 几分钟完成训练

4. Zero123++
   - 单图生成多视角

商业服务:

1. Luma AI
   - 手机扫描生成3D

2. Meshy
   - Text-to-3D服务

3. CSM
   - 生成可动画的3D角色
```

---

## 总结

| 技术 | 表示方法 | 特点 |
|------|----------|------|
| NeRF | 隐式神经网络 | 质量高，渲染慢 |
| 3DGS | 显式高斯点 | 实时渲染，易编辑 |
| SDS | 蒸馏方法 | 利用2D先验生成3D |
| LRM | 前馈网络 | 快速，泛化好 |

**学习重点：**
1. 理解NeRF的体渲染原理
2. 掌握3D Gaussian Splatting的核心思想
3. 了解SDS如何利用2D扩散模型
4. 实践开源工具进行3D生成
