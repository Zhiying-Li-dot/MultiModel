# 第七章：核心技术详解

## 7.1 扩散模型数学原理

### 前向过程

```
前向过程: 逐步向数据添加高斯噪声

q(x_t | x_{t-1}) = N(x_t; √(1-β_t)·x_{t-1}, β_t·I)

其中 β_t 是噪声调度，通常 β_1 < β_2 < ... < β_T

关键性质（一步加噪公式）：
q(x_t | x_0) = N(x_t; √ᾱ_t·x_0, (1-ᾱ_t)·I)

其中 ᾱ_t = ∏_{i=1}^t (1-β_i)

直接从x_0计算任意时刻x_t:
x_t = √ᾱ_t · x_0 + √(1-ᾱ_t) · ε,  ε ~ N(0, I)
```

### 推导一步加噪公式

```
数学推导：

已知: x_t = √(1-β_t)·x_{t-1} + √β_t·ε_t

设 α_t = 1-β_t，则:
x_t = √α_t·x_{t-1} + √(1-α_t)·ε_t

递归展开:
x_t = √α_t·x_{t-1} + √(1-α_t)·ε_t
    = √α_t·(√α_{t-1}·x_{t-2} + √(1-α_{t-1})·ε_{t-1}) + √(1-α_t)·ε_t
    = √(α_t·α_{t-1})·x_{t-2} + √α_t·√(1-α_{t-1})·ε_{t-1} + √(1-α_t)·ε_t

利用高斯分布的可加性:
N(0, σ₁²) + N(0, σ₂²) = N(0, σ₁² + σ₂²)

可得:
x_t = √ᾱ_t·x_0 + √(1-ᾱ_t)·ε

其中 ᾱ_t = ∏_{i=1}^t α_i = ∏_{i=1}^t (1-β_i)
```

### 反向过程

```
反向过程: 从噪声逐步恢复数据

p_θ(x_{t-1} | x_t) = N(x_{t-1}; μ_θ(x_t, t), σ_t²·I)

目标: 学习反向过程的均值 μ_θ

真实的反向条件分布（当给定x_0时）:
q(x_{t-1} | x_t, x_0) = N(x_{t-1}; μ̃_t(x_t, x_0), β̃_t·I)

其中:
μ̃_t = (√ᾱ_{t-1}·β_t)/(1-ᾱ_t)·x_0 + (√α_t·(1-ᾱ_{t-1}))/(1-ᾱ_t)·x_t
β̃_t = (1-ᾱ_{t-1})/(1-ᾱ_t)·β_t
```

### 训练目标推导

```
ELBO (Evidence Lower Bound):

log p(x) ≥ E_q[-log(q(x_T|x_0)) + ∑_{t>1} -log(q(x_{t-1}|x_t,x_0)/p_θ(x_{t-1}|x_t)) + log p_θ(x_0|x_1)]

简化后的训练目标:
L_simple = E_{t,x_0,ε}[||ε - ε_θ(x_t, t)||²]

即: 训练网络预测噪声ε

为什么预测噪声而不是x_0？
- 数值稳定性更好
- 梯度信号更一致
- 实验效果更好
```

### 代码实现

```python
class GaussianDiffusion:
    def __init__(self, timesteps=1000, beta_start=0.0001, beta_end=0.02, schedule='linear'):
        self.timesteps = timesteps

        # 噪声调度
        if schedule == 'linear':
            betas = torch.linspace(beta_start, beta_end, timesteps)
        elif schedule == 'cosine':
            betas = self.cosine_schedule(timesteps)

        alphas = 1 - betas
        alphas_cumprod = torch.cumprod(alphas, dim=0)

        # 预计算常用值
        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)
        self.posterior_variance = betas * (1 - alphas_cumprod.roll(1)) / (1 - alphas_cumprod)
        self.posterior_variance[0] = betas[0]

    def cosine_schedule(self, timesteps, s=0.008):
        """Cosine噪声调度，更平滑"""
        t = torch.linspace(0, timesteps, timesteps + 1)
        alphas_cumprod = torch.cos((t / timesteps + s) / (1 + s) * torch.pi / 2) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]
        return torch.clamp(betas, 0, 0.999)

    def q_sample(self, x_0, t, noise=None):
        """前向加噪: x_t = √ᾱ_t·x_0 + √(1-ᾱ_t)·ε"""
        if noise is None:
            noise = torch.randn_like(x_0)

        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)
        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)

        return sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise

    def training_loss(self, model, x_0):
        """训练损失: E[||ε - ε_θ(x_t, t)||²]"""
        batch_size = x_0.shape[0]

        # 随机采样时间步
        t = torch.randint(0, self.timesteps, (batch_size,), device=x_0.device)

        # 采样噪声并加噪
        noise = torch.randn_like(x_0)
        x_t = self.q_sample(x_0, t, noise)

        # 预测噪声
        noise_pred = model(x_t, t)

        # MSE损失
        loss = F.mse_loss(noise_pred, noise)
        return loss
```

---

## 7.2 Classifier-Free Guidance (CFG)

### 原理

```
CFG: 无分类器引导

思想: 同时训练有条件和无条件生成，推理时混合两者

训练时:
- 以概率 p (如10%) 随机丢弃条件 (设为空)
- 其余时间使用正常条件

推理时:
ε̃ = ε_θ(x_t, ∅) + s · (ε_θ(x_t, c) - ε_θ(x_t, ∅))
   = (1-s) · ε_θ(x_t, ∅) + s · ε_θ(x_t, c)

其中:
- c: 条件 (如文本)
- ∅: 空条件 (无条件)
- s: 引导强度 (guidance scale)
```

### 为什么CFG有效？

```
直觉理解：

无条件生成 ε_θ(x_t, ∅):
- 生成"一般"的图像
- 没有特定约束

有条件生成 ε_θ(x_t, c):
- 朝着条件方向生成
- 但可能不够强烈

差值 ε_θ(x_t, c) - ε_θ(x_t, ∅):
- 表示"条件相对于无条件的方向"
- 放大这个方向可以增强条件效果

CFG相当于：
"朝着条件的方向走得更远"

s=1: 标准条件生成
s>1: 更强的条件遵循（常用7.5）
s<1: 更弱的条件遵循
```

### CFG实现

```python
@torch.no_grad()
def sample_with_cfg(self, model, shape, condition, guidance_scale=7.5):
    """带CFG的采样"""
    x = torch.randn(shape)  # 从纯噪声开始

    # 需要同时传入有条件和无条件
    # 通常将batch维度翻倍
    for t in reversed(range(self.timesteps)):
        t_tensor = torch.tensor([t])

        # 无条件预测
        noise_uncond = model(x, t_tensor, condition=None)

        # 有条件预测
        noise_cond = model(x, t_tensor, condition=condition)

        # CFG混合
        noise_pred = noise_uncond + guidance_scale * (noise_cond - noise_uncond)

        # 采样步骤
        x = self.p_sample(x, t, noise_pred)

    return x


# 更高效的实现：batch合并
@torch.no_grad()
def sample_with_cfg_batched(self, model, shape, condition, guidance_scale=7.5):
    x = torch.randn(shape)
    uncond = torch.zeros_like(condition)  # 空条件

    for t in reversed(range(self.timesteps)):
        # 合并batch
        x_doubled = torch.cat([x, x], dim=0)
        cond_doubled = torch.cat([uncond, condition], dim=0)

        # 一次前向传播
        noise_pred = model(x_doubled, t, cond_doubled)

        # 分离
        noise_uncond, noise_cond = noise_pred.chunk(2)

        # CFG
        noise_pred = noise_uncond + guidance_scale * (noise_cond - noise_uncond)

        x = self.p_sample(x, t, noise_pred)

    return x
```

---

## 7.3 潜空间扩散 (Latent Diffusion)

### 为什么需要潜空间

```
像素空间 vs 潜空间：

像素空间:
- 512×512×3 = 786,432 维
- 计算量巨大
- 包含大量冗余信息

潜空间:
- 64×64×4 = 16,384 维 (压缩48倍)
- 去除了感知冗余
- 保留了语义信息

┌─────────────────────────────────────────────────────┐
│  图像 (512×512×3) ──► VAE Encoder ──► 潜码 (64×64×4)│
│                                                     │
│  在潜空间进行扩散:                                   │
│  - 计算量降低 ~50倍                                  │
│  - 内存占用降低 ~50倍                                │
│  - 可以用更大的batch size                           │
│                                                     │
│  潜码 (64×64×4) ──► VAE Decoder ──► 图像 (512×512×3)│
└─────────────────────────────────────────────────────┘
```

### VAE训练

```python
class VAE(nn.Module):
    def __init__(self, in_channels=3, latent_channels=4, hidden_dims=[64, 128, 256, 512]):
        super().__init__()

        # Encoder
        self.encoder = Encoder(in_channels, latent_channels * 2, hidden_dims)

        # Decoder
        self.decoder = Decoder(latent_channels, in_channels, hidden_dims[::-1])

    def encode(self, x):
        h = self.encoder(x)
        mean, log_var = h.chunk(2, dim=1)
        return mean, log_var

    def reparameterize(self, mean, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mean + eps * std

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        mean, log_var = self.encode(x)
        z = self.reparameterize(mean, log_var)
        x_recon = self.decode(z)
        return x_recon, mean, log_var


def vae_loss(x, x_recon, mean, log_var, kl_weight=0.00001):
    """
    VAE损失 = 重建损失 + KL散度

    注意: KL权重很小，因为主要目标是重建质量
    """
    # 重建损失 (可以用L1, L2, 或感知损失)
    recon_loss = F.mse_loss(x_recon, x)

    # KL散度
    kl_loss = -0.5 * torch.mean(1 + log_var - mean.pow(2) - log_var.exp())

    return recon_loss + kl_weight * kl_loss
```

### Latent Diffusion完整流程

```python
class LatentDiffusion:
    def __init__(self, vae, unet, text_encoder, scheduler):
        self.vae = vae          # 预训练的VAE (冻结)
        self.unet = unet        # 可训练的UNet
        self.text_encoder = text_encoder  # CLIP文本编码器 (冻结)
        self.scheduler = scheduler

        # VAE缩放因子
        self.vae_scale = 0.18215

    def encode_images(self, images):
        """图像编码到潜空间"""
        with torch.no_grad():
            latents = self.vae.encode(images).latent_dist.sample()
            latents = latents * self.vae_scale
        return latents

    def decode_latents(self, latents):
        """潜空间解码到图像"""
        latents = latents / self.vae_scale
        with torch.no_grad():
            images = self.vae.decode(latents).sample
        return images

    def training_step(self, images, text):
        # 1. 编码图像到潜空间
        latents = self.encode_images(images)

        # 2. 编码文本
        text_embeddings = self.text_encoder(text)

        # 3. 采样噪声和时间步
        noise = torch.randn_like(latents)
        t = torch.randint(0, self.scheduler.timesteps, (latents.shape[0],))

        # 4. 加噪
        noisy_latents = self.scheduler.add_noise(latents, noise, t)

        # 5. 预测噪声
        noise_pred = self.unet(noisy_latents, t, text_embeddings)

        # 6. 计算损失
        loss = F.mse_loss(noise_pred, noise)

        return loss

    @torch.no_grad()
    def generate(self, prompt, num_steps=50, guidance_scale=7.5):
        # 1. 编码文本
        text_emb = self.text_encoder(prompt)
        uncond_emb = self.text_encoder("")

        # 2. 初始化潜码噪声
        latents = torch.randn(1, 4, 64, 64)

        # 3. 去噪循环
        self.scheduler.set_timesteps(num_steps)
        for t in self.scheduler.timesteps:
            latent_input = torch.cat([latents] * 2)
            text_input = torch.cat([uncond_emb, text_emb])

            noise_pred = self.unet(latent_input, t, text_input)
            noise_uncond, noise_cond = noise_pred.chunk(2)
            noise_pred = noise_uncond + guidance_scale * (noise_cond - noise_uncond)

            latents = self.scheduler.step(noise_pred, t, latents)

        # 4. 解码到图像
        images = self.decode_latents(latents)

        return images
```

---

## 7.4 采样加速

### DDIM采样

```
DDIM: 确定性采样，大幅减少步数

核心思想: 将随机ODE转换为确定性ODE

DDPM采样 (随机):
x_{t-1} = μ_θ(x_t, t) + σ_t·z,  z ~ N(0, I)

DDIM采样 (确定性):
x_{t-1} = √ᾱ_{t-1}·x_0_pred + √(1-ᾱ_{t-1})·ε_θ(x_t, t)

其中 x_0_pred = (x_t - √(1-ᾱ_t)·ε_θ(x_t, t)) / √ᾱ_t

优势:
- 可以使用更少的步数 (1000 → 20-50)
- 结果确定性，便于调试
- 支持插值
```

```python
class DDIMSampler:
    def __init__(self, diffusion, ddim_steps=50):
        self.diffusion = diffusion

        # 选择时间步子序列
        total_steps = diffusion.timesteps
        self.ddim_timesteps = np.linspace(0, total_steps - 1, ddim_steps).astype(int)

    @torch.no_grad()
    def sample(self, model, shape, condition=None, eta=0.0):
        """
        eta=0: 完全确定性 (DDIM)
        eta=1: 完全随机 (DDPM)
        """
        x = torch.randn(shape)

        for i in range(len(self.ddim_timesteps) - 1, -1, -1):
            t = self.ddim_timesteps[i]
            t_prev = self.ddim_timesteps[i - 1] if i > 0 else 0

            # 预测噪声
            eps = model(x, torch.tensor([t]), condition)

            # 获取alpha值
            alpha = self.diffusion.alphas_cumprod[t]
            alpha_prev = self.diffusion.alphas_cumprod[t_prev] if t_prev > 0 else 1.0

            # 预测x_0
            x0_pred = (x - torch.sqrt(1 - alpha) * eps) / torch.sqrt(alpha)

            # 计算方向向量
            sigma = eta * torch.sqrt((1 - alpha_prev) / (1 - alpha) * (1 - alpha / alpha_prev))

            # DDIM更新
            x = torch.sqrt(alpha_prev) * x0_pred + \
                torch.sqrt(1 - alpha_prev - sigma ** 2) * eps

            # 添加噪声 (如果eta > 0)
            if sigma > 0 and i > 0:
                x = x + sigma * torch.randn_like(x)

        return x
```

### DPM-Solver

```
DPM-Solver: 更高阶的ODE求解器

将扩散过程视为ODE:
dx/dt = f(x, t)

使用高阶数值方法求解:
- DPM-Solver-1: 类似DDIM
- DPM-Solver-2: 二阶方法，更准确
- DPM-Solver++: 改进版本

效果:
- 10-25步达到高质量
- 比DDIM更快收敛
```

### 一致性模型 (Consistency Models)

```
Consistency Models / LCM:

思想: 直接学习从噪声到数据的映射

传统扩散: x_T → x_{T-1} → ... → x_1 → x_0 (多步)
一致性模型: x_t → x_0 (任意t直接到x_0)

训练方式:
1. 蒸馏: 从预训练扩散模型蒸馏
2. 直接训练: 端到端训练

效果:
- 1-4步生成
- 质量略有下降但可接受
- 实时应用友好
```

---

## 7.5 ControlNet原理

### 架构设计

```
ControlNet: 为预训练扩散模型添加可控条件

核心思想:
1. 复制UNet的encoder部分
2. 用Zero Convolution连接
3. 训练副本，冻结原模型

┌─────────────────────────────────────────────────────────┐
│                  ControlNet 架构                        │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  原始SD UNet (冻结)          ControlNet副本 (可训练)    │
│       │                            │                    │
│  ┌────┴────┐                  ┌────┴────┐              │
│  │ Encoder │◄──Zero Conv ────│ Encoder │              │
│  │ Block 1 │                  │ Block 1 │◄── 条件输入  │
│  └────┬────┘                  └────┬────┘              │
│       │                            │                    │
│  ┌────┴────┐                  ┌────┴────┐              │
│  │ Encoder │◄──Zero Conv ────│ Encoder │              │
│  │ Block 2 │                  │ Block 2 │              │
│  └────┬────┘                  └────┬────┘              │
│       │                            │                    │
│      ...                          ...                   │
│                                                         │
└─────────────────────────────────────────────────────────┘

Zero Convolution:
- 初始化权重为0
- 训练初期输出为0，不影响原模型
- 逐渐学习有意义的输出
```

### Zero Convolution

```python
class ZeroConv2d(nn.Module):
    """零初始化的卷积，用于ControlNet连接"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, 1)
        # 关键: 初始化为0
        nn.init.zeros_(self.conv.weight)
        nn.init.zeros_(self.conv.bias)

    def forward(self, x):
        return self.conv(x)


class ControlNet(nn.Module):
    def __init__(self, base_model):
        super().__init__()

        # 复制encoder部分
        self.encoder_blocks = copy.deepcopy(base_model.encoder_blocks)

        # 条件输入处理
        self.input_hint_block = nn.Sequential(
            nn.Conv2d(3, 16, 3, padding=1),  # 条件图(如边缘图)
            nn.SiLU(),
            nn.Conv2d(16, 32, 3, padding=1),
            nn.SiLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.SiLU(),
            nn.Conv2d(64, 128, 3, padding=1, stride=2),
            # ... 更多层
        )

        # Zero Convolutions
        self.zero_convs = nn.ModuleList([
            ZeroConv2d(ch, ch) for ch in encoder_channels
        ])

    def forward(self, x, timestep, context, hint):
        # 处理条件输入
        hint = self.input_hint_block(hint)

        # 加入条件
        x = x + hint

        # 通过encoder blocks
        outputs = []
        for block, zero_conv in zip(self.encoder_blocks, self.zero_convs):
            x = block(x, timestep, context)
            outputs.append(zero_conv(x))

        return outputs  # 返回给原模型的各层
```

### 使用ControlNet

```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel

# 加载ControlNet (如Canny边缘检测版本)
controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny")

# 创建pipeline
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet
)

# 准备条件图像
import cv2
image = cv2.imread("input.jpg")
edges = cv2.Canny(image, 100, 200)  # Canny边缘检测
control_image = Image.fromarray(edges)

# 生成
output = pipe(
    prompt="a beautiful landscape, oil painting style",
    image=control_image,
    num_inference_steps=30,
).images[0]
```

---

## 7.6 LoRA微调

### LoRA原理

```
LoRA: Low-Rank Adaptation

思想: 用低秩矩阵近似权重更新

原始: W' = W + ΔW     (ΔW是全秩矩阵)
LoRA: W' = W + BA     (B: d×r, A: r×k, r << min(d,k))

例如: d=768, k=768
全量微调: 589,824 参数
LoRA r=8: 768×8 + 8×768 = 12,288 参数 (减少98%)

┌─────────────────────────────────────────┐
│              LoRA 示意图                │
├─────────────────────────────────────────┤
│                                         │
│  输入 x                                 │
│    │                                    │
│    ├──────────────┬──────────────┐      │
│    │              │              │      │
│    ▼              ▼              │      │
│  ┌───┐        ┌───┐           │      │
│  │ W │        │ A │  (d→r)     │      │
│  │   │        └─┬─┘           │      │
│  │冻结│          │              │      │
│  │   │          ▼              │      │
│  └─┬─┘        ┌───┐           │      │
│    │          │ B │  (r→k)     │      │
│    │          └─┬─┘           │      │
│    │            │   ×α/r       │      │
│    │            │   (缩放)     │      │
│    └──────┬─────┘              │      │
│           │                    │      │
│           ▼                    │      │
│         输出 = Wx + (BA)x·α/r          │
│                                         │
└─────────────────────────────────────────┘
```

### LoRA实现

```python
class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=4, alpha=1.0):
        super().__init__()
        self.rank = rank
        self.alpha = alpha

        # 低秩分解: ΔW = BA
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))

        # 缩放因子
        self.scaling = alpha / rank

    def forward(self, x, original_output):
        # LoRA输出
        lora_output = F.linear(F.linear(x, self.lora_A), self.lora_B)
        return original_output + lora_output * self.scaling


class LinearWithLoRA(nn.Module):
    """将LoRA注入到Linear层"""
    def __init__(self, linear_layer, rank=4, alpha=1.0):
        super().__init__()
        self.linear = linear_layer
        self.lora = LoRALayer(
            linear_layer.in_features,
            linear_layer.out_features,
            rank=rank,
            alpha=alpha
        )

        # 冻结原始权重
        self.linear.weight.requires_grad = False
        if self.linear.bias is not None:
            self.linear.bias.requires_grad = False

    def forward(self, x):
        original_output = self.linear(x)
        return self.lora(x, original_output)


def inject_lora(model, rank=4, alpha=1.0, target_modules=['q_proj', 'v_proj']):
    """向模型注入LoRA"""
    for name, module in model.named_modules():
        if any(target in name for target in target_modules):
            if isinstance(module, nn.Linear):
                parent = get_parent_module(model, name)
                child_name = name.split('.')[-1]
                lora_layer = LinearWithLoRA(module, rank=rank, alpha=alpha)
                setattr(parent, child_name, lora_layer)
    return model
```

### LoRA训练

```python
# 使用PEFT库
from peft import LoraConfig, get_peft_model

# LoRA配置
lora_config = LoraConfig(
    r=8,                      # 秩
    lora_alpha=32,            # 缩放因子
    target_modules=["to_q", "to_v", "to_k", "to_out.0"],  # 目标模块
    lora_dropout=0.05,
)

# 应用LoRA
model = get_peft_model(unet, lora_config)

# 只有LoRA参数可训练
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
all_params = sum(p.numel() for p in model.parameters())
print(f"Trainable: {trainable_params:,} / {all_params:,} = {100*trainable_params/all_params:.2f}%")

# 训练
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
for batch in dataloader:
    loss = model.training_step(batch)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

# 保存LoRA权重 (很小，通常几MB)
model.save_pretrained("my_lora_weights")
```

---

## 总结

| 技术 | 作用 | 要点 |
|------|------|------|
| 扩散数学 | 理论基础 | 前向加噪、反向去噪、ELBO |
| CFG | 条件增强 | 混合有条件和无条件预测 |
| 潜空间扩散 | 效率提升 | VAE压缩，减少计算量 |
| DDIM/DPM | 采样加速 | 减少步数，保持质量 |
| ControlNet | 精确控制 | Zero Conv，条件注入 |
| LoRA | 高效微调 | 低秩分解，参数高效 |

**学习优先级：**
1. 扩散模型数学原理 (最重要)
2. CFG原理与实现
3. 潜空间扩散
4. LoRA微调技术
5. ControlNet架构
6. 采样加速方法
