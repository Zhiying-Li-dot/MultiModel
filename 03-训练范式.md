# 第三章：训练范式

## 3.1 预训练阶段

预训练是多模态模型获取基础能力的关键阶段。

### 对比学习 (Contrastive Learning)

#### CLIP风格的对比学习

```
目标：学习图像-文本对齐

Batch中有N个图文对：
        Text₁  Text₂  Text₃  ...  Textₙ
Image₁   ✓      ✗      ✗          ✗
Image₂   ✗      ✓      ✗          ✗
Image₃   ✗      ✗      ✓          ✗
  ...
Imageₙ   ✗      ✗      ✗          ✓

对角线为正样本，其余为负样本
```

#### 损失函数：InfoNCE

```python
def info_nce_loss(image_features, text_features, temperature=0.07):
    """
    image_features: (N, D) 归一化的图像特征
    text_features: (N, D) 归一化的文本特征
    """
    # 计算相似度矩阵
    logits = torch.matmul(image_features, text_features.T) / temperature
    # logits[i,j] = 第i个图像和第j个文本的相似度

    # 标签：第i个图像匹配第i个文本
    labels = torch.arange(len(logits), device=logits.device)

    # 图像到文本的损失
    loss_i2t = F.cross_entropy(logits, labels)

    # 文本到图像的损失
    loss_t2i = F.cross_entropy(logits.T, labels)

    return (loss_i2t + loss_t2i) / 2
```

#### 关键要点

- **温度参数τ**：控制分布的锐度，通常设为0.07
- **大Batch Size**：负样本越多效果越好，CLIP用32K
- **特征归一化**：计算余弦相似度

### 图文匹配 (Image-Text Matching, ITM)

```
输入：图像 + 文本
输出：二分类（匹配/不匹配）

用途：细粒度的对齐判断
```

```python
class ITMHead(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.classifier = nn.Linear(hidden_dim, 2)

    def forward(self, multimodal_features):
        # 取[CLS] token的输出
        cls_output = multimodal_features[:, 0]
        return self.classifier(cls_output)
```

### 图文生成 (Image-Grounded Text Generation)

```
输入：图像 + 部分文本 "A dog is"
输出：下一个token "running"

这是多模态LLM的主要训练目标
```

```python
def generation_loss(model, images, texts):
    # 获取视觉特征
    vision_features = model.vision_encoder(images)
    vision_tokens = model.connector(vision_features)

    # 拼接视觉和文本token
    text_tokens = model.tokenizer(texts)
    input_embeds = torch.cat([vision_tokens, text_tokens], dim=1)

    # 自回归生成损失
    outputs = model.llm(inputs_embeds=input_embeds, labels=text_tokens)
    return outputs.loss
```

---

## 3.2 指令微调阶段

### 目的

让模型学会遵循人类指令完成各种视觉-语言任务。

### 数据格式

```json
{
  "image": "path/to/image.jpg",
  "conversations": [
    {
      "from": "human",
      "value": "<image>\n这张图片里有什么？"
    },
    {
      "from": "assistant",
      "value": "这张图片显示了一只金毛犬在公园的草地上奔跑。狗狗看起来非常活泼开心，毛发在阳光下呈现金色光泽。背景是绿色的草坪和一些树木。"
    },
    {
      "from": "human",
      "value": "狗狗看起来开心吗？"
    },
    {
      "from": "assistant",
      "value": "是的，从它的肢体语言可以看出它非常开心：\n1. 嘴巴张开，舌头伸出，像是在微笑\n2. 尾巴高高翘起\n3. 奔跑的姿态轻快有活力\n4. 耳朵自然下垂，表示放松状态"
    }
  ]
}
```

### 指令微调的任务类型

| 任务类型 | 示例指令 | 能力要求 |
|----------|----------|----------|
| 图像描述 | "描述这张图片" | 基础感知 |
| 视觉问答 | "图中有几个人？" | 计数、识别 |
| OCR | "图中的文字是什么？" | 文字识别 |
| 推理 | "根据图片，接下来会发生什么？" | 逻辑推理 |
| 对话 | 多轮问答 | 上下文理解 |
| 定位 | "狗在图片的什么位置？" | 空间理解 |

### 训练流程

```python
def instruction_tuning_step(model, batch):
    images = batch['images']
    conversations = batch['conversations']

    # 构建输入序列
    # <image> tokens + User: xxx + Assistant: xxx
    input_ids, labels = build_conversation_input(conversations)

    # 只对Assistant的回复计算loss
    # User的输入部分labels设为-100（忽略）

    # 前向传播
    vision_features = model.vision_encoder(images)
    vision_tokens = model.connector(vision_features)

    outputs = model.llm(
        input_ids=input_ids,
        vision_tokens=vision_tokens,
        labels=labels
    )

    return outputs.loss
```

### LLaVA的两阶段训练

```
阶段1: 预训练对齐 (Feature Alignment)
┌─────────────────────────────────────────┐
│ 数据: 595K 图文对 (CC3M过滤)            │
│ 目标: 训练MLP投影层                      │
│ 冻结: 视觉编码器 + LLM                   │
│ 学习: 只学习投影层参数                   │
└─────────────────────────────────────────┘
                    ↓
阶段2: 指令微调 (Visual Instruction Tuning)
┌─────────────────────────────────────────┐
│ 数据: 158K 多轮对话数据                  │
│ 目标: 端到端微调                         │
│ 冻结: 视觉编码器                         │
│ 学习: 投影层 + LLM                       │
└─────────────────────────────────────────┘
```

---

## 3.3 RLHF (人类反馈强化学习)

### 为什么需要RLHF

- 减少幻觉（模型胡说八道）
- 提高回答质量和有用性
- 更好地对齐人类偏好

### RLHF流程

```
┌─────────────────────────────────────────────────────┐
│                    RLHF流程                         │
├─────────────────────────────────────────────────────┤
│                                                     │
│  Step 1: 收集人类偏好数据                           │
│  ┌─────────────────────────────────────┐            │
│  │ 给定图像和问题，模型生成多个回答：    │            │
│  │ Response A: "图中有一只猫..."        │            │
│  │ Response B: "这是一只狗在..."        │            │
│  │                                     │            │
│  │ 人类标注: A > B (A比B更好)           │            │
│  └─────────────────────────────────────┘            │
│                       ↓                             │
│  Step 2: 训练奖励模型 (Reward Model)                │
│  ┌─────────────────────────────────────┐            │
│  │ 输入: (图像, 问题, 回答)              │            │
│  │ 输出: 标量分数 (越高越好)             │            │
│  │                                     │            │
│  │ 训练目标:                            │            │
│  │ P(A>B) = σ(r(A) - r(B))             │            │
│  └─────────────────────────────────────┘            │
│                       ↓                             │
│  Step 3: PPO强化学习优化                            │
│  ┌─────────────────────────────────────┐            │
│  │ 用奖励模型的分数作为奖励信号          │            │
│  │ 优化策略模型生成更高分的回答          │            │
│  │                                     │            │
│  │ 目标: max E[r(response)] - β·KL     │            │
│  │ KL散度防止模型偏离太远               │            │
│  └─────────────────────────────────────┘            │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 奖励模型训练

```python
class RewardModel(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        self.reward_head = nn.Linear(hidden_dim, 1)

    def forward(self, image, text):
        features = self.base_model(image, text)
        reward = self.reward_head(features[:, -1])  # 取最后一个token
        return reward

def reward_model_loss(model, batch):
    # batch包含成对的(chosen, rejected)回答
    r_chosen = model(batch['image'], batch['chosen'])
    r_rejected = model(batch['image'], batch['rejected'])

    # Bradley-Terry模型
    loss = -F.logsigmoid(r_chosen - r_rejected).mean()
    return loss
```

### DPO (Direct Preference Optimization)

RLHF的简化替代方案，不需要训练单独的奖励模型：

```python
def dpo_loss(model, ref_model, batch, beta=0.1):
    """
    直接优化偏好，不需要奖励模型
    """
    # 计算策略模型的log概率
    pi_chosen = model.log_prob(batch['chosen'])
    pi_rejected = model.log_prob(batch['rejected'])

    # 计算参考模型的log概率
    ref_chosen = ref_model.log_prob(batch['chosen'])
    ref_rejected = ref_model.log_prob(batch['rejected'])

    # DPO损失
    loss = -F.logsigmoid(
        beta * ((pi_chosen - ref_chosen) - (pi_rejected - ref_rejected))
    ).mean()

    return loss
```

---

## 3.4 完整训练流程示例

以LLaVA-1.5为例：

```
┌─────────────────────────────────────────────────────┐
│               LLaVA-1.5 训练流程                    │
├─────────────────────────────────────────────────────┤
│                                                     │
│  第1阶段: 预训练                                    │
│  ├─ 数据: 558K图文对                                │
│  ├─ 训练: 只训练MLP投影层                           │
│  ├─ 冻结: CLIP ViT-L + Vicuna-7B                   │
│  ├─ Batch: 256                                     │
│  └─ Epochs: 1                                      │
│                                                     │
│  第2阶段: 指令微调                                  │
│  ├─ 数据: 665K混合数据                              │
│  │   ├─ LLaVA-Instruct: 158K                       │
│  │   ├─ VQA数据: 多个数据集                         │
│  │   └─ OCR数据: TextCaps等                         │
│  ├─ 训练: 投影层 + LLM全参数                        │
│  ├─ 冻结: CLIP ViT-L                               │
│  ├─ Batch: 128                                     │
│  └─ Epochs: 1                                      │
│                                                     │
└─────────────────────────────────────────────────────┘
```

---

## 总结

| 训练阶段 | 目标 | 数据 | 训练参数 |
|----------|------|------|----------|
| 预训练-对比学习 | 图文对齐 | 大规模图文对 | 双编码器 |
| 预训练-生成 | 语言建模 | 图文对 | 连接模块+LLM |
| 指令微调 | 遵循指令 | 对话数据 | 连接模块+LLM |
| RLHF/DPO | 对齐偏好 | 偏好数据 | 全模型/部分 |

训练技巧：
1. **渐进式解冻**：先训练连接模块，再解冻LLM
2. **学习率设置**：视觉编码器用小学习率或冻结
3. **数据混合**：混合多种任务数据提升泛化
4. **质量优先**：高质量小数据 > 低质量大数据
